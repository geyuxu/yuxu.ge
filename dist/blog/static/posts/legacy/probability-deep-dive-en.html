<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Deeper Dive into Probability: From Convergence to Core Concepts | Yuxu Ge</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1/themes/prism-tomorrow.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <style>
        :root {
            --black: #111;
            --dark-grey: #444;
            --off-white: #f4f4f4;
            --vermilion: #C41E3A;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            font-size: 16px;
            scroll-behavior: smooth;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: var(--black);
            background-color: var(--off-white);
        }

        a {
            color: var(--vermilion);
            text-decoration: none;
        }

        a:hover {
            opacity: 0.75;
        }

        .layout {
            min-height: 100vh;
        }

        /* Sidebar */
        .sidebar {
            position: fixed;
            top: 0;
            left: 0;
            width: 320px;
            height: 100vh;
            background: var(--black);
            color: var(--off-white);
            padding: 3rem 2rem;
            display: flex;
            flex-direction: column;
            justify-content: space-between;
        }

        .sidebar-top {
            display: flex;
            flex-direction: column;
            align-items: center;
            text-align: center;
        }

        .avatar {
            display: block;
            width: 120px;
            height: 120px;
            border-radius: 50%;
            border: 3px solid var(--vermilion);
            margin-bottom: 1.5rem;
            overflow: hidden;
            transition: transform 0.2s ease;
        }

        .avatar:hover {
            opacity: 1;
            transform: scale(1.05);
        }

        .avatar img {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        .name-block {
            margin-bottom: 0.5rem;
        }

        h1 {
            font-size: 1.8rem;
            font-weight: 700;
            letter-spacing: 1px;
            color: var(--off-white);
        }

        .title-role {
            font-size: 0.9rem;
            color: #888;
            margin-top: 0.25rem;
            font-weight: 400;
        }

        .contact-block {
            margin-top: 2.5rem;
            width: 100%;
        }

        .social-links {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 0.75rem;
        }

        .social-links a {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            width: 160px;
            color: var(--off-white);
            opacity: 0.7;
            padding: 0.5rem 0.75rem;
            border-radius: 4px;
            font-size: 0.9rem;
            transition: all 0.2s;
        }

        .social-links a:hover {
            color: var(--vermilion);
            opacity: 1;
            background: rgba(255, 255, 255, 0.05);
        }

        .social-links svg {
            width: 20px;
            height: 20px;
            flex-shrink: 0;
        }

        .social-links .orcid-link {
            font-size: 0.68rem;
            gap: 0.5rem;
            white-space: nowrap;
        }

        .sidebar-footer {
            text-align: center;
            font-size: 0.75rem;
            color: #555;
        }

        /* Content */
        .content {
            margin-left: 320px;
            padding: 3rem 4rem;
            max-width: calc(100vw - 320px - 8rem);
        }

        .back-link {
            display: inline-block;
            font-size: 0.85rem;
            margin-bottom: 2rem;
            color: var(--dark-grey);
        }

        .back-link:hover {
            color: var(--vermilion);
        }

        /* Article */
        article h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: 1.5rem;
            line-height: 1.3;
            color: var(--black);
        }

        article h2 {
            font-size: 1.4rem;
            font-weight: 600;
            margin: 2rem 0 1rem;
            color: var(--black);
        }

        article h3 {
            font-size: 1.1rem;
            font-weight: 600;
            margin: 1.5rem 0 0.75rem;
            color: var(--black);
        }

        article p {
            margin-bottom: 1rem;
            color: var(--dark-grey);
        }

        article ul,
        article ol {
            margin: 1rem 0 1rem 1.5rem;
            color: var(--dark-grey);
        }

        article li {
            margin-bottom: 0.5rem;
        }

        article strong {
            color: var(--black);
        }

        article code {
            font-family: "SF Mono", Monaco, monospace;
            font-size: 0.9em;
            background: #e8e8e8;
            padding: 0.15em 0.4em;
            border-radius: 3px;
        }

        article pre {
            margin: 1rem 0;
            border-radius: 6px;
            overflow-x: auto;
            background: #2d2d2d;
            padding: 1rem;
        }

        article pre code {
            background: none;
            padding: 0;
            font-size: 0.75em;
            white-space: pre !important;
            counter-reset: line;
            display: block;
            color: #ccc;
        }

        article pre code .line {
            counter-increment: line;
        }

        article pre code .line::before {
            content: counter(line);
            display: inline-block;
            width: 2.5em;
            margin-right: 1em;
            text-align: right;
            color: #666;
            user-select: none;
        }

        .code-toolbar {
            display: flex;
            justify-content: flex-start;
            padding: 0.35rem 0.5rem;
            background: #3a3a3a;
            border-radius: 6px 6px 0 0;
        }

        .code-toolbar+pre {
            margin-top: 0;
            border-radius: 0 0 6px 6px;
        }

        .copy-btn {
            padding: 0.2rem 0.5rem;
            font-size: 0.7rem;
            background: #555;
            color: #fff;
            border: none;
            border-radius: 3px;
            cursor: pointer;
            transition: all 0.2s;
        }

        .copy-btn:hover,
        .copy-btn:active {
            background: #777;
        }

        .copy-btn.copied {
            background: #2a2;
        }

        article blockquote {
            border-left: 3px solid var(--vermilion);
            padding-left: 1rem;
            margin: 1rem 0;
            color: #666;
            font-style: italic;
        }

        article hr {
            border: none;
            border-top: 1px solid #ddd;
            margin: 2rem 0;
        }

        article table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
            font-size: 0.9rem;
        }

        article th,
        article td {
            border: 1px solid #ddd;
            padding: 0.5rem 0.75rem;
            text-align: left;
        }

        article th {
            background: #e8e8e8;
            font-weight: 600;
        }

        article img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5rem 0;
            border-radius: 6px;
        }

        .loading {
            text-align: center;
            padding: 3rem;
            color: #888;
        }

        .error {
            color: var(--vermilion);
        }

        /* Language Selector */
        .lang-selector {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            margin-bottom: 1.5rem;
            padding: 0.75rem 1rem;
            background: #fff;
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            font-size: 0.85rem;
        }

        .lang-selector-label {
            color: #666;
            display: flex;
            align-items: center;
            gap: 0.35rem;
        }

        .lang-selector-label svg {
            width: 16px;
            height: 16px;
        }

        .lang-select {
            padding: 0.4rem 0.75rem;
            border: 1px solid #ddd;
            border-radius: 4px;
            background: white;
            font-size: 0.85rem;
            cursor: pointer;
            min-width: 140px;
        }

        .lang-select:focus {
            outline: none;
            border-color: var(--vermilion);
        }

        .lang-status {
            font-size: 0.8rem;
            color: #888;
            margin-left: auto;
        }

        .lang-status.translating {
            color: var(--vermilion);
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .lang-status.translating::before {
            content: '';
            width: 14px;
            height: 14px;
            border: 2px solid #f0f0f0;
            border-top-color: var(--vermilion);
            border-radius: 50%;
            animation: spin 0.8s linear infinite;
        }

        @keyframes spin {
            to { transform: rotate(360deg); }
        }

        .lang-status.cached {
            color: #4CAF50;
        }

        .lang-original-btn {
            padding: 0.35rem 0.6rem;
            font-size: 0.75rem;
            background: #f0f0f0;
            border: 1px solid #ddd;
            border-radius: 4px;
            cursor: pointer;
            color: #666;
            transition: all 0.2s;
        }

        .lang-original-btn:hover {
            background: #e0e0e0;
            color: #333;
        }

        @media (max-width: 600px) {
            .lang-selector {
                flex-wrap: wrap;
            }
            .lang-status {
                width: 100%;
                margin-left: 0;
                margin-top: 0.5rem;
            }
        }

        /* KaTeX math styles */
        .katex-display {
            overflow-x: auto;
            overflow-y: hidden;
            padding: 0.5rem 0;
        }

        .katex {
            font-size: 1.1em;
        }

        /* Jupyter Notebook Styles */
        .notebook-container {
            margin-top: 1rem;
        }

        .notebook-download {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            margin-bottom: 1.5rem;
            background: var(--black);
            color: var(--off-white);
            border-radius: 4px;
            font-size: 0.85rem;
            transition: opacity 0.2s;
        }

        .notebook-download:hover {
            opacity: 0.8;
            color: var(--off-white);
        }

        .notebook-download svg {
            width: 16px;
            height: 16px;
        }

        .nb-cell {
            margin-bottom: 1.5rem;
        }

        .nb-cell-code .nb-source {
            background: #2d2d2d;
            border-radius: 6px;
            overflow-x: auto;
        }

        .nb-cell-code .nb-source pre {
            margin: 0;
            padding: 1rem;
            color: #ccc;
            font-family: "SF Mono", Monaco, monospace;
            font-size: 0.85em;
        }

        .nb-cell-code .nb-source code {
            background: none;
            padding: 0;
        }

        .nb-output {
            background: #fff;
            border: 1px solid #ddd;
            border-radius: 6px;
            padding: 0.75rem;
            margin-top: 0.5rem;
        }

        .nb-output pre {
            margin: 0;
            padding: 0;
            background: transparent;
            white-space: pre-wrap;
            word-wrap: break-word;
            font-family: "SF Mono", Monaco, monospace;
            font-size: 0.85em;
            color: #222;
        }

        .nb-output img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 0.5rem 0;
        }

        .nb-output table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.9rem;
        }

        .nb-output th,
        .nb-output td {
            border: 1px solid #ddd;
            padding: 0.5rem;
            text-align: left;
        }

        .nb-output th {
            background: #e8e8e8;
            font-weight: 600;
        }

        .nb-cell-markdown {
            padding: 0;
        }

        .nb-cell-markdown h1:first-child {
            margin-top: 0;
        }

        /* Document Download Button */
        .doc-download {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            margin-bottom: 1.5rem;
            background: var(--black);
            color: var(--off-white);
            border-radius: 4px;
            font-size: 0.85rem;
            transition: opacity 0.2s;
        }

        .doc-download:hover {
            opacity: 0.8;
            color: var(--off-white);
        }

        .doc-download svg {
            width: 16px;
            height: 16px;
        }

        /* PDF Viewer */
        .pdf-container {
            margin-top: 1rem;
        }

        .pdf-viewer {
            width: 100%;
            height: 80vh;
            border: 1px solid #ddd;
            border-radius: 6px;
            background: #f5f5f5;
        }

        /* Text File Display */
        .text-container {
            margin-top: 1rem;
        }

        .text-content {
            background: #fff;
            border: 1px solid #ddd;
            border-radius: 6px;
            padding: 1.5rem;
            font-family: "SF Mono", Monaco, monospace;
            font-size: 0.9em;
            line-height: 1.6;
            white-space: pre-wrap;
            word-wrap: break-word;
            color: var(--dark-grey);
            max-height: 80vh;
            overflow-y: auto;
        }

        /* Word Document Display */
        .word-container {
            margin-top: 1rem;
        }

        .word-content {
            background: #fff;
            border: 1px solid #ddd;
            border-radius: 6px;
            padding: 2rem;
            line-height: 1.8;
            color: var(--dark-grey);
        }

        .word-content p {
            margin-bottom: 1rem;
        }

        .word-content h1,
        .word-content h2,
        .word-content h3 {
            color: var(--black);
            margin: 1.5rem 0 1rem;
        }

        .word-content table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
        }

        .word-content th,
        .word-content td {
            border: 1px solid #ddd;
            padding: 0.5rem;
        }

        .word-content img {
            max-width: 100%;
            height: auto;
        }

        /* Excel Display */
        .excel-container {
            margin-top: 1rem;
        }

        .excel-tabs {
            display: flex;
            gap: 0.5rem;
            margin-bottom: 1rem;
            flex-wrap: wrap;
        }

        .excel-tab {
            padding: 0.5rem 1rem;
            background: #e8e8e8;
            border: none;
            border-radius: 4px 4px 0 0;
            cursor: pointer;
            font-size: 0.85rem;
        }

        .excel-tab.active {
            background: #4CAF50;
            color: white;
        }

        .excel-content {
            background: #fff;
            border: 1px solid #ddd;
            border-radius: 6px;
            overflow-x: auto;
            max-height: 70vh;
            overflow-y: auto;
        }

        .excel-content table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.85rem;
        }

        .excel-content th,
        .excel-content td {
            border: 1px solid #ddd;
            padding: 0.4rem 0.6rem;
            text-align: left;
            white-space: nowrap;
        }

        .excel-content th {
            background: #f5f5f5;
            font-weight: 600;
            position: sticky;
            top: 0;
        }

        .excel-content tr:nth-child(even) {
            background: #fafafa;
        }

        /* PowerPoint Display */
        .ppt-container {
            margin-top: 1rem;
        }

        .ppt-viewer {
            width: 100%;
            height: 80vh;
            border: 1px solid #ddd;
            border-radius: 6px;
            background: #f5f5f5;
        }

        /* CSV Display (uses excel styles) */
        .csv-container {
            margin-top: 1rem;
        }

        /* Responsive */
        @media (max-width: 900px) {
            .sidebar {
                position: relative;
                width: 100%;
                height: auto;
                padding: 2rem 1.5rem 1rem;
            }

            .social-links {
                flex-direction: row;
                flex-wrap: wrap;
                justify-content: center;
            }

            .social-links a,
            .social-links .orcid-link {
                width: auto;
                padding: 0.5rem;
                font-size: 0;
                gap: 0;
            }

            .social-links svg {
                width: 24px;
                height: 24px;
            }

            .social-links .hide-mobile {
                display: none;
            }

            .sidebar-footer {
                margin-top: 1rem;
            }

            .content {
                margin-left: 0;
                padding: 2rem 1rem;
                max-width: 100%;
            }
        }

        @media (max-width: 480px) {
            .sidebar {
                padding: 1.5rem 1rem 1rem;
            }

            .avatar {
                width: 80px;
                height: 80px;
            }

            h1 {
                font-size: 1.4rem;
            }

            article h1 {
                font-size: 1.5rem;
            }
        }
    </style>
</head>

<body>

    <div class="layout">
        <aside class="sidebar" id="sidebar-content"></aside>
        <script src="/components/sidebar.js?v=20260128"></script>

        <main class="content">
            <a href="/blog/" class="back-link">‚Üê Back to Blog</a>

            <!-- Language Selector -->
            <div class="lang-selector" id="lang-selector" style="display: none;">
                <span class="lang-selector-label">
                    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 5h12M9 3v2m1.048 9.5A18.022 18.022 0 016.412 9m6.088 9h7M11 21l5-10 5 10M12.751 5C11.783 10.77 8.07 15.61 3 18.129" />
                    </svg>
                    Language
                </span>
                <select class="lang-select" id="lang-select">
                    <option value="en">üá∫üá∏ English</option>
                    <option value="zh">üá®üá≥ ‰∏≠Êñá</option>
                    <option value="ja">üáØüáµ Êó•Êú¨Ë™û</option>
                    <option value="ko">üá∞üá∑ ÌïúÍµ≠Ïñ¥</option>
                    <option value="es">üá™üá∏ Espa√±ol</option>
                    <option value="fr">üá´üá∑ Fran√ßais</option>
                    <option value="de">üá©üá™ Deutsch</option>
                    <option value="pt">üáµüáπ Portugu√™s</option>
                    <option value="ru">üá∑üá∫ –†—É—Å—Å–∫–∏–π</option>
                    <option value="ar">üá∏üá¶ ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</option>
                </select>
                <button class="lang-original-btn" id="lang-original-btn" style="display: none;">Show Original</button>
                <span class="lang-status" id="lang-status"></span>
            </div>

            <article>
<hr>
<h2>date: 2025-01-24
tags: [ai]
legacy: true</h2>
<h1>A Deeper Dive into Probability: From Convergence to Core Concepts</h1>
<h3>Almost Sure Convergence: The Strongest Form</h3>
<p><strong>Almost sure convergence</strong> means that the random variable sequence $X_n$ converges to $X$ on &quot;almost all&quot; sample paths in the probability space. Intuitively, with probability 1, when $n$ is sufficiently large, $X_n$ will get arbitrarily close to $X$ and eventually stay close forever.</p>
<p>For coin flipping, this is the <strong>Strong Law of Large Numbers</strong>: the sample proportion of heads almost surely converges to the true probability 0.5. This is the strongest form of convergence and implies all other types.</p>
<p><img src="/blog/images/legacy/2025-05-27-1606027p4nbP.png" alt="image-20250527160559436"></p>
<p><strong>The figure above (10 colored curves) illustrates almost sure convergence:</strong></p>
<p>Each curve represents one complete experiment‚Äîrepeatedly flipping a fair coin, where the x-axis shows the number of tosses $n$ and the y-axis shows the proportion of heads $\hat{p}_n$ in the first $n$ tosses.</p>
<ul>
<li><strong>Black dashed line:</strong> True probability $p=0.5$</li>
<li><strong>Gray band:</strong> The 0.45‚Äì0.55 interval, representing the intuitive &quot;settling&quot; range</li>
</ul>
<p>Key observations:</p>
<ol>
<li>Initially, fluctuations are enormous (some curves even reach 0 or 1)</li>
<li>As $n$ increases, all paths gradually stabilize and <em>permanently</em> stay within the gray band</li>
<li>They never stray far from 0.5 again‚Äîthis is the <strong>Strong Law of Large Numbers</strong>:</li>
</ol>
<p>$$\mathbb{P}\left(\lim_{n\to\infty}\hat{p}_n = 0.5\right) = 1$$</p>
<blockquote>
<p>If we could draw infinitely many paths, almost every one would behave this way. Only paths with probability 0 might oscillate forever‚Äîbut you&#39;d essentially never observe them.</p>
</blockquote>
<h3>Convergence in Probability: The Practical Standard</h3>
<p><strong>Convergence in probability</strong> means that as $n \to \infty$, the probability that $X_n$ differs from $X$ by more than any small threshold $\varepsilon$ approaches zero: $\Pr(|X_n - X| &gt; \varepsilon) \to 0$.</p>
<p>This is weaker than almost sure convergence. While $X_n$ is usually very close to $X$ for large $n$, occasional deviations are still possible. The <strong>Weak Law of Large Numbers</strong> proves that sample means converge in probability to their expected values.</p>
<p><strong>A Classic Counterexample:</strong> Consider $X_n$ that takes value 1 with probability $1/n$ and 0 otherwise. Then $\Pr(X_n = 1) = 1/n \to 0$, so $X_n \xrightarrow{p} 0$ (convergence in probability). However, since $\sum_n 1/n$ diverges, there will almost surely be infinitely many moments where $X_n = 1$, meaning the sample paths don&#39;t actually converge to 0. This shows that convergence in probability doesn&#39;t guarantee almost sure convergence.</p>
<p><img src="/blog/images/legacy/2025-05-27-161544OrE0kt.png" alt="image-20250527161542690"></p>
<p>The figure shows <strong>yellow spikes</strong> representing the random variable $X_n$‚Äîmostly taking value 0, but occasionally (with probability $1/n$) jumping to 1. The <strong>red curve</strong> shows how this probability $1/n$ decreases with $n$.</p>
<ul>
<li><p>As $n$ increases, <strong>the probability of getting 1 becomes smaller</strong>, satisfying:
$$\Pr(|X_n-0|&gt;\tfrac{1}{2}) = \Pr(X_n=1) = \tfrac{1}{n} \longrightarrow 0$$
This confirms <strong>convergence in probability</strong>: $X_n\xrightarrow{p}0$</p>
</li>
<li><p>However, observing individual paths reveals: <strong>The spikes become sparser but never disappear</strong>‚Äîno matter how large $n$ gets, 1 will still occasionally appear, so sample paths don&#39;t truly converge to 0.</p>
</li>
</ul>
<p>This demonstrates that <strong>convergence in probability ‚â† almost sure convergence</strong>.</p>
<h3>Convergence in Distribution: The Weakest Form</h3>
<p><strong>Convergence in distribution</strong> means the cumulative distribution function of $X_n$ converges to that of $X$. Roughly speaking, the probability distributions gradually become similar in shape, but we don&#39;t care whether individual realizations are close.</p>
<p>This is the weakest form of convergence. The <strong>Central Limit Theorem</strong> is a prime example: regardless of the original distribution (as long as variance is finite and observations are i.i.d.), the standardized sample mean converges in distribution to a standard normal distribution.</p>
<p><strong>Key Relationship:</strong> Almost sure convergence $\implies$ convergence in probability $\implies$ convergence in distribution. The reverse implications generally don&#39;t hold, as our counterexamples demonstrate.</p>
<h3>When Convergence of Moments Fails</h3>
<p>Here&#39;s a surprising fact: <strong>convergence in probability doesn&#39;t guarantee convergence of variance</strong>. Consider this counterexample:</p>
<p>Define $X_n$ as: with probability $1/n$, $X_n = \sqrt{n}$; otherwise $X_n = 0$.</p>
<ul>
<li><strong>Expectation:</strong> $\mathbb{E}[X_n] = \sqrt{n} \cdot \frac{1}{n} + 0 \cdot (1-\frac{1}{n}) = \frac{1}{\sqrt{n}} \to 0$</li>
<li><strong>Variance:</strong> Since $X_n$ is either 0 or $\sqrt{n}$, we have $\mathbb{E}[X_n^2] = n \cdot \frac{1}{n} = 1$. Thus $\operatorname{Var}(X_n) = 1 - \frac{1}{n} \to 1$</li>
</ul>
<p>This $X_n$ converges to 0 in probability (since $\Pr(X_n \neq 0) = 1/n \to 0$), yet its variance approaches 1, not 0! The reason: although extreme values become increasingly rare, they also become increasingly extreme, maintaining their contribution to the variance.</p>
<p>This example reveals that different aspects of random variables can behave very differently during convergence, requiring careful analysis of what exactly is converging.</p>
<h2>Part 2: The Limit Theorems That Shape Our World</h2>
<h3>The Central Limit Theorem: Why Standardization Matters</h3>
<p><strong>Scenario:</strong> Imagine rolling many dice and recording their sum. With 1 die, outcomes are uniform (1-6). With 2 dice, the sum distribution becomes triangular (2-12, centered at 7). With 10 dice, what happens? Intuition suggests the sum will approach a &quot;bell curve.&quot;</p>
<p>This intuition is formalized by the <strong>Central Limit Theorem (CLT)</strong>: the sum (or average) of many independent, identically distributed random variables, when properly standardized, converges in distribution to a normal distribution‚Äîregardless of the original distribution shape.</p>
<p><strong>Why Standardization?</strong> Without standardization, the sum $S_n = X_1 + \cdots + X_n$ would have mean $n\mu$ and standard deviation $\sqrt{n}\sigma$, growing without bound. To observe a non-trivial limiting distribution, we center by subtracting $n\mu$ and scale by dividing by $\sqrt{n}\sigma$:</p>
<p>$$Z_n = \frac{S_n - n\mu}{\sigma\sqrt{n}} = \frac{\overline{X}_n - \mu}{\sigma/\sqrt{n}}$$</p>
<p>The CLT states that as $n \to \infty$, $Z_n$ converges in distribution to $N(0,1)$.</p>
<p><strong>Practical Implication:</strong> For large $n$, $\Pr(|\overline{X}_n - \mu| &lt; 3\sigma/\sqrt{n}) \approx 0.997$. This quantifies the sample mean&#39;s fluctuation: it scales as $O(1/\sqrt{n})$, and the constant &quot;3&quot; corresponds to the 99.7% coverage of a normal distribution.</p>
<p><img src="/blog/images/legacy/2025-05-27-143541BbwlaJ.png" alt="image-20250527143536931"></p>
<p><em>The figure shows how dice sums gradually approach a normal distribution. Top left: 1 die (uniform). Top right: 2 dice (triangular). Bottom left: 3 dice (more concentrated). Bottom right: smooth curves for 1, 2, 3, 4 dice sums with the standard normal curve overlaid (black). As the number of dice increases, the sum distribution progressively approaches normality.</em></p>
<p><strong>Example:</strong> Suppose we measure machine part errors $X$ with unknown distribution but $\mu=0$, $\sigma=2$ mm. For $n=36$ parts, the average error $\overline{X}<em>{36}$ satisfies $\sqrt{36}(\overline{X}</em>{36}-0)/2 \approx N(0,1)$. Therefore, $\Pr(|\overline{X}_{36}| &lt; 1) \approx \Pr(|Z| &lt; 3) \approx 0.997$. The probability that the average error falls within ¬±1 mm is about 99.7%.</p>
<h3>From Binomial to Poisson: The Law of Rare Events</h3>
<p><strong>Scenario:</strong> A website has many users $n$, each with a small probability $p = \lambda/n$ of performing some action (e.g., logging in on a given day). How many users will perform this action?</p>
<p>When $n$ is large and $p$ is small while $np = \lambda$ remains moderate, the binomial distribution can be approximated by a Poisson distribution. This is the <strong>law of rare events</strong>, fundamental in telecommunications, queueing theory, and reliability engineering.</p>
<p><strong>Mathematical Development:</strong> Let $X_n \sim \text{Binomial}(n, \lambda/n)$. We have $\mathbb{E}[X_n] = \lambda$ and $\operatorname{Var}(X_n) \approx \lambda$ (since $(1-p) \approx 1$ when $p$ is small).</p>
<p>For the probability mass function:
$$\Pr(X_n = k) = \binom{n}{k} p^k (1-p)^{n-k} = \frac{n!}{k!(n-k)!}\left(\frac{\lambda}{n}\right)^k \left(1-\frac{\lambda}{n}\right)^{n-k}$$</p>
<p>As $n \to \infty$ with fixed $k$:</p>
<ol>
<li>$\frac{n!}{(n-k)!} = n(n-1)\cdots(n-k+1) \approx n^k$</li>
<li>Thus $\binom{n}{k} \left(\frac{\lambda}{n}\right)^k \approx \frac{\lambda^k}{k!}$</li>
<li>$\left(1-\frac{\lambda}{n}\right)^{n-k} \approx e^{-\lambda}$ (using the standard limit)</li>
</ol>
<p>Combining these: $\Pr(X_n = k) \to e^{-\lambda}\frac{\lambda^k}{k!}$, which is the Poisson$(\lambda)$ probability mass function.</p>
<p><strong>Rule of Thumb:</strong> If $n \geq 100$ and $np \leq 10$, the Poisson approximation to the binomial is quite accurate.</p>
<h2>Part 3: A Practical Toolkit for Bounding Uncertainty</h2>
<p>When we know little about a random variable&#39;s distribution, <strong>probability inequalities</strong> provide crucial bounds on tail probabilities. Different inequalities require different assumptions and provide bounds of varying tightness.</p>
<h3>Markov&#39;s Inequality: The Most General Bound</h3>
<p>For non-negative random variables $X \geq 0$:
$$\Pr(X \geq a) \leq \frac{\mathbb{E}[X]}{a}$$</p>
<p><strong>Pros:</strong> Requires only knowledge of the mean. <strong>Cons:</strong> Often provides very loose bounds.</p>
<p><strong>Example:</strong> If a model&#39;s error $E \geq 0$ has $\mathbb{E}[E] = 5$, then $\Pr(E \geq 50) \leq 0.1$. This is an upper bound‚Äîthe true probability might be much smaller.</p>
<h3>Chebyshev&#39;s Inequality: Leveraging Variance Information</h3>
<p>For any random variable with finite variance:
$$\Pr(|X - \mathbb{E}[X]| \geq \varepsilon) \leq \frac{\operatorname{Var}(X)}{\varepsilon^2}$$</p>
<p><strong>Advantages:</strong> Doesn&#39;t require non-negativity or boundedness, and typically gives tighter bounds than Markov when variance is known.</p>
<p><strong>Example:</strong> If model error $E$ has $\mathbb{E}[E] = 0$ and $\operatorname{Var}(E) = 25$, then $\Pr(|E| \geq 10) \leq 0.25$.</p>
<h3>Hoeffding&#39;s Inequality: The Power of Bounded Variables</h3>
<p>For independent bounded random variables $X_1, \ldots, X_n$ with $X_i \in [0,1]$:
$$\Pr(|\overline{X}_n - \mathbb{E}[\overline{X}_n]| \geq \varepsilon) \leq 2\exp(-2n\varepsilon^2)$$</p>
<p>This provides <strong>exponential concentration</strong>, making it extremely powerful for large $n$.</p>
<p><strong>Corrected Example:</strong> To ensure the deviation probability $\varepsilon = 0.1$ is below 5%:</p>
<ul>
<li><strong>Hoeffding:</strong> Solving $2e^{-2n(0.1)^2} &lt; 0.05$ gives $n &gt; 184$ (approximately 185 samples)</li>
<li><strong>Chebyshev:</strong> With worst-case variance 0.25, solving $\frac{0.25}{n(0.1)^2} &lt; 0.05$ gives $n &gt; 500$</li>
</ul>
<p>As $n$ increases, Hoeffding&#39;s exponential advantage becomes dramatic: it provides $e^{-cn}$ decay versus Chebyshev&#39;s $1/n$ decay.</p>
<h3>When to Use Which Inequality?</h3>
<ul>
<li><strong>Markov:</strong> When you only know the mean and the variable is non-negative. The bound is often conservative but better than nothing.</li>
<li><strong>Chebyshev:</strong> When you know the variance but can&#39;t guarantee boundedness. Provides universal tail control for any finite-variance distribution.</li>
<li><strong>Hoeffding:</strong> When variables are bounded and independent. Gives exponential concentration bounds, particularly powerful for large $n$. Essential in machine learning generalization analysis and A/B testing.</li>
</ul>
<h2>Part 4: Properties of Foundational Distributions</h2>
<h3>The Memoryless Property: Does Waiting Longer Improve Your Chances?</h3>
<p><strong>Scenario:</strong> You&#39;ve been waiting at a bus stop for 30 minutes. Someone says, &quot;Don&#39;t worry, you&#39;ve waited so long that the bus must come soon!&quot; Is this comfort mathematically justified?</p>
<p>If bus arrival times follow an exponential distribution, this intuition is wrong. The exponential distribution has the <strong>memoryless property</strong>: past waiting doesn&#39;t affect future waiting.</p>
<p><strong>Mathematical Definition:</strong> For any $s, t \geq 0$:
$$\Pr(X &gt; s+t \mid X &gt; s) = \Pr(X &gt; t)$$</p>
<p>The probability of waiting an additional $t$ time units, given you&#39;ve already waited $s$ units, equals the probability of initially waiting $t$ units.</p>
<p><strong>Verification for Exponential Distribution:</strong> With $F(x) = 1 - e^{-\lambda x}$:
$$\Pr(X &gt; s+t \mid X &gt; s) = \frac{\Pr(X &gt; s+t)}{\Pr(X &gt; s)} = \frac{e^{-\lambda(s+t)}}{e^{-\lambda s}} = e^{-\lambda t} = \Pr(X &gt; t)$$</p>
<p><strong>Implications:</strong></p>
<ul>
<li>In queueing systems with exponential service times, the system has no &quot;memory&quot; of how long you&#39;ve waited</li>
<li>This greatly simplifies Markov process analysis</li>
<li>However, most real systems do exhibit aging effects, so exponential models are approximations</li>
</ul>
<h3>Working with the Standard Normal Distribution</h3>
<p><strong>Scenario:</strong> Statistics exams often ask: &quot;Given $X \sim N(\mu, \sigma^2)$, find $\Pr(X \leq a)$.&quot; Since normal distributions lack closed-form CDFs, we rely on standardization and tables.</p>
<p><strong>The Standardization Process:</strong></p>
<ol>
<li>Convert to standard normal: $Z = \frac{X - \mu}{\sigma}$, where $Z \sim N(0,1)$</li>
<li>Rewrite the probability: $\Pr(X \leq a) = \Pr\left(Z \leq \frac{a - \mu}{\sigma}\right)$</li>
<li>Use the standard normal table to find $\Phi(z) = \Pr(Z \leq z)$</li>
</ol>
<p><strong>Key Techniques:</strong></p>
<ul>
<li><strong>For negative values:</strong> Use symmetry: $\Pr(Z \leq -z) = \Pr(Z \geq z) = 1 - \Pr(Z \leq z)$</li>
<li><strong>For intervals:</strong> $\Pr(a &lt; X &lt; b) = \Pr(X \leq b) - \Pr(X \leq a)$</li>
<li><strong>Remember the 68-95-99.7 rule:</strong> Approximately 68%, 95%, and 99.7% of data falls within 1, 2, and 3 standard deviations, respectively</li>
</ul>
<h2>Part 5: The Linear Algebra Backbone</h2>
<p>Many advanced probability concepts, especially in multivariate settings or machine learning applications like Principal Component Analysis, rely heavily on linear algebra. Here&#39;s an intuitive review of key concepts.</p>
<h3>Understanding Matrix Properties Through Geometric Intuition</h3>
<p><strong>Scenario:</strong> Imagine a linear transformation $A$ acting on a 2D plane, stretching a square into a rectangle. What properties characterize this transformation?</p>
<ul>
<li><p><strong>Rank:</strong> The number of linearly independent rows/columns, measuring the dimensionality of the transformation&#39;s output space. Rank $r &lt; n$ means some dimensions are compressed (there exist non-zero vectors $v$ with $Av = 0$).</p>
</li>
<li><p><strong>Determinant:</strong> Measures volume scaling with sign indicating orientation preservation. $\det(A) = 0$ means the transformation compresses space to zero volume (rank deficiency).</p>
</li>
<li><p><strong>Eigenvalues and Eigenvectors:</strong> Special directions where the transformation only scales: $Av = \lambda v$. Eigenvalues $\lambda$ give scaling factors; eigenvectors $v$ show invariant directions.</p>
</li>
<li><p><strong>Trace:</strong> Sum of diagonal elements, which equals the sum of all eigenvalues: $\operatorname{tr}(A) = \lambda_1 + \lambda_2 + \cdots + \lambda_n$</p>
</li>
</ul>
<h3>Key Relationships</h3>
<p>For any $n \times n$ matrix $A$ with eigenvalues $\lambda_1, \ldots, \lambda_n$:</p>
<ul>
<li>$\operatorname{tr}(A) = \lambda_1 + \lambda_2 + \cdots + \lambda_n$ (sum of eigenvalues)</li>
<li>$\det(A) = \lambda_1 \cdot \lambda_2 \cdots \lambda_n$ (product of eigenvalues)</li>
<li>Rank = number of non-zero eigenvalues</li>
</ul>
<p>These relationships reveal deep connections: zero eigenvalues ‚ü∫ zero determinant ‚ü∫ rank deficiency ‚ü∫ some directions compressed to zero.</p>
<p><strong>Geometric Insight:</strong> For a diagonal matrix $A = \begin{pmatrix}3 &amp; 0\0 &amp; 2\end{pmatrix}$, the x-axis is stretched by factor 3, the y-axis by factor 2. Here, the coordinate axes are eigenvectors with eigenvalues 3 and 2. We have $\operatorname{tr}(A) = 5$, $\det(A) = 6$, and rank = 2.</p>
<h2>Conclusion</h2>
<p>This journey through probability theory reveals how seemingly simple concepts like &quot;convergence&quot; hide subtle distinctions with profound practical implications. Understanding when the Central Limit Theorem applies, choosing appropriate probability inequalities, and recognizing the memoryless property&#39;s implications are essential skills for modern data science and machine learning.</p>
<p>The interconnections between these concepts‚Äîfrom convergence modes through limit theorems to practical bounds‚Äîform the mathematical foundation that enables us to reason precisely about uncertainty. Whether you&#39;re analyzing A/B test results, building machine learning models, or designing experiments, these tools provide the rigorous framework needed to transform data into reliable insights.</p>
<p>As we continue to grapple with increasingly complex data and models, returning to these fundamental principles ensures our conclusions rest on solid mathematical ground rather than intuitive but potentially misleading heuristics.</p>

    </article>
    </main>
</div>

<script src="https://cdn.jsdelivr.net/npm/prismjs@1/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-python.min.js"></script>
<script>
Prism.highlightAll();
document.querySelectorAll('article pre').forEach(pre => {
    const code = pre.querySelector('code');
    if (!code) return;
    const toolbar = document.createElement('div');
    toolbar.className = 'code-toolbar';
    const btn = document.createElement('button');
    btn.className = 'copy-btn';
    btn.textContent = 'Copy';
    btn.onclick = () => {
        navigator.clipboard.writeText(code.textContent).then(() => {
            btn.textContent = 'Copied!';
            btn.classList.add('copied');
            setTimeout(() => { btn.textContent = 'Copy'; btn.classList.remove('copied'); }, 2000);
        });
    };
    toolbar.appendChild(btn);
    pre.parentNode.insertBefore(toolbar, pre);
});
</script>
</body>
</html>