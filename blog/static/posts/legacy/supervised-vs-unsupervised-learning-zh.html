<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>监督学习 vs 非监督学习：概念、算法与实践 | Yuxu Ge</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1/themes/prism-tomorrow.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <style>
        :root {
            --black: #111;
            --dark-grey: #444;
            --off-white: #f4f4f4;
            --vermilion: #C41E3A;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }

        html {
            font-size: 16px;
            scroll-behavior: smooth;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: var(--black);
            background-color: var(--off-white);
        }

        a { color: var(--vermilion); text-decoration: none; }
        a:hover { opacity: 0.75; }

        .layout { min-height: 100vh; }

        /* Sidebar */
        .sidebar {
            position: fixed; top: 0; left: 0;
            width: 320px; height: 100vh;
            background: var(--black); color: var(--off-white);
            padding: 3rem 2rem;
            display: flex; flex-direction: column; justify-content: space-between;
        }
        .sidebar-top { display: flex; flex-direction: column; align-items: center; text-align: center; }
        .avatar { display: block; width: 120px; height: 120px; border-radius: 50%; border: 3px solid var(--vermilion); margin-bottom: 1.5rem; overflow: hidden; transition: transform 0.2s ease; }
        .avatar:hover { opacity: 1; transform: scale(1.05); }
        .avatar img { width: 100%; height: 100%; object-fit: cover; }
        .name-block { margin-bottom: 0.5rem; }
        h1 { font-size: 1.8rem; font-weight: 700; letter-spacing: 1px; color: var(--off-white); }
        .title-role { font-size: 0.9rem; color: #888; margin-top: 0.25rem; font-weight: 400; }
        .contact-block { margin-top: 2.5rem; width: 100%; }
        .social-links { display: flex; flex-direction: column; align-items: center; gap: 0.75rem; }
        .social-links a { display: flex; align-items: center; gap: 0.75rem; width: 160px; color: var(--off-white); opacity: 0.7; padding: 0.5rem 0.75rem; border-radius: 4px; font-size: 0.9rem; transition: all 0.2s; }
        .social-links a:hover { color: var(--vermilion); opacity: 1; background: rgba(255,255,255,0.05); }
        .social-links svg { width: 20px; height: 20px; flex-shrink: 0; }
        .social-links .orcid-link { font-size: 0.68rem; gap: 0.5rem; white-space: nowrap; }
        .sidebar-footer { text-align: center; font-size: 0.75rem; color: #555; }

        /* Content */
        .content { margin-left: 320px; padding: 3rem 4rem; max-width: 900px; }
        .back-link { display: inline-block; font-size: 0.85rem; margin-bottom: 2rem; color: var(--dark-grey); }
        .back-link:hover { color: var(--vermilion); }

        /* Article */
        article h1 { font-size: 2rem; font-weight: 700; margin-bottom: 1.5rem; line-height: 1.3; color: var(--black); }
        article h2 { font-size: 1.4rem; font-weight: 600; margin: 2rem 0 1rem; color: var(--black); }
        article h3 { font-size: 1.1rem; font-weight: 600; margin: 1.5rem 0 0.75rem; color: var(--black); }
        article p { margin-bottom: 1rem; color: var(--dark-grey); }
        article ul, article ol { margin: 1rem 0 1rem 1.5rem; color: var(--dark-grey); }
        article li { margin-bottom: 0.5rem; }
        article strong { color: var(--black); }
        article code { font-family: "SF Mono", Monaco, monospace; font-size: 0.9em; background: #e8e8e8; padding: 0.15em 0.4em; border-radius: 3px; }
        article pre { margin: 1rem 0; border-radius: 6px; overflow-x: auto; background: #2d2d2d; padding: 1rem; }
        article pre code { background: none; padding: 0; font-size: 0.75em; white-space: pre !important; counter-reset: line; display: block; color: #ccc; }
        article pre code .line { counter-increment: line; }
        article pre code .line::before { content: counter(line); display: inline-block; width: 2.5em; margin-right: 1em; text-align: right; color: #666; user-select: none; }
        .code-toolbar { display: flex; justify-content: flex-start; padding: 0.35rem 0.5rem; background: #3a3a3a; border-radius: 6px 6px 0 0; }
        .code-toolbar + pre { margin-top: 0; border-radius: 0 0 6px 6px; }
        .copy-btn { padding: 0.2rem 0.5rem; font-size: 0.7rem; background: #555; color: #fff; border: none; border-radius: 3px; cursor: pointer; transition: all 0.2s; }
        .copy-btn:hover, .copy-btn:active { background: #777; }
        .copy-btn.copied { background: #2a2; }
        article blockquote { border-left: 3px solid var(--vermilion); padding-left: 1rem; margin: 1rem 0; color: #666; font-style: italic; }
        article hr { border: none; border-top: 1px solid #ddd; margin: 2rem 0; }
        article table { width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem; }
        article th, article td { border: 1px solid #ddd; padding: 0.5rem 0.75rem; text-align: left; }
        article th { background: #e8e8e8; font-weight: 600; }
        article img { max-width: 100%; height: auto; display: block; margin: 1.5rem 0; border-radius: 6px; }

        .loading { text-align: center; padding: 3rem; color: #888; }
        .error { color: var(--vermilion); }

        /* KaTeX math styles */
        .katex-display { overflow-x: auto; overflow-y: hidden; padding: 0.5rem 0; }
        .katex { font-size: 1.1em; }

        /* Responsive */
        @media (max-width: 900px) {
            .sidebar { position: relative; width: 100%; height: auto; padding: 2rem 1.5rem 1rem; }
            .social-links { flex-direction: row; flex-wrap: wrap; justify-content: center; }
            .social-links a, .social-links .orcid-link { width: auto; padding: 0.5rem; font-size: 0; gap: 0; }
            .social-links svg { width: 24px; height: 24px; }
            .social-links .hide-mobile { display: none; }
            .sidebar-footer { margin-top: 1rem; }
            .content { margin-left: 0; padding: 2rem 1rem; }
        }
        @media (max-width: 480px) {
            .sidebar { padding: 1.5rem 1rem 1rem; }
            .avatar { width: 80px; height: 80px; }
            h1 { font-size: 1.4rem; }
            article h1 { font-size: 1.5rem; }
        }
    </style>
</head>
<body>

<div class="layout">
    <aside class="sidebar" id="sidebar-content"></aside>
    <script src="/components/sidebar.js"></script>

    <main class="content">
        <a href="/blog/" class="back-link">← Back to Blog</a>
        <article>
<hr>
<h2>date: 2024-01-01
tags: [ai, 机器学习, 监督学习, 非监督学习, ai]
legacy: true</h2>
<h1>监督学习 vs 非监督学习：概念、算法与实践</h1>
<hr>
<h2>2. 核心概念与差异</h2>
<h3>2.1 监督学习（Supervised Learning）</h3>
<p>监督学习的核心思想是<strong>从“有标签”的数据中学习</strong>。这里的“标签”（Label）是我们希望模型预测的正确答案。</p>
<ul>
<li><strong>定义</strong>：给定一组输入数据 <code>X</code> 和其对应的输出标签 <code>y</code>，算法的目标是学习一个映射函数 <code>f</code>，使得 <code>f(x) ≈ y</code>。</li>
<li><strong>目标</strong>：最小化模型预测值与真实标签之间的<strong>可度量误差</strong>，例如均方误差（MSE）用于回归，交叉熵（Cross-Entropy）用于分类。</li>
<li><strong>典型任务</strong>：<ul>
<li><strong>分类（Classification）</strong>：预测离散的类别标签。例如，判断一封邮件是否为垃圾邮件（二分类），或者识别一张图片中的动物是猫、狗还是鸟（多分类）。</li>
<li><strong>回归（Regression）</strong>：预测连续的数值。例如，根据房屋特征预测其售价，或者根据历史数据预测未来一天的气温。</li>
</ul>
</li>
</ul>
<h3>2.2 非监督学习（Unsupervised Learning）</h3>
<p>与监督学习相反，非监督学习处理的是**“无标签”的数据**。它不需要人工标注的答案，而是致力于发现数据自身内在的结构和模式。</p>
<ul>
<li><strong>定义</strong>：仅给定输入数据 <code>X</code>，算法的目标是挖掘数据中隐藏的结构。</li>
<li><strong>目标</strong>：探索数据的内在规律，如<strong>相似性、密度、潜在因子</strong>等。</li>
<li><strong>典型任务</strong>：<ul>
<li><strong>聚类（Clustering）</strong>：将相似的数据点分到同一个簇（Cluster）。例如，根据用户的购买行为将其划分为不同群体（高价值用户、潜力用户等）。</li>
<li><strong>降维（Dimensionality Reduction）/可视化</strong>：在保留核心信息的前提下，减少数据的特征数量。例如，将高维的用户画像数据压缩到二维平面上进行可视化。</li>
<li><strong>密度估计（Density Estimation）/生成建模</strong>：学习数据的分布，从而可以生成新的、与原始数据相似的样本。例如，生成逼真的人脸图像。</li>
</ul>
</li>
</ul>
<h3>2.3 差异一览表</h3>
<table>
<thead>
<tr>
<th align="left">维度</th>
<th align="left">监督学习</th>
<th align="left">非监督学习</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>训练数据</strong></td>
<td align="left">有标签 (X, y)</td>
<td align="left">无标签 (X)</td>
</tr>
<tr>
<td align="left"><strong>主要目标</strong></td>
<td align="left">预测明确的输出</td>
<td align="left">发现隐藏的结构</td>
</tr>
<tr>
<td align="left"><strong>评价方式</strong></td>
<td align="left">对比真实标签 (准确率, RMSE, F1-Score...)</td>
<td align="left">间接指标 (轮廓系数, 重构误差...)</td>
</tr>
<tr>
<td align="left"><strong>常见风险</strong></td>
<td align="left">过拟合、高昂的标注成本</td>
<td align="left">结果难以解释、评价标准模糊</td>
</tr>
</tbody></table>
<hr>
<h2>3. 工作流程对比</h2>
<h3>3.1 监督学习流水线</h3>
<p>一个典型的监督学习项目遵循一个相对标准化的流程：</p>
<ol>
<li><strong>数据标注与划分</strong>：获取或标注高质量的标签数据，并将其划分为训练集、验证集和测试集。</li>
<li><strong>特征工程与模型选择</strong>：根据业务理解提取有效特征，并选择合适的模型（如线性模型、树模型或神经网络）。</li>
<li><strong>训练与调优</strong>：在训练集上训练模型，并在验证集上调整超参数（如学习率、树的深度）。</li>
<li><strong>评估与上线</strong>：在测试集上评估最终模型的性能，达到标准后部署上线。</li>
<li><strong>监控与迭代</strong>：持续监控线上模型的表现，警惕“概念漂移”（数据分布变化），并定期使用新数据进行再训练。</li>
</ol>
<h3>3.2 非监督学习流水线</h3>
<p>非监督学习的流程更具探索性：</p>
<ol>
<li><strong>数据预处理</strong>：数据标准化或归一化至关重要，因为许多算法（如 K-means、PCA）对尺度敏感。同时需要选择合适的距离度量方式（如欧氏距离、余弦相似度）。</li>
<li><strong>算法与超参数探索</strong>：选择合适的算法（如 K-means、DBSCAN），并探索其关键超参数（如簇的数量 <code>k</code>、邻域半径 <code>ε</code>）。</li>
<li><strong>结果可视化与业务验证</strong>：由于没有“正确答案”，通常需要将结果（如聚类簇、降维图）可视化，并结合业务知识来验证其有效性和可解释性。</li>
<li><strong>下游应用</strong>：非监督学习的结果往往作为下游任务的输入。例如，将聚类结果作为用户标签，或将降维后的特征用于后续的监督学习模型。</li>
</ol>
<hr>
<h2>4. 典型算法速览</h2>
<h3>4.1 监督学习算法</h3>
<table>
<thead>
<tr>
<th align="left">算法</th>
<th align="left">一句话简介</th>
<th align="left">关键特点 / 适用场景</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>线性回归</strong></td>
<td align="left">最小化预测值与真实值之间的平方误差来拟合一条直线。</td>
<td align="left">解释性强，是许多复杂模型的基线；常用于房价、销量预测。</td>
</tr>
<tr>
<td align="left"><strong>逻辑回归</strong></td>
<td align="left">使用 Sigmoid 函数将线性输出映射到 (0,1) 区间，用于二分类。</td>
<td align="left">输出概率，易于理解和实现；广泛用于 CTR 预估、信用评分。</td>
</tr>
<tr>
<td align="left"><strong>决策树 (CART)</strong></td>
<td align="left">通过递归地将数据划分到不同节点来构建一棵树，以提升节点的“纯度”。</td>
<td align="left">规则直观，能处理非线性和缺失值，但容易过拟合。</td>
</tr>
<tr>
<td align="left"><strong>随机森林</strong></td>
<td align="left">通过构建并结合多棵决策树的投票结果来提升性能。</td>
<td align="left">有效抵抗过拟合，能评估特征重要性，是强大的基线模型。</td>
</tr>
<tr>
<td align="left"><strong>支持向量机 (SVM)</strong></td>
<td align="left">寻找一个能以最大间隔将不同类别分开的超平面，并通过核技巧处理非线性问题。</td>
<td align="left">在小样本、高维数据集上效果显著；常用于文本分类、图像识别。</td>
</tr>
<tr>
<td align="left"><strong>Boosting (XGBoost/LightGBM)</strong></td>
<td align="left">逐步迭代，每一轮都专注于拟合前一轮留下的残差，将弱学习器叠加为强模型。</td>
<td align="left">在表格数据（Tabular Data）上达到顶尖水平，特征工程友好。</td>
</tr>
<tr>
<td align="left"><strong>深度网络 (CNN/Transformer)</strong></td>
<td align="left">通过多层非线性变换自动学习数据的层次化特征。</td>
<td align="left"><strong>CNN</strong> 擅长捕捉局部空间特征（图像），<strong>Transformer</strong> 擅长处理序列数据的全局依赖（文本、语音）。</td>
</tr>
</tbody></table>
<h3>4.2 非监督学习算法</h3>
<table>
<thead>
<tr>
<th align="left">算法</th>
<th align="left">一句话简介</th>
<th align="left">关键特点 / 典型场景</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>K-means</strong></td>
<td align="left">迭代地更新簇中心点，以最小化每个数据点到其所属簇中心的距离平方和。</td>
<td align="left">简单高效，但需预先指定簇数 <code>k</code> 且对初始值敏感；常用于用户分群。</td>
</tr>
<tr>
<td align="left"><strong>DBSCAN</strong></td>
<td align="left">基于密度来定义簇，能够自动识别噪声点并发现任意形状的簇。</td>
<td align="left">无需预设簇数 <code>k</code>，对噪声不敏感；适用于地理空间数据分析。</td>
</tr>
<tr>
<td align="left"><strong>层次聚类</strong></td>
<td align="left">通过不断合并（自底向上）或拆分（自顶向下）数据点来形成一个树状的簇结构。</td>
<td align="left">无需预设 <code>k</code>，可以得到一个谱系图，有助于理解数据层次；用于物种进化分析。</td>
</tr>
<tr>
<td align="left"><strong>PCA</strong></td>
<td align="left">通过线性变换将数据投影到方差最大的几个正交方向上。</td>
<td align="left">最经典的降维方法，用于数据压缩、去噪和可视化。</td>
</tr>
<tr>
<td align="left"><strong>t-SNE / UMAP</strong></td>
<td align="left">通过非线性嵌入，在低维空间中保持高维数据的局部邻域结构。</td>
<td align="left">是高维数据（如文本、基因）可视化的利器，效果优于 PCA。</td>
</tr>
<tr>
<td align="left"><strong>高斯混合模型 (GMM)</strong></td>
<td align="left">假设数据由多个高斯分布混合而成，通过期望最大化（EM）算法进行软聚类。</td>
<td align="left">能够处理更复杂的簇形状（椭圆），并输出数据点属于各簇的概率。</td>
</tr>
<tr>
<td align="left"><strong>核密度估计 (KDE)</strong></td>
<td align="left">通过在每个数据点上放置一个核函数（如高斯核）来平滑地估计数据的概率密度函数。</td>
<td align="left">用于数据分布可视化、异常检测。</td>
</tr>
<tr>
<td align="left"><strong>生成对抗网络 (GAN)</strong></td>
<td align="left">一个生成器和一个判别器相互对抗，生成器努力创造逼真数据，判别器努力区分真假。</td>
<td align="left">在图像合成、数据增强领域效果惊人。</td>
</tr>
<tr>
<td align="left"><strong>变分自编码器 (VAE)</strong></td>
<td align="left">将输入编码到一个潜在分布中，再从该分布中采样进行解码重构，是一种生成模型。</td>
<td align="left">能够生成可控的新样本，其潜在变量具有一定的语义解释性。</td>
</tr>
</tbody></table>
<hr>
<h2>5. 场景与案例</h2>
<table>
<thead>
<tr>
<th align="left">任务</th>
<th align="left">方法范式</th>
<th align="left">示例</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>医学影像诊断</strong></td>
<td align="left"><strong>监督学习</strong> → CNN/Transformer</td>
<td align="left">输入 CT 图像，模型自动分类病灶区域（如肿瘤、结节）。</td>
</tr>
<tr>
<td align="left"><strong>电商用户分群</strong></td>
<td align="left"><strong>非监督学习</strong> → K-means/DBSCAN</td>
<td align="left">基于用户的浏览、加购、购买行为日志，将用户划分为不同价值群体。</td>
</tr>
<tr>
<td align="left"><strong>风格化图像生成</strong></td>
<td align="left"><strong>非监督学习</strong> → GAN/VAE</td>
<td align="left">输入一张普通照片，生成梵高或水墨画风格的艺术图像。</td>
</tr>
<tr>
<td align="left"><strong>半监督文本分类</strong></td>
<td align="left"><strong>自监督预训练 + 监督微调</strong></td>
<td align="left">使用海量无标签文本进行自监督学习（如 BERT），再用少量有标签数据进行微调，即可达到很高的分类精度。这是现代 NLP 的主流范式。</td>
</tr>
</tbody></table>
<hr>
<h2>6. 拓展范式</h2>
<p>监督与非监督并非泾渭分明，实践中涌现了许多强大的混合范式：</p>
<ul>
<li><strong>半监督学习（Semi-supervised Learning）</strong>：当拥有少量有标签数据和大量无标签数据时，通过伪标签（Pseudo-Labeling）、一致性正则化等技术，利用无标签数据提升模型性能。</li>
<li><strong>弱监督学习（Weakly Supervised Learning）</strong>：标签不完全准确或不完整（例如，只知道一张图里有猫，但不知道猫的具体位置）。</li>
<li><strong>自监督学习（Self-supervised Learning）</strong>：从数据自身构造“伪任务”来生成标签。例如，在文本中随机遮盖一个词（Masked Language Model），让模型去预测它，这是 BERT 等预训练语言模型的核心思想。</li>
<li><strong>强化学习（Reinforcement Learning）</strong>：智能体（Agent）通过与环境交互，根据获得的奖励或惩罚来学习最优策略。它常与监督学习结合使用，如 AlphaGo。</li>
</ul>
<hr>
<h2>7. 选型指南 &amp; 实战技巧</h2>
<ol>
<li><p><strong>从数据和标签出发</strong>：</p>
<ul>
<li><strong>有高质量标签</strong>：首选监督学习。</li>
<li><strong>标签获取成本高昂</strong>：优先考虑非监督学习进行数据探索（聚类、可视化），或使用自监督/半监督方法减少对标签的依赖。</li>
</ul>
</li>
<li><p><strong>考虑模型规模与数据复杂度</strong>：</p>
<ul>
<li><strong>大规模感知任务（图像、语音、文本）</strong>：深度学习网络是最佳选择。</li>
<li><strong>小样本、高维度数据</strong>：SVM 或树模型（如随机森林）可能表现更佳。</li>
<li><strong>结构化/表格数据</strong>：XGBoost/LightGBM 通常是性能之王。</li>
</ul>
</li>
<li><p><strong>平衡可解释性与精度</strong>：</p>
<ul>
<li><strong>金融风控、医疗等高风险或需合规的场景</strong>：线性模型、逻辑回归或决策树因其良好的可解释性而备受青睐。</li>
<li><strong>互联网广告、推荐等追求极致效果的场景</strong>：精度更高的复杂模型（如深度网络）是首选。</li>
</ul>
</li>
<li><p><strong>结合离线探索与在线应用</strong>：</p>
<ul>
<li>一个常见的模式是：先用<strong>非监督学习</strong>在离线数据上进行探索性分析，发现潜在的用户群体或数据模式。然后，将这些发现作为特征或目标，用于构建<strong>监督学习</strong>模型，并部署到线上提供实时预测服务。</li>
</ul>
</li>
</ol>
<hr>
<h2>8. 总结</h2>
<p>监督学习与非监督学习是解决不同问题的两种强大工具，它们的核心区别在于是否依赖“标准答案”。</p>
<ul>
<li><strong>监督学习擅长“回答已知问题”</strong>：在明确的目标和高质量的标签驱动下，它能做出精准的预测。</li>
<li><strong>非监督学习擅长“发现未知问题”</strong>：在没有先验知识的情况下，它能揭示数据中隐藏的结构、模式和洞见。</li>
</ul>
<p>在真实的机器学习项目中，两者往往不是孤立的。最强大的解决方案常常是将它们结合起来：<strong>先用非监督学习探索数据的可能性，再用监督学习实现精准的目标建模，形成一个从数据洞察到价值创造的完整闭环。</strong></p>
<hr>
<h2>9. 代码示例</h2>
<h3>9.1 环境准备</h3>
<pre><code class="language-bash"><span class="line">pip install scikit-learn matplotlib torch torchvision</span></code></pre>
<h3>9.2 监督学习示例</h3>
<h4>线性回归 (Boston Housing)</h4>
<p><em>注意：<code>load_boston</code> 在 scikit-learn 1.2 版本后被移除，这里仅作经典示例。可替换为其他回归数据集。</em></p>
<pre><code class="language-python"><span class="line">from sklearn.datasets import fetch_california_housing</span>
<span class="line">from sklearn.model_selection import train_test_split</span>
<span class="line">from sklearn.linear_model import LinearRegression</span>
<span class="line">from sklearn.metrics import mean_squared_error</span>
<span class="line"></span>
<span class="line"># 加州房价数据集</span>
<span class="line">X, y = fetch_california_housing(return_X_y=True)</span>
<span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span>
<span class="line"></span>
<span class="line">model = LinearRegression().fit(X_train, y_train)</span>
<span class="line">pred = model.predict(X_test)</span>
<span class="line">print(f&quot;RMSE on California Housing: {mean_squared_error(y_test, pred, squared=False):.2f}&quot;)</span></code></pre>
<h4>逻辑回归 (乳腺癌二分类)</h4>
<pre><code class="language-python"><span class="line">from sklearn.datasets import load_breast_cancer</span>
<span class="line">from sklearn.linear_model import LogisticRegression</span>
<span class="line">from sklearn.preprocessing import StandardScaler</span>
<span class="line"></span>
<span class="line">X, y = load_breast_cancer(return_X_y=True)</span>
<span class="line"># 归一化提升性能</span>
<span class="line">scaler = StandardScaler()</span>
<span class="line">X_scaled = scaler.fit_transform(X)</span>
<span class="line"></span>
<span class="line">clf = LogisticRegression(max_iter=1000).fit(X_scaled, y)</span>
<span class="line">print(f&quot;Accuracy on Breast Cancer: {clf.score(X_scaled, y):.3f}&quot;)</span></code></pre>
<h3>9.3 非监督学习示例</h3>
<h4>K-means 聚类 + 可视化</h4>
<pre><code class="language-python"><span class="line">from sklearn.datasets import load_iris</span>
<span class="line">from sklearn.cluster import KMeans</span>
<span class="line">import matplotlib.pyplot as plt</span>
<span class="line"></span>
<span class="line">X, y = load_iris(return_X_y=True) # y 在这里只用于后续对比，K-means本身不用</span>
<span class="line">kmeans = KMeans(n_clusters=3, random_state=42, n_init=10).fit(X) # n_init=&#39;auto&#39; in future</span>
<span class="line"></span>
<span class="line"># 可视化前两个特征</span>
<span class="line">plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap=&#39;viridis&#39;)</span>
<span class="line">plt.title(&#39;K-means Clustering on Iris Dataset&#39;)</span>
<span class="line">plt.xlabel(&#39;Sepal Length&#39;)</span>
<span class="line">plt.ylabel(&#39;Sepal Width&#39;)</span>
<span class="line">plt.show()</span></code></pre>
<h4>PCA + t-SNE 可视化</h4>
<pre><code class="language-python"><span class="line">from sklearn.decomposition import PCA</span>
<span class="line">from sklearn.manifold import TSNE</span>
<span class="line">import matplotlib.pyplot as plt</span>
<span class="line">from sklearn.datasets import load_iris</span>
<span class="line"></span>
<span class="line">X, y = load_iris(return_X_y=True)</span>
<span class="line"></span>
<span class="line"># 先用PCA降到合理的中间维度</span>
<span class="line">X_reduced = PCA(n_components=50, random_state=42).fit_transform(X) if X.shape[1] &gt; 50 else X</span>
<span class="line"></span>
<span class="line"># 再用t-SNE进行非线性降维以可视化</span>
<span class="line">X_embedded = TSNE(n_components=2, learning_rate=&#39;auto&#39;, init=&#39;pca&#39;, random_state=42).fit_transform(X_reduced)</span>
<span class="line"></span>
<span class="line">plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, cmap=&#39;viridis&#39;) # 用真实标签y着色以验证效果</span>
<span class="line">plt.title(&#39;t-SNE Visualization of Iris Dataset&#39;)</span>
<span class="line">plt.xlabel(&#39;t-SNE feature 1&#39;)</span>
<span class="line">plt.ylabel(&#39;t-SNE feature 2&#39;)</span>
<span class="line">plt.show()</span></code></pre>
<h3>9.4 简易 GAN 骨架 (PyTorch)</h3>
<p>这是一个极简的 GAN 结构，用于演示其核心组件，并非一个完整的训练脚本。</p>
<pre><code class="language-python"><span class="line">import torch</span>
<span class="line">from torch import nn</span>
<span class="line"></span>
<span class="line"># 定义生成器</span>
<span class="line">class Generator(nn.Module):</span>
<span class="line">    def __init__(self, z_dim=100, img_dim=784):</span>
<span class="line">        super().__init__()</span>
<span class="line">        self.net = nn.Sequential(</span>
<span class="line">            nn.Linear(z_dim, 256),</span>
<span class="line">            nn.ReLU(True),</span>
<span class="line">            nn.Linear(256, 512),</span>
<span class="line">            nn.ReLU(True),</span>
<span class="line">            nn.Linear(512, img_dim),</span>
<span class="line">            nn.Tanh()  # 将输出归一化到[-1, 1]</span>
<span class="line">        )</span>
<span class="line">    def forward(self, z):</span>
<span class="line">        return self.net(z)</span>
<span class="line"></span>
<span class="line"># 定义判别器</span>
<span class="line">class Discriminator(nn.Module):</span>
<span class="line">    def __init__(self, img_dim=784):</span>
<span class="line">        super().__init__()</span>
<span class="line">        self.net = nn.Sequential(</span>
<span class="line">            nn.Linear(img_dim, 512),</span>
<span class="line">            nn.LeakyReLU(0.2, inplace=True),</span>
<span class="line">            nn.Linear(512, 256),</span>
<span class="line">            nn.LeakyReLU(0.2, inplace=True),</span>
<span class="line">            nn.Linear(256, 1),</span>
<span class="line">            nn.Sigmoid() # 输出一个[0, 1]的概率值</span>
<span class="line">        )</span>
<span class="line">    def forward(self, x):</span>
<span class="line">        return self.net(x)</span>
<span class="line"></span>
<span class="line"># 初始化模型、优化器和损失函数</span>
<span class="line">G = Generator()</span>
<span class="line">D = Discriminator()</span>
<span class="line">g_opt = torch.optim.Adam(G.parameters(), lr=2e-4)</span>
<span class="line">d_opt = torch.optim.Adam(D.parameters(), lr=2e-4)</span>
<span class="line">criterion = nn.BCELoss()</span>
<span class="line"></span>
<span class="line">print(&quot;GAN components initialized successfully.&quot;)</span></code></pre>
<hr>
<h2>10. 参考资料</h2>
<ul>
<li><em>Pattern Recognition and Machine Learning</em> — Christopher M. Bishop</li>
<li><em>Deep Learning</em> — Ian Goodfellow, Yoshua Bengio, and Aaron Courville</li>
<li><a href="https://scikit-learn.org/stable/documentation.html">Scikit-learn 官方文档</a></li>
<li><a href="https://pytorch.org/docs/stable/index.html">PyTorch 官方文档</a></li>
</ul>

    </article>
    </main>
</div>

<script src="https://cdn.jsdelivr.net/npm/prismjs@1/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-python.min.js"></script>
<script>
Prism.highlightAll();
document.querySelectorAll('article pre').forEach(pre => {
    const code = pre.querySelector('code');
    if (!code) return;
    const toolbar = document.createElement('div');
    toolbar.className = 'code-toolbar';
    const btn = document.createElement('button');
    btn.className = 'copy-btn';
    btn.textContent = 'Copy';
    btn.onclick = () => {
        navigator.clipboard.writeText(code.textContent).then(() => {
            btn.textContent = 'Copied!';
            btn.classList.add('copied');
            setTimeout(() => { btn.textContent = 'Copy'; btn.classList.remove('copied'); }, 2000);
        });
    };
    toolbar.appendChild(btn);
    pre.parentNode.insertBefore(toolbar, pre);
});
</script>
</body>
</html>