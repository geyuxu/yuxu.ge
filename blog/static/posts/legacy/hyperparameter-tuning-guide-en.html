<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Comprehensive Guide to Hyperparameter Tuning in Machine Learning: From Theory to Practice | Yuxu Ge</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1/themes/prism-tomorrow.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <style>
        :root {
            --black: #111;
            --dark-grey: #444;
            --off-white: #f4f4f4;
            --vermilion: #C41E3A;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }

        html {
            font-size: 16px;
            scroll-behavior: smooth;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: var(--black);
            background-color: var(--off-white);
        }

        a { color: var(--vermilion); text-decoration: none; }
        a:hover { opacity: 0.75; }

        .layout { min-height: 100vh; }

        /* Sidebar */
        .sidebar {
            position: fixed; top: 0; left: 0;
            width: 320px; height: 100vh;
            background: var(--black); color: var(--off-white);
            padding: 3rem 2rem;
            display: flex; flex-direction: column; justify-content: space-between;
        }
        .sidebar-top { display: flex; flex-direction: column; align-items: center; text-align: center; }
        .avatar { display: block; width: 120px; height: 120px; border-radius: 50%; border: 3px solid var(--vermilion); margin-bottom: 1.5rem; overflow: hidden; transition: transform 0.2s ease; }
        .avatar:hover { opacity: 1; transform: scale(1.05); }
        .avatar img { width: 100%; height: 100%; object-fit: cover; }
        .name-block { margin-bottom: 0.5rem; }
        h1 { font-size: 1.8rem; font-weight: 700; letter-spacing: 1px; color: var(--off-white); }
        .title-role { font-size: 0.9rem; color: #888; margin-top: 0.25rem; font-weight: 400; }
        .contact-block { margin-top: 2.5rem; width: 100%; }
        .social-links { display: flex; flex-direction: column; align-items: center; gap: 0.75rem; }
        .social-links a { display: flex; align-items: center; gap: 0.75rem; width: 160px; color: var(--off-white); opacity: 0.7; padding: 0.5rem 0.75rem; border-radius: 4px; font-size: 0.9rem; transition: all 0.2s; }
        .social-links a:hover { color: var(--vermilion); opacity: 1; background: rgba(255,255,255,0.05); }
        .social-links svg { width: 20px; height: 20px; flex-shrink: 0; }
        .social-links .orcid-link { font-size: 0.68rem; gap: 0.5rem; white-space: nowrap; }
        .sidebar-footer { text-align: center; font-size: 0.75rem; color: #555; }

        /* Content */
        .content { margin-left: 320px; padding: 3rem 4rem; max-width: 900px; }
        .back-link { display: inline-block; font-size: 0.85rem; margin-bottom: 2rem; color: var(--dark-grey); }
        .back-link:hover { color: var(--vermilion); }

        /* Article */
        article h1 { font-size: 2rem; font-weight: 700; margin-bottom: 1.5rem; line-height: 1.3; color: var(--black); }
        article h2 { font-size: 1.4rem; font-weight: 600; margin: 2rem 0 1rem; color: var(--black); }
        article h3 { font-size: 1.1rem; font-weight: 600; margin: 1.5rem 0 0.75rem; color: var(--black); }
        article p { margin-bottom: 1rem; color: var(--dark-grey); }
        article ul, article ol { margin: 1rem 0 1rem 1.5rem; color: var(--dark-grey); }
        article li { margin-bottom: 0.5rem; }
        article strong { color: var(--black); }
        article code { font-family: "SF Mono", Monaco, monospace; font-size: 0.9em; background: #e8e8e8; padding: 0.15em 0.4em; border-radius: 3px; }
        article pre { margin: 1rem 0; border-radius: 6px; overflow-x: auto; background: #2d2d2d; padding: 1rem; }
        article pre code { background: none; padding: 0; font-size: 0.75em; white-space: pre !important; counter-reset: line; display: block; color: #ccc; }
        article pre code .line { counter-increment: line; }
        article pre code .line::before { content: counter(line); display: inline-block; width: 2.5em; margin-right: 1em; text-align: right; color: #666; user-select: none; }
        .code-toolbar { display: flex; justify-content: flex-start; padding: 0.35rem 0.5rem; background: #3a3a3a; border-radius: 6px 6px 0 0; }
        .code-toolbar + pre { margin-top: 0; border-radius: 0 0 6px 6px; }
        .copy-btn { padding: 0.2rem 0.5rem; font-size: 0.7rem; background: #555; color: #fff; border: none; border-radius: 3px; cursor: pointer; transition: all 0.2s; }
        .copy-btn:hover, .copy-btn:active { background: #777; }
        .copy-btn.copied { background: #2a2; }
        article blockquote { border-left: 3px solid var(--vermilion); padding-left: 1rem; margin: 1rem 0; color: #666; font-style: italic; }
        article hr { border: none; border-top: 1px solid #ddd; margin: 2rem 0; }
        article table { width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem; }
        article th, article td { border: 1px solid #ddd; padding: 0.5rem 0.75rem; text-align: left; }
        article th { background: #e8e8e8; font-weight: 600; }
        article img { max-width: 100%; height: auto; display: block; margin: 1.5rem 0; border-radius: 6px; }

        .loading { text-align: center; padding: 3rem; color: #888; }
        .error { color: var(--vermilion); }

        /* KaTeX math styles */
        .katex-display { overflow-x: auto; overflow-y: hidden; padding: 0.5rem 0; }
        .katex { font-size: 1.1em; }

        /* Responsive */
        @media (max-width: 900px) {
            .sidebar { position: relative; width: 100%; height: auto; padding: 2rem 1.5rem 1rem; }
            .social-links { flex-direction: row; flex-wrap: wrap; justify-content: center; }
            .social-links a, .social-links .orcid-link { width: auto; padding: 0.5rem; font-size: 0; gap: 0; }
            .social-links svg { width: 24px; height: 24px; }
            .social-links .hide-mobile { display: none; }
            .sidebar-footer { margin-top: 1rem; }
            .content { margin-left: 0; padding: 2rem 1rem; }
        }
        @media (max-width: 480px) {
            .sidebar { padding: 1.5rem 1rem 1rem; }
            .avatar { width: 80px; height: 80px; }
            h1 { font-size: 1.4rem; }
            article h1 { font-size: 1.5rem; }
        }
    </style>
</head>
<body>

<div class="layout">
    <aside class="sidebar" id="sidebar-content"></aside>
    <script src="/components/sidebar.js"></script>

    <main class="content">
        <a href="/blog/" class="back-link">← Back to Blog</a>
        <article>
<hr>
<h2>date: 2024-01-01
tags: [ai, machine learning, hyperparameter tuning, python, pytorch, optuna, scikit-learn]
legacy: true</h2>
<h1>A Comprehensive Guide to Hyperparameter Tuning in Machine Learning: From Theory to Practice</h1>
<h2>2. Tuning Methods: Overview and Code Examples</h2>
<p>Choosing the right tuning strategy is an art of balancing exploration (covering a wider search space) and exploitation (finer search in promising areas). Let&#39;s delve into the mainstream methods one by one, complete with runnable code snippets.</p>
<h3>2.1 Grid Search</h3>
<p><strong>Core Idea</strong>: A brute-force enumeration of all possible combinations (Cartesian product) of the provided hyperparameter values. It&#39;s the most reliable method when the number of dimensions is small.</p>
<pre><code class="language-python"><span class="line"># pip install scikit-learn</span>
<span class="line">from sklearn.model_selection import GridSearchCV</span>
<span class="line">from sklearn.svm import SVC</span>
<span class="line"></span>
<span class="line"># Assume X_train, y_train are loaded</span>
<span class="line"># from sklearn.datasets import make_classification</span>
<span class="line"># X_train, y_train = make_classification(n_samples=1000, n_features=20, n_informative=10, random_state=42)</span>
<span class="line"></span>
<span class="line">param_grid = {</span>
<span class="line">    &quot;C&quot;:     [0.1, 1, 10],</span>
<span class="line">    &quot;gamma&quot;: [1, 0.1, 0.01],</span>
<span class="line">    &quot;kernel&quot;: [&quot;rbf&quot;]</span>
<span class="line">}</span>
<span class="line">grid = GridSearchCV(</span>
<span class="line">    estimator=SVC(),</span>
<span class="line">    param_grid=param_grid,</span>
<span class="line">    cv=5,               # 5-fold cross-validation</span>
<span class="line">    scoring=&quot;accuracy&quot;,</span>
<span class="line">    n_jobs=-1           # Use all available CPU cores</span>
<span class="line">)</span>
<span class="line"># grid.fit(X_train, y_train)</span>
<span class="line"># print(f&quot;Best Params: {grid.best_params_}&quot;)</span>
<span class="line"># print(f&quot;Best Score: {grid.best_score_:.4f}&quot;)</span></code></pre>
<p><strong>Tip</strong>: Start with a coarse grid to scan a wide range, then define a finer, more localized grid around the best-performing region.</p>
<h3>2.2 Random Search</h3>
<p><strong>Core Idea</strong>: Unlike Grid Search, Random Search samples a fixed number of parameter combinations from specified distributions. It is generally more efficient than Grid Search, especially when dealing with a large number of hyperparameters.</p>
<pre><code class="language-python"><span class="line"># pip install scikit-learn scipy</span>
<span class="line">from sklearn.model_selection import RandomizedSearchCV</span>
<span class="line">from sklearn.ensemble import GradientBoostingClassifier</span>
<span class="line">from scipy.stats import loguniform, randint</span>
<span class="line"></span>
<span class="line"># Assume X_train, y_train are loaded</span>
<span class="line">param_dist = {</span>
<span class="line">    &quot;learning_rate&quot;: loguniform(1e-4, 1e-1),</span>
<span class="line">    &quot;n_estimators&quot;:  randint(100, 1000),</span>
<span class="line">    &quot;max_depth&quot;:     randint(2, 6)</span>
<span class="line">}</span>
<span class="line">rs = RandomizedSearchCV(</span>
<span class="line">    GradientBoostingClassifier(),</span>
<span class="line">    param_distributions=param_dist,</span>
<span class="line">    n_iter=60,          # Sample 60 combinations</span>
<span class="line">    cv=5,</span>
<span class="line">    random_state=42,</span>
<span class="line">    n_jobs=-1</span>
<span class="line">)</span>
<span class="line"># rs.fit(X_train, y_train)</span>
<span class="line"># print(f&quot;Best Params: {rs.best_params_}&quot;)</span>
<span class="line"># print(f&quot;Best Score: {rs.best_score_:.4f}&quot;)</span></code></pre>
<p><strong>Key Insight</strong>: For hyperparameters sensitive to their order of magnitude, like learning rate and regularization strength, sampling from a log-uniform distribution (<code>loguniform</code>) is more effective than a linear uniform distribution.</p>
<h3>2.3 Bayesian Optimization</h3>
<p><strong>Core Idea</strong>: This is a more intelligent search strategy. It uses a probabilistic model (a surrogate) to model the hyperparameter-to-performance function and leverages historical evaluation results to select the next most promising point to evaluate. This allows it to approach the optimal solution in fewer iterations. <code>Optuna</code> is a popular library for this.</p>
<pre><code class="language-python"><span class="line"># pip install optuna torch torchvision</span>
<span class="line">import torch</span>
<span class="line">import torch.nn as nn</span>
<span class="line">import optuna</span>
<span class="line">from torchvision.datasets import MNIST</span>
<span class="line">from torch.utils.data import DataLoader</span>
<span class="line">from torchvision import transforms</span>
<span class="line"></span>
<span class="line">DEVICE = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;</span>
<span class="line"></span>
<span class="line">def objective(trial):</span>
<span class="line">    # 1. Sample hyperparameters</span>
<span class="line">    lr = trial.suggest_loguniform(&quot;lr&quot;, 1e-4, 1e-1)</span>
<span class="line">    dropout = trial.suggest_uniform(&quot;dropout&quot;, 0.1, 0.5)</span>
<span class="line">    hidden = trial.suggest_categorical(&quot;hidden&quot;, [64, 128, 256])</span>
<span class="line"></span>
<span class="line">    # 2. Build the model</span>
<span class="line">    model = nn.Sequential(</span>
<span class="line">        nn.Flatten(),</span>
<span class="line">        nn.Linear(28*28, hidden), nn.ReLU(), nn.Dropout(dropout),</span>
<span class="line">        nn.Linear(hidden, 10)</span>
<span class="line">    ).to(DEVICE)</span>
<span class="line"></span>
<span class="line">    # 3. Train</span>
<span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=lr)</span>
<span class="line">    loss_fn = nn.CrossEntropyLoss()</span>
<span class="line">    train_loader = DataLoader(</span>
<span class="line">        MNIST(&quot;.&quot;, train=True, download=True, transform=transforms.ToTensor()),</span>
<span class="line">        batch_size=128, shuffle=True</span>
<span class="line">    )</span>
<span class="line">    model.train()</span>
<span class="line">    for epoch in range(2):  # Run only 2 epochs per trial for fast iteration</span>
<span class="line">        for x, y in train_loader:</span>
<span class="line">            x, y = x.to(DEVICE), y.to(DEVICE)</span>
<span class="line">            optimizer.zero_grad()</span>
<span class="line">            loss = loss_fn(model(x), y)</span>
<span class="line">            loss.backward()</span>
<span class="line">            optimizer.step()</span>
<span class="line"></span>
<span class="line">    # 4. Validate</span>
<span class="line">    correct = 0</span>
<span class="line">    val_loader = DataLoader(</span>
<span class="line">        MNIST(&quot;.&quot;, train=False, transform=transforms.ToTensor()),</span>
<span class="line">        batch_size=512</span>
<span class="line">    )</span>
<span class="line">    model.eval()</span>
<span class="line">    with torch.no_grad():</span>
<span class="line">        for x, y in val_loader:</span>
<span class="line">            preds = model(x.to(DEVICE)).argmax(1).cpu()</span>
<span class="line">            correct += (preds == y).sum().item()</span>
<span class="line">    </span>
<span class="line">    accuracy = correct / len(val_loader.dataset)</span>
<span class="line">    return accuracy # Optuna maximizes the objective by default</span>
<span class="line"></span>
<span class="line"># study = optuna.create_study(direction=&quot;maximize&quot;)</span>
<span class="line"># study.optimize(objective, n_trials=40, timeout=600) # 40 trials or 10 minutes</span>
<span class="line"># print(f&quot;Best score: {study.best_value:.4f}&quot;)</span>
<span class="line"># print(f&quot;Best hyperparameters: {study.best_params}&quot;)</span></code></pre>
<p><strong>Pro Tip</strong>: Combine this with <code>optuna.pruners</code> to terminate unpromising trials early, significantly saving computational resources.</p>
<h3>2.4 Early-Stopping Based Algorithms (Successive Halving / Hyperband)</h3>
<p><strong>Core Idea</strong>: These algorithms aim to speed up the search through dynamic resource allocation. They start by allocating a small amount of resources (e.g., a few training epochs) to many configurations, then eliminate the poor performers and allocate more resources only to the &quot;survivors.&quot;</p>
<pre><code class="language-python"><span class="line"># pip install scikit-learn</span>
<span class="line">from sklearn.experimental import enable_halving_search_cv</span>
<span class="line">from sklearn.model_selection import HalvingGridSearchCV</span>
<span class="line">from sklearn.ensemble import RandomForestClassifier</span>
<span class="line"></span>
<span class="line"># Assume X_train, y_train are loaded</span>
<span class="line">param_grid = {&quot;max_depth&quot;: [5, 10, 15, None],</span>
<span class="line">              &quot;min_samples_leaf&quot;: [1, 2, 4]}</span>
<span class="line">sh = HalvingGridSearchCV(</span>
<span class="line">    RandomForestClassifier(n_estimators=200),</span>
<span class="line">    param_grid,</span>
<span class="line">    cv=5,</span>
<span class="line">    factor=3,            # Keep 1/factor of candidates each round</span>
<span class="line">    resource=&quot;n_samples&quot;, # In scikit-learn, the resource is the number of samples</span>
<span class="line">    scoring=&quot;accuracy&quot;,</span>
<span class="line">    n_jobs=-1</span>
<span class="line">)</span>
<span class="line"># sh.fit(X_train, y_train)</span>
<span class="line"># print(f&quot;Best Params: {sh.best_params_}&quot;)</span></code></pre>
<p>For deep learning, <code>KerasTuner</code>&#39;s <code>Hyperband</code> or <code>Optuna</code>&#39;s <code>SuccessiveHalvingPruner</code> are more natural choices, as they can use <code>epoch</code> as the resource dimension.</p>
<h3>2.5 Population-Based Training (PBT)</h3>
<p><strong>Core Idea</strong>: This is an advanced hybrid strategy, often used in large-scale distributed training. It trains a group of models (a &quot;population&quot;) in parallel. Periodically, it replaces the weights of poor-performing models with those of high-performing models, while also applying small random perturbations (&quot;mutations&quot;) to their hyperparameters.</p>
<pre><code class="language-python"><span class="line"># pip install &quot;ray[tune]&quot; torch</span>
<span class="line">from ray import tune</span>
<span class="line">from ray.tune.schedulers import PopulationBasedTraining</span>
<span class="line"></span>
<span class="line">def train_model(config):</span>
<span class="line">    # Model, data loading, and training loop definition omitted here</span>
<span class="line">    # The training loop needs to report validation metrics via tune.report()</span>
<span class="line">    # e.g., tune.report(mean_accuracy=acc)</span>
<span class="line">    pass</span>
<span class="line"></span>
<span class="line">pbt = PopulationBasedTraining(</span>
<span class="line">    time_attr=&quot;training_iteration&quot;,</span>
<span class="line">    metric=&quot;mean_accuracy&quot;,</span>
<span class="line">    mode=&quot;max&quot;,</span>
<span class="line">    perturbation_interval=5, # Perturb every 5 training iterations</span>
<span class="line">    hyperparam_mutations={</span>
<span class="line">        &quot;lr&quot;: lambda: tune.loguniform(1e-4, 1e-1).sample(),</span>
<span class="line">        &quot;dropout&quot;: [0.2, 0.3, 0.4, 0.5]</span>
<span class="line">    }</span>
<span class="line">)</span>
<span class="line"></span>
<span class="line"># analysis = tune.run(</span>
<span class="line">#     train_model,</span>
<span class="line">#     resources_per_trial={&quot;cpu&quot;: 2, &quot;gpu&quot;: 1},</span>
<span class="line">#     config={&quot;lr&quot;: 1e-3, &quot;dropout&quot;: 0.4},</span>
<span class="line">#     num_samples=10, # Population size</span>
<span class="line">#     scheduler=pbt</span>
<span class="line"># )</span>
<span class="line"># print(&quot;Best config: &quot;, analysis.get_best_config(metric=&quot;mean_accuracy&quot;, mode=&quot;max&quot;))</span></code></pre>
<p>The power of PBT lies in its ability not only to optimize hyperparameters but also to learn effective learning rate schedules online.</p>
<h2>3. Common Hyperparameters and Their Impact</h2>
<p>Understanding how different hyperparameters affect model behavior is key to making informed tuning decisions.</p>
<table>
<thead>
<tr>
<th align="left">Category</th>
<th align="left">Hyperparameter</th>
<th align="left">Too Small</th>
<th align="left">Too Large</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>Optimization</strong></td>
<td align="left">Learning Rate (lr)</td>
<td align="left">Slow convergence, gets stuck in local minima</td>
<td align="left">Loss function oscillates or diverges, fails to converge</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Batch Size</td>
<td align="left">Noisy gradient updates, unstable convergence</td>
<td align="left">High memory consumption, may lead to poorer generalization</td>
</tr>
<tr>
<td align="left"><strong>Model Capacity</strong></td>
<td align="left"># Layers / # Neurons</td>
<td align="left">Underfitting, fails to learn complex patterns</td>
<td align="left">Overfitting, memorizes training data, poor generalization</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Convolutional Kernel Size</td>
<td align="left">Insufficient receptive field, fails to capture large-scale features</td>
<td align="left">Drastic increase in parameters, computationally expensive, prone to overfitting</td>
</tr>
<tr>
<td align="left"><strong>Regularization</strong></td>
<td align="left">Dropout Ratio</td>
<td align="left">Insufficient regularization, prone to overfitting</td>
<td align="left">Excessive reduction in effective model capacity, leading to underfitting</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">L1/L2 Regularization (λ)</td>
<td align="left">Insufficient penalty on model complexity</td>
<td align="left">Model becomes too simple, leading to underfitting</td>
</tr>
</tbody></table>
<h2>4. Practical Tuning Tips</h2>
<ol>
<li><strong>Set the Main Direction</strong>: Have a lot of data? Prioritize increasing model capacity. Limited data? Focus on regularization or data augmentation first.</li>
<li><strong>Tune in Groups</strong>: Don&#39;t try to tune everything at once. Start with optimization-related parameters (like learning rate, batch size), then move to model architecture, and finally, tune regularization.</li>
<li><strong>Use a Logarithmic Scale</strong>: For hyperparameters like learning rate and regularization strength, searching on a log scale (e.g., from <code>1e-5</code> to <code>1e-1</code>) is far more efficient than a linear scale.</li>
<li><strong>Visualize and Use Early Stopping</strong>: Monitor training/validation curves with tools like TensorBoard or WandB. If the validation loss stops decreasing or starts to rise, consider stopping the training early or increasing regularization.</li>
<li><strong>Leverage Existing Work</strong>: Start with the default configurations from relevant papers or open-source repositories. They are often a great baseline. Fine-tune within the same order of magnitude first.</li>
<li><strong>Prioritize Resources</strong>: First, determine the maximum batch size and model size your hardware (especially GPU memory) can handle. Then, fine-tune other parameters within these constraints.</li>
</ol>
<h2>5. Summary and Decision Tree</h2>
<p>How to choose the right tuning method?</p>
<ol>
<li><strong>Dimensions ≤ 3, small dataset, CPU training?</strong> → Go straight for <code>GridSearchCV</code>.</li>
<li><strong>Want a quick 80% solution?</strong> → <code>RandomizedSearchCV</code> with log-uniform sampling is your friend.</li>
<li><strong>Training on a GPU with a limited budget?</strong> → <code>Optuna</code> (TPE) with pruning or <code>Hyperband</code> is the most efficient choice.</li>
<li><strong>Have a large distributed cluster?</strong> → <code>Ray Tune</code>&#39;s PBT will unleash its full power.</li>
<li><strong>Doing academic research or need extreme fine-tuning?</strong> → You can explore hypergradient optimization, but be prepared for its complexity.</li>
</ol>
<p>By combining this theoretical knowledge with hands-on code examples, you will be able to perform hyperparameter tuning more systematically and efficiently, leading to significant improvements in your model&#39;s performance.</p>

    </article>
    </main>
</div>

<script src="https://cdn.jsdelivr.net/npm/prismjs@1/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-python.min.js"></script>
<script>
Prism.highlightAll();
document.querySelectorAll('article pre').forEach(pre => {
    const code = pre.querySelector('code');
    if (!code) return;
    const toolbar = document.createElement('div');
    toolbar.className = 'code-toolbar';
    const btn = document.createElement('button');
    btn.className = 'copy-btn';
    btn.textContent = 'Copy';
    btn.onclick = () => {
        navigator.clipboard.writeText(code.textContent).then(() => {
            btn.textContent = 'Copied!';
            btn.classList.add('copied');
            setTimeout(() => { btn.textContent = 'Copy'; btn.classList.remove('copied'); }, 2000);
        });
    };
    toolbar.appendChild(btn);
    pre.parentNode.insertBefore(toolbar, pre);
});
</script>
</body>
</html>