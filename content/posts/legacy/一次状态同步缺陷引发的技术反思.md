---
date: 2022-04-25
tags: [general_notes]
legacy: true
---

# 一次状态同步缺陷引发的技术反思

## 原因分析

问题原因归结起来其实很简单：批量操作时状态更新和日志记录不一致。究其根本，是代码实现上忽略了和单条操作逻辑的一致性。开发者在新增批量冻结功能时，没有复用单条冻结的完整逻辑，只是直接对主表数据进行了批量更新。由于单条冻结时日志记录是一个单独步骤，批量情况下如果不考虑日志，这一步就丢失了。

进一步来说，这反映了分布式系统中双写不一致的典型问题：当我们需要将状态变化写到两个地方（这里是主状态表和操作日志表）时，如果没有保证原子性或严格的一致性流程，就可能出现一处更新了而另一处未更新的情况。此时，不同组件各自依赖不同的数据源（主表或日志表），就会各自得到不一致的状态视图。在本例中，主库的状态是冻结了，但日志缺失导致依赖日志的组件未感知到冻结。

为何会采用日志表同步而不直接用主表？这是出于系统解耦和性能考虑。一方面，直接让同步任务大范围扫描主表找变化成本高，不如按日志增量同步高效；另一方面，日志表还承担了操作审计的作用，记录谁在何时对哪些资源做了什么操作。因此，日志作为事实来源之一是很多系统的设计选择。但这种设计要求每次状态变化时同时维护主数据和日志的一致更新。这种双写操作若无事务保障，就必须小心确保代码层面两者的完整更新。

回到我们的案例，根本原因可以说是一次疏忽：批量操作代码缺少日志写入，使得状态变更没有完全被记录。幸运的是，这一问题被及时发现并定位，没有进一步造成严重后果。

## 修复过程
找准原因之后，解决方案也就相对明确了。修复工作主要包括两个部分：
* 补录日志： 针对已经被冻结但未记录日志的资源，补写相应的操作日志。这样可以修复那些已经执行过批量冻结但日志缺失的数据，使后续同步任务能正确识别这些资源的冻结状态。具体做法是根据主状态表中标记为冻结且在日志表中没有对应记录的条目，补充插入日志记录。这一步相当于数据补救，确保历史数据的一致性。
* 修改代码： 在批量冻结和批量解冻的代码实现中，加入与单条操作相同的日志记录逻辑。也就是说，每冻结一个资源，不仅要更新其主状态，还要在日志表插入一条记录。为了防止再次遗漏，我们采用了重用单条操作逻辑的方式：将原有单个资源冻结的过程提炼成可以被批量调用的方法，使批量和单条操作走同一套流程。这样避免了代码分叉导致的不一致。与此同时，我们还为这两个表的更新加上了数据库事务，确保主表和日志表的更新要么一起成功，要么一起回滚，杜绝了部分成功的情况。

完成修复后，我们进行了多轮测试。首先单独验证批量冻结/解冻操作，在数据库的主表和日志表都能看到正确更新；接着模拟同步任务运行，确认批量操作后的资源状态能被其他模块正确感知。一切验证通过后，补丁顺利发布，线上问题得到解决。


## 日志驱动同步的常见风险
这个案例凸显了日志驱动同步模式中常见的问题与注意事项。在分布式系统里，常用一种做法是由一个模块产生日志（或事件），其他模块消费日志来达到数据同步的目的。这种模式包含以下关键点：
•	日志即事件源： 日志记录代表了数据变化的事件流，消费方通过读取新增的日志来感知变化。优点是解耦，消费方无需直接访问主库即可获取变更，提高了安全性和效率。
•	一致性依赖日志完整性： 日志必须完整、准确地记录每一次状态变更，否则会导致消费方与实际状态不一致。漏记一条日志，在消费方看来就等于漏掉了一次状态更新事件。
•	双写原子性： 如果主数据更新和日志记录无法在同一事务中完成，就存在中间状态不一致的风险。必须通过代码控制、重试机制或事务机制（如将日志写入和状态更新放在同一数据库事务，或者使用消息队列事务消息）来保证两者要么都成功要么都失败。
•	故障恢复： 即使做到以上两点，也需要考虑日志驱动同步中的故障场景。例如，消费方可能一段时间无法处理日志，恢复后需要有能力补消费之前漏掉的日志；或者主库和日志长时间不一致时，需要有校验工具能够扫描主表和日志表，找出差异并修正（这类似我们对历史数据补录日志的过程）。

日志驱动的模式本质上是一种最终一致性方案：允许不同组件暂时不一致，但通过日志传播最终达到一致。在实际工程中，它常见于事件溯源、变更订阅、分布式缓存刷新等场景。不过，此模式给开发提出了更严格的要求——确保日志正确记录所有事件且不重不漏。一旦日志机制出现纰漏，就会直接导致状态同步偏差。因此，设计这类系统时需要周全的考虑和大量的测试来验证日志同步的可靠性。


## 批量操作一致性设计要点
批量操作在系统设计中也有一些特别需要注意的一致性问题。批量更新往往涉及对多条数据的改动，如果不能保证一致的成功或失败，很容易造成部分成功的尴尬局面。结合本次经验和常见的实践，批量操作要保证一致性应考虑以下要点：
•	原子性（All or Nothing）： 尽可能使用事务或其他机制确保批量中各子操作要么全部完成要么全部取消。这样可以避免出现只冻结了一部分资源而另一些未冻结的情况（除非业务允许部分成功）。
•	重试与幂等： 批量操作通常执行量大，出错概率也相对更高。如果批量过程中某个子操作失败，系统应支持重试失败部分。同时，要设计操作为幂等的，以便重复执行不会引入不一致。例如，可以根据资源当前状态决定是否需要再次冻结，确保重复冻结不会改变最终结果。
•	代码路径统一： 正如我们修复时所做的，复用单条操作的逻辑可以减少遗漏。如果批量和单条操作共享同一实现，能保证他们遵循相同的业务规则和数据更新步骤，避免因为两套实现细微差异导致的数据不一致。
•	性能与一致性平衡： 批量操作有时为了性能会分批次处理或者异步处理。这种情况下，需要设计好每批次之间的隔离和提交策略，防止批次之间数据依赖问题。例如，批量冻结1000条，可以每100条一个事务，但要防止前一批提交后后一批未提交时系统读取到半成品结果。如果无法避免，则需要在更高层次以“任务”作为单位进行隔离，在整个批量任务完成前对外表现为进行中状态。
•	日志与监控： 批量操作涉及范围大，一旦出问题影响面广。因此要有良好的日志记录每个批次、每个子项的处理结果，并结合监控及时发现异常情况。比如批量冻结应记录冻结了哪些资源，哪些成功哪些失败，耗时多少等，方便出问题时快速定位和补救。

通过遵循这些要点，我们在设计批量操作时就能预先考虑一致性问题，减少类似本次事故的发生概率。

## 示例：Python 实现批量状态更新与日志记录
为了更直观地展示上述问题和解决方案，我们可以用一个简化的 Python 示例来模拟批量状态更新的过程。假设有两个数据结构：state_table 用于保存资源的当前状态，log_table 用于记录操作日志。状态用字符串表示，"active" 表示生效，"frozen" 表示冻结。

首先，我们实现单个资源的冻结函数和不完善的批量冻结函数：
```python
# 定义示例数据表
state_table = {
    "A": "active",
    "B": "active",
    "C": "active",
}
log_table = []  # 日志表开始时为空

# 单个资源冻结
def freeze_single(resource_id, operator):
    # 更新主状态表
    state_table[resource_id] = "frozen"
    # 记录操作日志
    log_entry = {
        "resource": resource_id,
        "action": "freeze",
        "operator": operator
    }
    log_table.append(log_entry)

# 批量冻结（初始版本，存在缺陷：没有记录日志）
def freeze_batch(resources, operator):
    for res in resources:
        state_table[res] = "frozen"
    # 忘记记录每个资源的冻结日志
```
使用上述函数，假设我们冻结资源 A，以及批量冻结资源 B 和 C：
```py
# 冻结单个资源A
freeze_single("A", operator="User1")
# 批量冻结资源B和C
freeze_batch(["B", "C"], operator="User1")

print(state_table)  # 查看主状态表
print(log_table)    # 查看日志表
```

输出结果：

```
{'A': 'frozen', 'B': 'frozen', 'C': 'frozen'}
[{'resource': 'A', 'action': 'freeze', 'operator': 'User1'}]
```
可以看到，主状态表中A、B、C三个资源都被标记为"frozen"，但是日志表中只有A的冻结操作记录，而没有B、C的记录。这就类似于我们遇到的问题场景：批量冻结更新了状态，但忘记记录日志。假如系统的其他组件依赖扫描log_table来感知状态变化，那么它们只会知道A被冻结了，完全察觉不到B和C的冻结。这将导致B和C在别的模块仍被视为“active”，造成数据不一致。

接下来，我们编写改进的批量冻结函数，确保每个资源的状态更新和日志记录同时进行，并利用Python的异常处理来模拟事务回滚机制：
```py
# 改进的批量冻结，增加日志记录和简单的事务机制
def freeze_batch_with_log(resources, operator):
    # 备份原始状态，以备回滚
    original_states = {}
    try:
        for res in resources:
            # 备份状态
            original_states[res] = state_table[res]
            # 更新状态
            state_table[res] = "frozen"
            # 写入日志
            log_entry = {
                "resource": res,
                "action": "freeze",
                "operator": operator
            }
            log_table.append(log_entry)
            # 模拟某种可能的错误，例如操作某个特殊资源出问题
            if res == "C":
                raise Exception(f"Failed to freeze {res}")  # 模拟错误
    except Exception as e:
        # 发生错误，回滚之前的更新
        for res in resources:
            if res in original_states:
                state_table[res] = original_states[res]  # 恢复原状态
        # 回滚日志（简单起见，将此次批量中的日志移除）
        log_table[:] = [entry for entry in log_table if entry["resource"] not in resources]
        print("Error during batch operation:", e)
```
这段代码中，freeze_batch_with_log 函数会对传入的每个资源执行冻结，并写日志。一旦某个资源处理报错，我们在except中将已经处理过的资源状态恢复原样，并撤销它们对应的日志记录，从而保证整个批量操作对外“没发生过”（即原子性）。上述代码里我们特意在处理资源C时抛出一个异常来模拟中途出错的情况。

现在重置数据，并尝试使用改进后的函数冻结B和C：
```py
# 重置状态
state_table = {"A": "frozen", "B": "active", "C": "active"}
log_table = [{"resource": "A", "action": "freeze", "operator": "User1"}]

# 尝试批量冻结B和C，期间模拟出错
freeze_batch_with_log(["B", "C"], operator="User1")

print(state_table)  # 查看主状态表
print(log_table)    # 查看日志表
```
输出结果可能如下：
```
Error during batch operation: Failed to freeze C
{'A': 'frozen', 'B': 'active', 'C': 'active'}
[{'resource': 'A', 'action': 'freeze', 'operator': 'User1'}]
```
可以看到，尽管在冻结C时发生了异常，错误被捕获并触发了回滚机制。最终主状态表中B仍保持为"active"，C也是"active"（批量操作前的状态），日志表也没有留下B或C的残留日志。也就是说，此次批量冻结操作对A、B、C的最终状态没有造成影响——要么全部成功（如果没有错误发生），要么在出错后完全回到初始状态。这实现了批量操作的原子性。实际生产环境中，我们会利用数据库事务或更健壮的机制来实现这种原子性保障，这里用Python代码做了一个形象的演示。

通过这个示例，我们直观地看到保持状态表和日志表同步更新以及保证批操作原子性的重要性。在分布式系统中，这种细节决定了各组件认知的一致性，稍有不慎就会埋下隐患。

## 结语
这次批量状态同步缺陷的经历提醒我们：魔鬼藏在细节里。在分布式系统中，数据的一致性往往比功能本身更具挑战。当我们引入新功能（比如批量操作）或采用新架构（比如日志驱动同步）时，需要从全局视角去考虑数据流转和状态同步，确保每一步都严谨可靠。

具体而言，一方面要强化开发流程中的审查和测试，对可能影响数据一致性的改动进行重点关注；另一方面，在架构设计上也应尽量避免让系统状态散落在多个来源而缺乏统一校验机制。如果不得不如此（例如为了性能和解耦需要拆分主数据和日志），那就务必在实现上保证强一致或提供自我修复能力（如定期对账校验）。

分布式系统的高可用和高性能常常需要在一致性上做出权衡，但正如这次事件，我们宁可事先多花些心思，也不想事后疲于救火。希望这次分享的案例和思考能给你带来一些启发，在今后的系统设计与实现中少踩坑，更从容地保障数据一致性。