<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Ultimate Guide to Classification Model Evaluation Metrics | Yuxu Ge</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1/themes/prism-tomorrow.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <style>
        :root {
            --black: #111;
            --dark-grey: #444;
            --off-white: #f4f4f4;
            --vermilion: #C41E3A;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }

        html {
            font-size: 16px;
            scroll-behavior: smooth;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: var(--black);
            background-color: var(--off-white);
        }

        a { color: var(--vermilion); text-decoration: none; }
        a:hover { opacity: 0.75; }

        .layout { min-height: 100vh; }

        /* Sidebar */
        .sidebar {
            position: fixed; top: 0; left: 0;
            width: 320px; height: 100vh;
            background: var(--black); color: var(--off-white);
            padding: 3rem 2rem;
            display: flex; flex-direction: column; justify-content: space-between;
        }
        .sidebar-top { display: flex; flex-direction: column; align-items: center; text-align: center; }
        .avatar { display: block; width: 120px; height: 120px; border-radius: 50%; border: 3px solid var(--vermilion); margin-bottom: 1.5rem; overflow: hidden; transition: transform 0.2s ease; }
        .avatar:hover { opacity: 1; transform: scale(1.05); }
        .avatar img { width: 100%; height: 100%; object-fit: cover; }
        .name-block { margin-bottom: 0.5rem; }
        h1 { font-size: 1.8rem; font-weight: 700; letter-spacing: 1px; color: var(--off-white); }
        .title-role { font-size: 0.9rem; color: #888; margin-top: 0.25rem; font-weight: 400; }
        .contact-block { margin-top: 2.5rem; width: 100%; }
        .social-links { display: flex; flex-direction: column; align-items: center; gap: 0.75rem; }
        .social-links a { display: flex; align-items: center; gap: 0.75rem; width: 160px; color: var(--off-white); opacity: 0.7; padding: 0.5rem 0.75rem; border-radius: 4px; font-size: 0.9rem; transition: all 0.2s; }
        .social-links a:hover { color: var(--vermilion); opacity: 1; background: rgba(255,255,255,0.05); }
        .social-links svg { width: 20px; height: 20px; flex-shrink: 0; }
        .social-links .orcid-link { font-size: 0.68rem; gap: 0.5rem; white-space: nowrap; }
        .sidebar-footer { text-align: center; font-size: 0.75rem; color: #555; }

        /* Content */
        .content { margin-left: 320px; padding: 3rem 4rem; max-width: 900px; }
        .back-link { display: inline-block; font-size: 0.85rem; margin-bottom: 2rem; color: var(--dark-grey); }
        .back-link:hover { color: var(--vermilion); }

        /* Article */
        article h1 { font-size: 2rem; font-weight: 700; margin-bottom: 1.5rem; line-height: 1.3; color: var(--black); }
        article h2 { font-size: 1.4rem; font-weight: 600; margin: 2rem 0 1rem; color: var(--black); }
        article h3 { font-size: 1.1rem; font-weight: 600; margin: 1.5rem 0 0.75rem; color: var(--black); }
        article p { margin-bottom: 1rem; color: var(--dark-grey); }
        article ul, article ol { margin: 1rem 0 1rem 1.5rem; color: var(--dark-grey); }
        article li { margin-bottom: 0.5rem; }
        article strong { color: var(--black); }
        article code { font-family: "SF Mono", Monaco, monospace; font-size: 0.9em; background: #e8e8e8; padding: 0.15em 0.4em; border-radius: 3px; }
        article pre { margin: 1rem 0; border-radius: 6px; overflow-x: auto; background: #2d2d2d; padding: 1rem; }
        article pre code { background: none; padding: 0; font-size: 0.75em; white-space: pre !important; counter-reset: line; display: block; color: #ccc; }
        article pre code .line { counter-increment: line; }
        article pre code .line::before { content: counter(line); display: inline-block; width: 2.5em; margin-right: 1em; text-align: right; color: #666; user-select: none; }
        .code-toolbar { display: flex; justify-content: flex-start; padding: 0.35rem 0.5rem; background: #3a3a3a; border-radius: 6px 6px 0 0; }
        .code-toolbar + pre { margin-top: 0; border-radius: 0 0 6px 6px; }
        .copy-btn { padding: 0.2rem 0.5rem; font-size: 0.7rem; background: #555; color: #fff; border: none; border-radius: 3px; cursor: pointer; transition: all 0.2s; }
        .copy-btn:hover, .copy-btn:active { background: #777; }
        .copy-btn.copied { background: #2a2; }
        article blockquote { border-left: 3px solid var(--vermilion); padding-left: 1rem; margin: 1rem 0; color: #666; font-style: italic; }
        article hr { border: none; border-top: 1px solid #ddd; margin: 2rem 0; }
        article table { width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem; }
        article th, article td { border: 1px solid #ddd; padding: 0.5rem 0.75rem; text-align: left; }
        article th { background: #e8e8e8; font-weight: 600; }
        article img { max-width: 100%; height: auto; display: block; margin: 1.5rem 0; border-radius: 6px; }

        .loading { text-align: center; padding: 3rem; color: #888; }
        .error { color: var(--vermilion); }

        /* KaTeX math styles */
        .katex-display { overflow-x: auto; overflow-y: hidden; padding: 0.5rem 0; }
        .katex { font-size: 1.1em; }

        /* Responsive */
        @media (max-width: 900px) {
            .sidebar { position: relative; width: 100%; height: auto; padding: 2rem 1.5rem 1rem; }
            .social-links { flex-direction: row; flex-wrap: wrap; justify-content: center; }
            .social-links a, .social-links .orcid-link { width: auto; padding: 0.5rem; font-size: 0; gap: 0; }
            .social-links svg { width: 24px; height: 24px; }
            .social-links .hide-mobile { display: none; }
            .sidebar-footer { margin-top: 1rem; }
            .content { margin-left: 0; padding: 2rem 1rem; }
        }
        @media (max-width: 480px) {
            .sidebar { padding: 1.5rem 1rem 1rem; }
            .avatar { width: 80px; height: 80px; }
            h1 { font-size: 1.4rem; }
            article h1 { font-size: 1.5rem; }
        }
    </style>
</head>
<body>

<div class="layout">
    <aside class="sidebar" id="sidebar-content"></aside>
    <script src="/components/sidebar.js"></script>

    <main class="content">
        <a href="/blog/" class="back-link">← Back to Blog</a>
        <article>
<hr>
<h2>date: 2024-01-01
tags: [ai, machine learning, classification, metrics, python]
legacy: true</h2>
<h1>The Ultimate Guide to Classification Model Evaluation Metrics</h1>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">Predicted Positive</th>
<th align="left">Predicted Negative</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>Actual Positive</strong></td>
<td align="left">TP (True Positive)</td>
<td align="left">FN (False Negative)</td>
</tr>
<tr>
<td align="left"><strong>Actual Negative</strong></td>
<td align="left">FP (False Positive)</td>
<td align="left">TN (True Negative)</td>
</tr>
</tbody></table>
<ul>
<li><strong>TP (True Positive)</strong>: Correctly predicted positive.</li>
<li><strong>FN (False Negative)</strong>: Actual positive, but predicted negative (Type I Error).</li>
<li><strong>FP (False Positive)</strong>: Actual negative, but predicted positive (Type II Error).</li>
<li><strong>TN (True Negative)</strong>: Correctly predicted negative.</li>
</ul>
<p>Nearly all subsequent classification metrics are derived from these four fundamental counts.</p>
<h3>2. Point-Based Core Metrics (Threshold-Dependent)</h3>
<p>These metrics are calculated based on a specific classification threshold (usually 0.5 by default).</p>
<table>
<thead>
<tr>
<th align="left">Metric</th>
<th align="left">Formula</th>
<th align="left">Intuition</th>
<th align="left">Use Case</th>
<th align="left">Caveats</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>Accuracy</strong></td>
<td align="left">$\frac{TP+TN}{TP+FP+FN+TN}$</td>
<td align="left">The overall &quot;correctness rate.&quot;</td>
<td align="left">Balanced datasets where error costs are equal.</td>
<td align="left">Highly misleading on imbalanced data.</td>
</tr>
<tr>
<td align="left"><strong>Precision</strong></td>
<td align="left">$\frac{TP}{TP+FP}$</td>
<td align="left">How trustworthy is a positive prediction?</td>
<td align="left">Spam detection, medical diagnosis (confirmation).</td>
<td align="left">Ignores FN; sensitive to threshold changes.</td>
</tr>
<tr>
<td align="left"><strong>Recall (Sensitivity)</strong></td>
<td align="left">$\frac{TP}{TP+FN}$</td>
<td align="left">What fraction of actual positives were caught?</td>
<td align="left">Disease screening, fraud detection.</td>
<td align="left">Ignores FP; insufficient alone if costs are unequal.</td>
</tr>
<tr>
<td align="left"><strong>Specificity</strong></td>
<td align="left">$\frac{TN}{TN+FP}$</td>
<td align="left">What fraction of actual negatives were caught?</td>
<td align="left">Credit risk (avoiding rejecting good customers).</td>
<td align="left">Forms the ROC curve with Recall.</td>
</tr>
<tr>
<td align="left"><strong>F1-Score</strong></td>
<td align="left">$2 \cdot \frac{\text{P} \cdot \text{R}}{\text{P} + \text{R}}$</td>
<td align="left">The harmonic mean of Precision and Recall.</td>
<td align="left">When P and R are equally important.</td>
<td align="left">Can be macro-averaged for multi-class.</td>
</tr>
<tr>
<td align="left"><strong>Fβ-Score</strong></td>
<td align="left">$(1+\beta^2) \frac{\text{P} \cdot \text{R}}{\beta^2 P + R}$</td>
<td align="left">Generalizes F1, weighting Recall higher (β&gt;1) or Precision higher (β&lt;1).</td>
<td align="left">When Recall is more critical (e.g., β=2).</td>
<td align="left">β must be defined by business needs.</td>
</tr>
<tr>
<td align="left"><strong>Balanced Accuracy</strong></td>
<td align="left">$\frac{\text{Recall} + \text{Specificity}}{2}$</td>
<td align="left">Macro-average of Recall, robust to imbalance.</td>
<td align="left">Fraud detection, rare disease identification.</td>
<td align="left">Still dependent on a fixed threshold.</td>
</tr>
<tr>
<td align="left"><strong>MCC</strong></td>
<td align="left">$\frac{TP \cdot TN - FP \cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$</td>
<td align="left">A correlation coefficient between -1 and 1.</td>
<td align="left">Highly imbalanced binary classification.</td>
<td align="left">Uses all four matrix cells; considered very robust.</td>
</tr>
</tbody></table>
<h3>3. Threshold-Curve Metrics (Threshold-Independent)</h3>
<p>These metrics evaluate a model&#39;s performance across all possible classification thresholds.</p>
<table>
<thead>
<tr>
<th align="left">Metric</th>
<th align="left">Construction</th>
<th align="left">When to Prioritize</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>ROC Curve &amp; AUC</strong></td>
<td align="left">Plots TPR (Recall) vs. FPR (1-Specificity) as the threshold varies. AUC is the area under this curve.</td>
<td align="left">Balanced data; when the threshold is tunable or costs are unknown; evaluating overall ranking ability.</td>
</tr>
<tr>
<td align="left"><strong>PR Curve &amp; AUPRC</strong></td>
<td align="left">Plots Precision vs. Recall. AUPRC (or Average Precision) is the area under this curve.</td>
<td align="left">When the positive class is rare (e.g., click-through rate &lt;1%); when you are concerned about surges in FP.</td>
</tr>
</tbody></table>
<p><strong>Key Takeaway</strong>: For imbalanced datasets, the <strong>PR curve and its AUPRC</strong> are often more informative than the ROC-AUC. A high ROC-AUC can be misleadingly optimistic when the number of true negatives (TN) is massive.</p>
<h3>4. Probability Quality Metrics</h3>
<p>These metrics assess the quality of the predicted probabilities themselves, not the final class labels.</p>
<table>
<thead>
<tr>
<th align="left">Metric</th>
<th align="left">Meaning</th>
<th align="left">Use Case</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>Log Loss / Cross-Entropy</strong></td>
<td align="left">The negative log-likelihood. Penalizes confident but incorrect predictions heavily. Lower is better.</td>
<td align="left">The default loss function for training/tuning probabilistic models.</td>
</tr>
<tr>
<td align="left"><strong>Brier Score</strong></td>
<td align="left">The mean squared error between predicted probabilities and actual outcomes (0 or 1). Lower is better.</td>
<td align="left">Weather forecasting, sports predictions; assessing probability accuracy.</td>
</tr>
<tr>
<td align="left"><strong>ECE / MCE</strong></td>
<td align="left">Expected/Maximum Calibration Error. Measures the consistency between predicted probabilities and observed frequencies.</td>
<td align="left">When model confidence needs to be trustworthy (e.g., in AutoML or high-stakes decisions).</td>
</tr>
</tbody></table>
<h3>5. Averaging Strategies for Multi-Class &amp; Imbalanced Data</h3>
<ul>
<li><strong>Micro Average</strong>: Aggregates the contributions of all classes to compute the average metric. It weights each <strong>sample</strong> equally. Best for assessing overall performance.</li>
<li><strong>Macro Average</strong>: Calculates the metric independently for each class and then takes the average. It weights each <strong>class</strong> equally. Highlights performance on rare classes.</li>
<li><strong>Weighted Average</strong>: A macro average where each class&#39;s score is weighted by its number of samples. A compromise between micro and macro.</li>
</ul>
<p><strong>Recommendation</strong>: Always plot the class distribution first. If it&#39;s a long-tail distribution, report both <strong>Macro-F1</strong> and <strong>Micro-F1</strong> to give a complete picture.</p>
<h3>6. Common Pitfalls &amp; How to Avoid Them</h3>
<ol>
<li><strong>Confusing Precision &amp; Recall</strong>: Remember, Recall is about finding all positives (minimizing FN), while Precision is about not mislabeling negatives as positives (minimizing FP).</li>
<li><strong>High Accuracy ≠ Good Model</strong>: On a 99:1 imbalanced dataset, a model that predicts &quot;negative&quot; every time has 99% accuracy but is useless.</li>
<li><strong>F1 Isn&#39;t a Silver Bullet</strong>: If your business cares more about Recall (e.g., cancer screening), use the F2-Score. If Precision is paramount (e.g., legal document review), use the F0.5-Score.</li>
<li><strong>Comparing AUC Across Datasets</strong>: The shape of ROC/PR curves depends on the dataset&#39;s class distribution. AUC scores are not directly comparable across different test sets.</li>
<li><strong>Separation vs. Calibration</strong>: A model can have a high ROC-AUC (good at separating classes) but produce poorly calibrated probabilities. For production use, check both separation (AUC) and calibration (LogLoss/ECE).</li>
</ol>
<h3>7. Python Quick-Start Template</h3>
<p>This template uses <code>scikit-learn</code> to compute multiple metrics for an imbalanced, multi-class classification problem.</p>
<pre><code class="language-python"><span class="line">import numpy as np</span>
<span class="line">from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,</span>
<span class="line">                             roc_auc_score, average_precision_score,</span>
<span class="line">                             log_loss, brier_score_loss, confusion_matrix, matthews_corrcoef)</span>
<span class="line">from sklearn.datasets import make_classification</span>
<span class="line">from sklearn.model_selection import train_test_split</span>
<span class="line">from sklearn.linear_model import LogisticRegression</span>
<span class="line"></span>
<span class="line"># 1. Generate an imbalanced multi-class dataset</span>
<span class="line">X, y = make_classification(n_samples=3000, n_classes=3,</span>
<span class="line">                           weights=[0.7, 0.25, 0.05], # Imbalanced class weights</span>
<span class="line">                           n_informative=5, n_redundant=2,</span>
<span class="line">                           flip_y=0.03, random_state=42)</span>
<span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)</span>
<span class="line"></span>
<span class="line"># 2. Train a simple Logistic Regression model</span>
<span class="line">model = LogisticRegression(max_iter=1000, multi_class=&quot;ovr&quot;, random_state=42)</span>
<span class="line">model.fit(X_train, y_train)</span>
<span class="line">y_proba = model.predict_proba(X_test)</span>
<span class="line">y_pred = model.predict(X_test)</span>
<span class="line"></span>
<span class="line"># 3. Calculate various metrics</span>
<span class="line">print(&quot;--- Classification Metrics ---&quot;)</span>
<span class="line"></span>
<span class="line"># Point-based metrics (weighted average)</span>
<span class="line">acc = accuracy_score(y_test, y_pred)</span>
<span class="line">prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=&#39;weighted&#39;)</span>
<span class="line">mcc = matthews_corrcoef(y_test, y_pred)</span>
<span class="line"></span>
<span class="line">print(f&quot;Accuracy (Weighted) : {acc:.4f}&quot;)</span>
<span class="line">print(f&quot;Precision (Weighted): {prec:.4f}&quot;)</span>
<span class="line">print(f&quot;Recall (Weighted)   : {rec:.4f}&quot;)</span>
<span class="line">print(f&quot;F1-Score (Weighted) : {f1:.4f}&quot;)</span>
<span class="line">print(f&quot;Matthews Corr Coef  : {mcc:.4f}\n&quot;)</span>
<span class="line"></span>
<span class="line"># Macro-averaged metrics (better for seeing performance on rare classes)</span>
<span class="line">macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(y_test, y_pred, average=&#39;macro&#39;)</span>
<span class="line">print(f&quot;Precision (Macro)   : {macro_prec:.4f}&quot;)</span>
<span class="line">print(f&quot;Recall (Macro)      : {macro_rec:.4f}&quot;)</span>
<span class="line">print(f&quot;F1-Score (Macro)    : {macro_f1:.4f}\n&quot;)</span>
<span class="line"></span>
<span class="line"># Probability quality metrics</span>
<span class="line">logloss = log_loss(y_test, y_proba)</span>
<span class="line"># Brier Score needs to be averaged across classes</span>
<span class="line">brier = np.mean([brier_score_loss((y_test == k), y_proba[:, k]) for k in np.unique(y_test)])</span>
<span class="line"></span>
<span class="line">print(f&quot;LogLoss             : {logloss:.4f}&quot;)</span>
<span class="line">print(f&quot;Brier Score (Avg)   : {brier:.4f}&quot;)</span></code></pre>
<p><strong>Example Output</strong>:</p>
<pre><code><span class="line">--- Classification Metrics ---</span>
<span class="line">Accuracy (Weighted) : 0.8944</span>
<span class="line">Precision (Weighted): 0.8861</span>
<span class="line">Recall (Weighted)   : 0.8944</span>
<span class="line">F1-Score (Weighted) : 0.8862</span>
<span class="line">Matthews Corr Coef  : 0.7183</span>
<span class="line"></span>
<span class="line">Precision (Macro)   : 0.7989</span>
<span class="line">Recall (Macro)      : 0.7311</span>
<span class="line">F1-Score (Macro)    : 0.7552</span>
<span class="line"></span>
<span class="line">LogLoss             : 0.2819</span>
<span class="line">Brier Score (Avg)   : 0.0542</span></code></pre>
<p>Notice that the Macro-F1 (0.755) is significantly lower than the Weighted-F1 (0.886). This reveals that the model struggles with the rare classes—an insight easily missed by looking only at overall accuracy.</p>
<h3>8. A Quick Flowchart for Choosing Metrics</h3>
<ol>
<li><strong>Define Business Cost</strong>: What is the cost of a False Positive vs. a False Negative?</li>
<li><strong>Select Point-Based Metrics</strong>: If costs are known, tune your threshold and report Precision/Recall/Fβ. If costs are equal or unknown, F1-Score is a good default.</li>
<li><strong>Evaluate Probability</strong>: Before deploying, check if your model&#39;s probabilities are calibrated using LogLoss and ECE/Brier Score.</li>
<li><strong>Monitor Overall Quality</strong>: During model iteration or A/B testing, use AUPRC (for imbalanced data) or ROC-AUC (for balanced data) to assess overall ranking power.</li>
<li><strong>Address Imbalance</strong>: If Macro-F1 is much lower than Micro-F1, analyze the confusion matrix to see which minority classes are failing. Consider using resampling, cost-sensitive learning, or Focal Loss to improve performance.</li>
</ol>

    </article>
    </main>
</div>

<script src="https://cdn.jsdelivr.net/npm/prismjs@1/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-python.min.js"></script>
<script>
Prism.highlightAll();
document.querySelectorAll('article pre').forEach(pre => {
    const code = pre.querySelector('code');
    if (!code) return;
    const toolbar = document.createElement('div');
    toolbar.className = 'code-toolbar';
    const btn = document.createElement('button');
    btn.className = 'copy-btn';
    btn.textContent = 'Copy';
    btn.onclick = () => {
        navigator.clipboard.writeText(code.textContent).then(() => {
            btn.textContent = 'Copied!';
            btn.classList.add('copied');
            setTimeout(() => { btn.textContent = 'Copy'; btn.classList.remove('copied'); }, 2000);
        });
    };
    toolbar.appendChild(btn);
    pre.parentNode.insertBefore(toolbar, pre);
});
</script>
</body>
</html>