<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Deep Dive into Regression Evaluation Metrics: From MAE to R², Choosing the Right Tool for the Job | Yuxu Ge</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1/themes/prism-tomorrow.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <style>
        :root {
            --black: #111;
            --dark-grey: #444;
            --off-white: #f4f4f4;
            --vermilion: #C41E3A;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }

        html {
            font-size: 16px;
            scroll-behavior: smooth;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: var(--black);
            background-color: var(--off-white);
        }

        a { color: var(--vermilion); text-decoration: none; }
        a:hover { opacity: 0.75; }

        .layout { min-height: 100vh; }

        /* Sidebar */
        .sidebar {
            position: fixed; top: 0; left: 0;
            width: 320px; height: 100vh;
            background: var(--black); color: var(--off-white);
            padding: 3rem 2rem;
            display: flex; flex-direction: column; justify-content: space-between;
        }
        .sidebar-top { display: flex; flex-direction: column; align-items: center; text-align: center; }
        .avatar { display: block; width: 120px; height: 120px; border-radius: 50%; border: 3px solid var(--vermilion); margin-bottom: 1.5rem; overflow: hidden; transition: transform 0.2s ease; }
        .avatar:hover { opacity: 1; transform: scale(1.05); }
        .avatar img { width: 100%; height: 100%; object-fit: cover; }
        .name-block { margin-bottom: 0.5rem; }
        h1 { font-size: 1.8rem; font-weight: 700; letter-spacing: 1px; color: var(--off-white); }
        .title-role { font-size: 0.9rem; color: #888; margin-top: 0.25rem; font-weight: 400; }
        .contact-block { margin-top: 2.5rem; width: 100%; }
        .social-links { display: flex; flex-direction: column; align-items: center; gap: 0.75rem; }
        .social-links a { display: flex; align-items: center; gap: 0.75rem; width: 160px; color: var(--off-white); opacity: 0.7; padding: 0.5rem 0.75rem; border-radius: 4px; font-size: 0.9rem; transition: all 0.2s; }
        .social-links a:hover { color: var(--vermilion); opacity: 1; background: rgba(255,255,255,0.05); }
        .social-links svg { width: 20px; height: 20px; flex-shrink: 0; }
        .social-links .orcid-link { font-size: 0.68rem; gap: 0.5rem; white-space: nowrap; }
        .sidebar-footer { text-align: center; font-size: 0.75rem; color: #555; }

        /* Content */
        .content { margin-left: 320px; padding: 3rem 4rem; max-width: 900px; }
        .back-link { display: inline-block; font-size: 0.85rem; margin-bottom: 2rem; color: var(--dark-grey); }
        .back-link:hover { color: var(--vermilion); }

        /* Article */
        article h1 { font-size: 2rem; font-weight: 700; margin-bottom: 1.5rem; line-height: 1.3; color: var(--black); }
        article h2 { font-size: 1.4rem; font-weight: 600; margin: 2rem 0 1rem; color: var(--black); }
        article h3 { font-size: 1.1rem; font-weight: 600; margin: 1.5rem 0 0.75rem; color: var(--black); }
        article p { margin-bottom: 1rem; color: var(--dark-grey); }
        article ul, article ol { margin: 1rem 0 1rem 1.5rem; color: var(--dark-grey); }
        article li { margin-bottom: 0.5rem; }
        article strong { color: var(--black); }
        article code { font-family: "SF Mono", Monaco, monospace; font-size: 0.9em; background: #e8e8e8; padding: 0.15em 0.4em; border-radius: 3px; }
        article pre { margin: 1rem 0; border-radius: 6px; overflow-x: auto; background: #2d2d2d; padding: 1rem; }
        article pre code { background: none; padding: 0; font-size: 0.75em; white-space: pre !important; counter-reset: line; display: block; color: #ccc; }
        article pre code .line { counter-increment: line; }
        article pre code .line::before { content: counter(line); display: inline-block; width: 2.5em; margin-right: 1em; text-align: right; color: #666; user-select: none; }
        .code-toolbar { display: flex; justify-content: flex-start; padding: 0.35rem 0.5rem; background: #3a3a3a; border-radius: 6px 6px 0 0; }
        .code-toolbar + pre { margin-top: 0; border-radius: 0 0 6px 6px; }
        .copy-btn { padding: 0.2rem 0.5rem; font-size: 0.7rem; background: #555; color: #fff; border: none; border-radius: 3px; cursor: pointer; transition: all 0.2s; }
        .copy-btn:hover, .copy-btn:active { background: #777; }
        .copy-btn.copied { background: #2a2; }
        article blockquote { border-left: 3px solid var(--vermilion); padding-left: 1rem; margin: 1rem 0; color: #666; font-style: italic; }
        article hr { border: none; border-top: 1px solid #ddd; margin: 2rem 0; }
        article table { width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem; }
        article th, article td { border: 1px solid #ddd; padding: 0.5rem 0.75rem; text-align: left; }
        article th { background: #e8e8e8; font-weight: 600; }
        article img { max-width: 100%; height: auto; display: block; margin: 1.5rem 0; border-radius: 6px; }

        .loading { text-align: center; padding: 3rem; color: #888; }
        .error { color: var(--vermilion); }

        /* KaTeX math styles */
        .katex-display { overflow-x: auto; overflow-y: hidden; padding: 0.5rem 0; }
        .katex { font-size: 1.1em; }

        /* Responsive */
        @media (max-width: 900px) {
            .sidebar { position: relative; width: 100%; height: auto; padding: 2rem 1.5rem 1rem; }
            .social-links { flex-direction: row; flex-wrap: wrap; justify-content: center; }
            .social-links a, .social-links .orcid-link { width: auto; padding: 0.5rem; font-size: 0; gap: 0; }
            .social-links svg { width: 24px; height: 24px; }
            .social-links .hide-mobile { display: none; }
            .sidebar-footer { margin-top: 1rem; }
            .content { margin-left: 0; padding: 2rem 1rem; }
        }
        @media (max-width: 480px) {
            .sidebar { padding: 1.5rem 1rem 1rem; }
            .avatar { width: 80px; height: 80px; }
            h1 { font-size: 1.4rem; }
            article h1 { font-size: 1.5rem; }
        }
    </style>
</head>
<body>

<div class="layout">
    <aside class="sidebar" id="sidebar-content"></aside>
    <script src="/components/sidebar.js"></script>

    <main class="content">
        <a href="/blog/" class="back-link">← Back to Blog</a>
        <article>
<hr>
<h2>date: 2024-01-01
tags: [ai, machine learning, data science, regression, python]
legacy: true</h2>
<h1>A Deep Dive into Regression Evaluation Metrics: From MAE to R², Choosing the Right Tool for the Job</h1>
<table>
<thead>
<tr>
<th align="left">Metric</th>
<th align="left">Formula</th>
<th align="left">Scale</th>
<th align="left">Range</th>
<th align="left">Interpretation &amp; Focus</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>MAE</strong> (Mean Absolute Error)</td>
<td align="left">$\text{MAE} = \frac{1}{n} \sum_{i=1}^{n}</td>
<td align="left">y_i - \hat{y}_i</td>
<td align="left">$</td>
<td align="left">Same as target</td>
</tr>
<tr>
<td align="left"><strong>MSE</strong> (Mean Squared Error)</td>
<td align="left">$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$</td>
<td align="left">Squared scale</td>
<td align="left">$[0, \infty)$</td>
<td align="left">Penalizes larger errors more heavily. Sensitive to outliers; convenient for gradient-based optimization.</td>
</tr>
<tr>
<td align="left"><strong>RMSE</strong> (Root Mean Squared Error)</td>
<td align="left">$\sqrt{\text{MSE}}$</td>
<td align="left">Same as target</td>
<td align="left">$[0, \infty)$</td>
<td align="left">Balances MSE&#39;s sensitivity with MAE&#39;s interpretability. One of the most widely used metrics.</td>
</tr>
<tr>
<td align="left"><strong>MedAE</strong> (Median Absolute Error)</td>
<td align="left">$\text{MedAE} = \text{median}(</td>
<td align="left">y_1 - \hat{y}_1</td>
<td align="left">, \dots,</td>
<td align="left">y_n - \hat{y}_n</td>
</tr>
<tr>
<td align="left"><strong>R²</strong> (Coefficient of Determination)</td>
<td align="left">$1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}$</td>
<td align="left">Unitless</td>
<td align="left">$(-\infty, 1]$</td>
<td align="left">The percentage of variance in the data explained by the model. Closer to 1 is better, but it can be negative.</td>
</tr>
<tr>
<td align="left"><strong>Adjusted R²</strong></td>
<td align="left">Adjusted R²</td>
<td align="left">Unitless</td>
<td align="left">$(-\infty, 1]$</td>
<td align="left">An adjusted version of R² that penalizes for the number of features, making it more suitable for comparing models with different features.</td>
</tr>
</tbody></table>
<h3>2. Other Common Metrics</h3>
<p>Beyond the core metrics, the following are extremely useful in specific scenarios.</p>
<table>
<thead>
<tr>
<th align="left">Metric</th>
<th align="left">Formula &amp; Characteristics</th>
<th align="left">Use Case</th>
<th align="left">Notes</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>MAPE</strong> (Mean Absolute Percentage Error)</td>
<td align="left">$\frac{100}{n}\sum</td>
<td align="left">\frac{y_i - \hat{y}_i}{y_i}</td>
<td align="left">%$</td>
</tr>
<tr>
<td align="left"><strong>SMAPE</strong> (Symmetric MAPE)</td>
<td align="left">$\frac{100}{n}\sum\frac{</td>
<td align="left">y_i - \hat{y}_i</td>
<td align="left">}{(</td>
</tr>
<tr>
<td align="left"><strong>RMSLE</strong> (Root Mean Squared Log Error)</td>
<td align="left">$\sqrt{\frac{1}{n}\sum (\ln(y_i+1)-\ln(\hat{y}_i+1))^2}$</td>
<td align="left">Predicting positive, long-tailed data (e.g., house prices, web traffic).</td>
<td align="left">Focuses on relative error; less penalty for large value deviations. Requires $y &gt; -1$.</td>
</tr>
<tr>
<td align="left"><strong>Explained Variance</strong></td>
<td align="left">$1 - \frac{\text{Var}(y - \hat{y})}{\text{Var}(y)}$</td>
<td align="left">Similar to R², but focuses on variance rather than systematic bias.</td>
<td align="left">Measures the correlation between the fluctuations of predicted and true values.</td>
</tr>
<tr>
<td align="left"><strong>Max Error</strong></td>
<td align="left">$\max_i</td>
<td align="left">y_i - \hat{y}_i</td>
<td align="left">$</td>
</tr>
</tbody></table>
<h3>3. Quick Calculation with Python</h3>
<p><code>scikit-learn</code> provides convenient tools to calculate most of these metrics. Here is a template you can quickly adapt for your projects.</p>
<pre><code class="language-python"><span class="line">import numpy as np</span>
<span class="line">from sklearn.metrics import (mean_absolute_error, mean_squared_error,</span>
<span class="line">                             median_absolute_error, r2_score,</span>
<span class="line">                             mean_absolute_percentage_error,</span>
<span class="line">                             explained_variance_score, max_error)</span>
<span class="line"></span>
<span class="line"># Assuming y_true and y_pred are your ground truth and predictions</span>
<span class="line">rng = np.random.RandomState(42)</span>
<span class="line">y_true = rng.uniform(50, 150, size=30)</span>
<span class="line">noise  = rng.normal(0, 10, size=30)</span>
<span class="line">y_pred = y_true + noise</span>
<span class="line"></span>
<span class="line"># Calculate multiple metrics at once</span>
<span class="line">metrics = {</span>
<span class="line">    &quot;MAE&quot;: mean_absolute_error(y_true, y_pred),</span>
<span class="line">    &quot;MSE&quot;: mean_squared_error(y_true, y_pred),</span>
<span class="line">    &quot;RMSE&quot;: mean_squared_error(y_true, y_pred, squared=False), # or np.sqrt(mse)</span>
<span class="line">    &quot;MedAE&quot;: median_absolute_error(y_true, y_pred),</span>
<span class="line">    &quot;MAPE(%)&quot;: mean_absolute_percentage_error(y_true, y_pred) * 100,</span>
<span class="line">    &quot;R2&quot;: r2_score(y_true, y_pred),</span>
<span class="line">    &quot;ExplainedVar&quot;: explained_variance_score(y_true, y_pred),</span>
<span class="line">    &quot;MaxError&quot;: max_error(y_true, y_pred)</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line">print(&quot;--- Regression Metrics ---&quot;)</span>
<span class="line">for k, v in metrics.items():</span>
<span class="line">    print(f&quot;{k:&lt;12}: {v:.4f}&quot;)</span></code></pre>
<p><strong>Example Output</strong> (may vary slightly with each run):</p>
<pre><code><span class="line">--- Regression Metrics ---</span>
<span class="line">MAE         : 8.7285</span>
<span class="line">MSE         : 105.1905</span>
<span class="line">RMSE        : 10.2554</span>
<span class="line">MedAE       : 7.6170</span>
<span class="line">MAPE(%)     : 7.8522</span>
<span class="line">R2          : 0.8925</span>
<span class="line">ExplainedVar: 0.8949</span>
<span class="line">MaxError    : 28.1245</span></code></pre>
<h3>4. How Do Outliers Affect Metrics? A Small Experiment</h3>
<p>To intuitively understand the sensitivity of different metrics to outliers, let&#39;s conduct a simple experiment.</p>
<pre><code class="language-python"><span class="line">import numpy as np</span>
<span class="line">from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score</span>
<span class="line"></span>
<span class="line"># 1) Create a clean dataset</span>
<span class="line">y_true_clean = np.linspace(10, 20, 50)</span>
<span class="line">y_pred_clean = y_true_clean + np.random.normal(0, 0.8, size=len(y_true_clean))</span>
<span class="line"></span>
<span class="line">def report(title, y_true, y_pred):</span>
<span class="line">    mse = mean_squared_error(y_true, y_pred)</span>
<span class="line">    mae = mean_absolute_error(y_true, y_pred)</span>
<span class="line">    r2 = r2_score(y_true, y_pred)</span>
<span class="line">    print(f&quot;{title:18s}  MSE={mse:6.3f}  MAE={mae:5.3f}  R2={r2:5.3f}&quot;)</span>
<span class="line"></span>
<span class="line">report(&quot;Without Outlier&quot;, y_true_clean, y_pred_clean)</span>
<span class="line"></span>
<span class="line"># 2) Inject one extreme outlier (true value 100, predicted 50)</span>
<span class="line">y_true_outlier = np.append(y_true_clean, 100)</span>
<span class="line">y_pred_outlier = np.append(y_pred_clean, 50)</span>
<span class="line"></span>
<span class="line">report(&quot;With Outlier&quot;, y_true_outlier, y_pred_outlier)</span></code></pre>
<p><strong>Typical Output</strong>:</p>
<pre><code><span class="line">Without Outlier     MSE= 0.631  MAE=0.642  R2=0.985</span>
<span class="line">With Outlier        MSE=82.641  MAE=1.624  R2=-0.250</span></code></pre>
<p><strong>Interpreting the Experiment</strong>:</p>
<ul>
<li><strong>MSE</strong> skyrockets from 0.631 to 82.641. It&#39;s &quot;detonated&quot; by a single outlier because squaring the error massively amplifies its impact.</li>
<li><strong>MAE</strong> increases gently from 0.642 to 1.624, demonstrating its robustness against outliers.</li>
<li><strong>R²</strong> drops to a negative value. This means that after adding the outlier, the model&#39;s performance is worse than the baseline model of simply predicting the mean of all true values.</li>
</ul>
<p>This experiment shows that when your business cannot tolerate large errors, MSE/RMSE serves as a more sensitive sentinel. However, if you want to assess overall model performance without being skewed by a few anomalies, MAE/MedAE is a more robust choice.</p>
<h3>5. How to Choose the Right Metric for Your Project: A Scenario-Based Guide</h3>
<p>Choosing the right metric begins with understanding your business needs and data characteristics. Here, we provide a scenario-based guide by directly answering the key questions posed earlier.</p>
<h4>Scenario 1: Are we more afraid of being &quot;off by 1° on average&quot; or &quot;off by 5° occasionally&quot;?</h4>
<p>This question gets to the heart of your model&#39;s error tolerance, especially its sensitivity to large errors.</p>
<ul>
<li><p><strong>If you&#39;re more afraid of being &quot;off by 5° occasionally&quot; (High-Risk Aversion):</strong></p>
<ul>
<li><strong>Primary Metrics: MSE / RMSE</strong></li>
<li><strong>Reasoning</strong>: Mean Squared Error (MSE) and its square root (RMSE) square the error term. This means a 5° error ($5^2=25$) contributes 25 times more to the total error than a 1° error ($1^2=1$). This makes MSE/RMSE extremely sensitive to large errors and outliers, acting like an alarm system that penalizes any significant deviation harshly.</li>
<li><strong>Applicable Industries</strong>: Financial risk management (e.g., predicting default losses), industrial manufacturing (e.g., predicting equipment failure times), and weather forecasting. In these fields, a single extreme error can be very costly.</li>
</ul>
</li>
<li><p><strong>If you&#39;re more concerned with being &quot;off by 1° on average&quot; (Stability Priority):</strong></p>
<ul>
<li><strong>Primary Metrics: MAE / MedAE</strong></li>
<li><strong>Reasoning</strong>: Mean Absolute Error (MAE) treats all errors equally, calculating their average linearly. A 5° error is simply 5 times worse than a 1° error. This allows MAE to provide a more robust measure of the model&#39;s general performance, without being skewed by a few extreme values. Median Absolute Error (MedAE) is even more robust, as it is completely insensitive to outliers.</li>
<li><strong>Applicable Industries</strong>: Retail sales forecasting, inventory management, and human resources planning. In these areas, stakeholders often care more about the overall, expected deviation rather than being misled by a few anomalous transactions.</li>
</ul>
</li>
</ul>
<h4>Scenario 2: Does management care about &quot;relative percentages&quot; or &quot;absolute values&quot;?</h4>
<p>This question is about the metric&#39;s audience and its ease of communication.</p>
<ul>
<li><p><strong>If your audience thinks in &quot;relative percentages&quot;:</strong></p>
<ul>
<li><strong>Primary Metrics: MAPE / SMAPE</strong></li>
<li><strong>Reasoning</strong>: Mean Absolute Percentage Error (MAPE) presents the error as a percentage, which is highly intuitive, especially for reporting to non-technical stakeholders (e.g., &quot;Our sales forecast is accurate to within 5%&quot;). It&#39;s also excellent for comparing performance across different scales, such as forecasting sales for a $10 product versus a $1,000 product.</li>
<li><strong>Note</strong>: If your true values can be zero, use Symmetric MAPE (SMAPE) or another metric to avoid division-by-zero errors.</li>
</ul>
</li>
<li><p><strong>If business decisions depend on &quot;absolute values&quot;:</strong></p>
<ul>
<li><strong>Primary Metrics: RMSE / MAE</strong></li>
<li><strong>Reasoning</strong>: The units of these metrics are the same as your target variable (e.g., the RMSE for a housing price model is in &quot;dollars&quot;; the MAE for inventory prediction is in &quot;units&quot;). This allows business teams to directly relate the model&#39;s error to financial metrics like cost and profit, enabling more concrete decisions (e.g., &quot;The model&#39;s average error is $1,000, which is within our acceptable range&quot;).</li>
</ul>
</li>
</ul>
<h4>Scenario 3: What are the characteristics of my data distribution?</h4>
<p>The nature of your data is a critical technical prerequisite for selecting a metric.</p>
<ul>
<li><p><strong>If your data has a long-tail distribution (e.g., housing prices, web traffic, personal income):</strong></p>
<ul>
<li><strong>Primary Metric: RMSLE (Root Mean Squared Log Error)</strong></li>
<li><strong>Reasoning</strong>: For this type of data, we often care more about relative errors than absolute ones. For example, predicting a $1M house as $1.1M (a $100k error) is different from predicting a $10M house as $10.1M (also a $100k error). RMSLE addresses this by taking the logarithm of the predictions and actual values, effectively turning absolute differences into relative ones. It also naturally penalizes underestimation more than overestimation.</li>
</ul>
</li>
<li><p><strong>If your data contains many important zero values:</strong></p>
<ul>
<li><strong>Primary Metrics: MAE / RMSE</strong></li>
<li><strong>Reasoning</strong>: As mentioned, MAPE fails when the actual value is zero. In this case, MAE or RMSE, which directly evaluate the absolute error, are the safest and most straightforward choices.</li>
</ul>
</li>
</ul>
<h4>Summary: A Decision-Making Flowchart</h4>
<ol>
<li><strong>Qualitative Analysis (Business Dialogue)</strong>: Start by talking to stakeholders to clarify error tolerance and reporting habits, addressing Scenarios 1 and 2.</li>
<li><strong>Quantitative Analysis (Data Exploration)</strong>: Plot a histogram of your data to check for long tails, outliers, or zero values, addressing Scenario 3.</li>
<li><strong>Monitor Multiple Metrics (Model Iteration)</strong>: During training and validation, track 2-3 key metrics simultaneously (e.g., a robust metric like MAE, a sensitive one like RMSE, and a business-friendly one like MAPE) to gain a comprehensive understanding of your model&#39;s behavior.</li>
<li><strong>Final Selection (Deployment)</strong>: Based on the analysis, choose 1-2 of the most critical metrics to serve as the final standard for model selection and online monitoring.</li>
</ol>
<h3>Conclusion</h3>
<p>There is no single &quot;best&quot; metric—only the &quot;most appropriate&quot; one for your specific context. A deep understanding of the mathematical principles and business intuition behind each metric is the bridge that connects your model to real-world value. We hope this guide helps you choose and interpret evaluation metrics with more confidence in your future regression projects, leading to more powerful and reliable models.</p>

    </article>
    </main>
</div>

<script src="https://cdn.jsdelivr.net/npm/prismjs@1/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-python.min.js"></script>
<script>
Prism.highlightAll();
document.querySelectorAll('article pre').forEach(pre => {
    const code = pre.querySelector('code');
    if (!code) return;
    const toolbar = document.createElement('div');
    toolbar.className = 'code-toolbar';
    const btn = document.createElement('button');
    btn.className = 'copy-btn';
    btn.textContent = 'Copy';
    btn.onclick = () => {
        navigator.clipboard.writeText(code.textContent).then(() => {
            btn.textContent = 'Copied!';
            btn.classList.add('copied');
            setTimeout(() => { btn.textContent = 'Copy'; btn.classList.remove('copied'); }, 2000);
        });
    };
    toolbar.appendChild(btn);
    pre.parentNode.insertBefore(toolbar, pre);
});
</script>
</body>
</html>