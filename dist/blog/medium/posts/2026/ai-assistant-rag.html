<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building a RAG-Powered AI Assistant for My Personal Website</title>
    <style>
        body { font-family: Georgia, serif; max-width: 700px; margin: 2rem auto; padding: 0 1rem; line-height: 1.7; color: #333; }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; }
        h2 { font-size: 1.4rem; margin-top: 2rem; }
        pre { background: #f4f4f4; padding: 1rem; overflow-x: auto; border-radius: 4px; }
        code { font-family: Menlo, Monaco, monospace; font-size: 0.9em; }
        p code { background: #f4f4f4; padding: 0.2em 0.4em; border-radius: 3px; }
        img { max-width: 100%; }
        blockquote { border-left: 3px solid #ddd; margin-left: 0; padding-left: 1rem; color: #666; }
        ul { padding-left: 1.5rem; }
        hr { border: none; border-top: 1px solid #ddd; margin: 2rem 0; }
        a { color: #1a8917; }
    </style>
</head>
<body>
<hr>
<h2>date: 2026-01-28
tags: [ai, rag, cloudflare-workers, javascript, llm]
description: Building a RAG-powered AI assistant for my personal website using hybrid search, Cloudflare Workers, and OpenAI API.</h2>
<h1>Building a RAG-Powered AI Assistant for My Personal Website</h1>
<p>I recently added an AI chat assistant to my personal website that can answer questions about my blog posts, projects, and background. This post documents the implementation journey, from architecture design to security considerations.</p>
<h2>The Goal</h2>
<p>Create a floating chat widget that:</p>
<ul>
<li>Answers questions about blog content using RAG (Retrieval-Augmented Generation)</li>
<li>Provides friendly technical discussions</li>
<li>Handles sensitive topics gracefully</li>
<li>Works entirely on static hosting (GitHub Pages) with Cloudflare Workers as the API layer</li>
</ul>
<h2>Architecture Overview</h2>
<blockquote><code>┌─────────────────────────────────────────────────────────────────┐</code><br>
<code>│                        Browser (Frontend)                        │</code><br>
<code>├─────────────────────────────────────────────────────────────────┤</code><br>
<code>│  Chat Widget ──────▶ Search Client                              │</code><br>
<code>│       │              ├─ BM25 Keyword Search (local)             │</code><br>
<code>│       │              └─ Voy WASM Semantic Search                │</code><br>
<code>│       │                        │                                │</code><br>
<code>│       │◀───────────────────────┘ (Top 3 chunks as context)      │</code><br>
<code># ... (17 more lines)</code></blockquote>
<p><em>Full code available in the <a href="https://github.com/geyuxu">GitHub repository</a>.</em></p><h2>Implementation</h2>
<h3>1. Chat Widget (Frontend)</h3>
<p>The chat widget is a self-contained JavaScript module that creates a floating bubble UI:</p>
<blockquote><code>export class ChatWidget {</code><br>
<code>    constructor() {</code><br>
<code>        this.messages = [];</code><br>
<code>        this.isOpen = false;</code><br>
<code>        this.searchClient = null;</code><br>
<code>    }</code><br>
<code></code><br>
<code>    async sendMessage(text) {</code><br>
<code># ... (21 more lines)</code></blockquote>
<p><em>Full code available in the <a href="https://github.com/geyuxu">GitHub repository</a>.</em></p><p>Key features:</p>
<ul>
<li><strong>Markdown rendering</strong>: Converts <code>**bold**</code> and <code>[links](url)</code> to HTML</li>
<li><strong>CSS-in-JS</strong>: All styles are injected dynamically, no external dependencies</li>
<li><strong>ESC to close</strong>: Keyboard accessibility</li>
<li><strong>Mobile responsive</strong>: Adapts to screen size</li>
</ul>
<h3>2. Hybrid Search for RAG</h3>
<p>I already had a search system built with:</p>
<ul>
<li><strong>BM25 keyword search</strong>: Local inverted index for exact term matching</li>
<li><strong>Voy WASM semantic search</strong>: Vector similarity using pre-computed embeddings</li>
<li><strong>RRF fusion</strong>: Combines both results using Reciprocal Rank Fusion</li>
</ul>
<p>The chat widget reuses this existing infrastructure:</p>
<blockquote><code>const [keywordResults, semanticResults] = await Promise.all([</code><br>
<code>    this.keywordSearch(query, limit * 2),</code><br>
<code>    this.semanticSearch(query, limit * 2),</code><br>
<code>]);</code><br>
<code></code><br>
<code>// Merge using RRF</code><br>
<code>for (const result of keywordResults) {</code><br>
<code>    rrfScores[result.url] = keywordWeight / (k + result.rank);</code><br>
<code>}</code><br>
<code>for (const result of semanticResults) {</code><br>
<code>    rrfScores[result.url] += semanticWeight / (k + result.rank);</code><br>
<code>}</code></blockquote><h3>3. Cloudflare Worker (API Proxy)</h3>
<p>The Worker handles two endpoints:</p>
<p><strong><code>/api/embedding</code></strong> - Converts query text to vector for semantic search:</p>
<blockquote><code>const response = await fetch(&#039;https://api.openai.com/v1/embeddings&#039;, {</code><br>
<code>    method: &#039;POST&#039;,</code><br>
<code>    headers: {</code><br>
<code>        &#039;Authorization&#039;: `Bearer ${env.OPENAI_API_KEY}`,</code><br>
<code>        &#039;Content-Type&#039;: &#039;application/json&#039;,</code><br>
<code>    },</code><br>
<code>    body: JSON.stringify({</code><br>
<code>        model: &#039;text-embedding-3-small&#039;,</code><br>
<code>        input: text,</code><br>
<code>        dimensions: 512,</code><br>
<code>    }),</code><br>
<code>});</code></blockquote><p><strong><code>/api/chat</code></strong> - Chat completion with RAG context:</p>
<blockquote><code>const systemMessage = context</code><br>
<code>    ? `${env.system_prompt}\n\n## Related blog content:\n${context}`</code><br>
<code>    : env.system_prompt;</code><br>
<code></code><br>
<code>const response = await fetch(&#039;https://api.openai.com/v1/chat/completions&#039;, {</code><br>
<code>    method: &#039;POST&#039;,</code><br>
<code>    headers: { &#039;Authorization&#039;: `Bearer ${env.OPENAI_API_KEY}` },</code><br>
<code>    body: JSON.stringify({</code><br>
<code>        model: &#039;gpt-4o-mini&#039;,</code><br>
<code>        messages: [</code><br>
<code>            { role: &#039;system&#039;, content: systemMessage },</code><br>
<code>            ...messages,</code><br>
<code>        ],</code><br>
<code>    }),</code><br>
<code>});</code></blockquote><h3>4. Security: Handling Sensitive Topics</h3>
<p>For a personal website, I needed the assistant to:</p>
<ul>
<li>Freely discuss the website owner (me)</li>
<li>Refuse political/sensitive topics gracefully</li>
</ul>
<p>This is handled in the system prompt (stored as environment variable):</p>
<blockquote><code>## About the website owner</code><br>
<code>Yuxu Ge is the website owner. You can freely discuss:</code><br>
<code>- Professional background, technical experience, projects</code><br>
<code>- Blog content, technical opinions</code><br>
<code>- Public information (education, work history)</code><br>
<code></code><br>
<code>## Conversation boundaries</code><br>
<code>Politely decline and redirect for:</code><br>
<code># ... (10 more lines)</code></blockquote>
<p><em>Full code available in the <a href="https://github.com/geyuxu">GitHub repository</a>.</em></p><h3>5. Conversation History with localStorage</h3>
<p>For better UX, the chat widget persists conversation history in the browser&#39;s localStorage:</p>
<blockquote><code>const CHAT_CONFIG = {</code><br>
<code>    storageKey: &#039;chat_history&#039;,</code><br>
<code>    historyTTL: 24 * 60 * 60 * 1000, // 24 hours</code><br>
<code>};</code><br>
<code></code><br>
<code>// Save after each successful response</code><br>
<code>saveHistory() {</code><br>
<code>    const data = {</code><br>
<code># ... (22 more lines)</code></blockquote>
<p><em>Full code available in the <a href="https://github.com/geyuxu">GitHub repository</a>.</em></p><p>Features:</p>
<ul>
<li><strong>Auto-save</strong>: Saves after each assistant response</li>
<li><strong>Auto-restore</strong>: Restores conversation when page loads</li>
<li><strong>24-hour TTL</strong>: Automatically clears stale conversations</li>
<li><strong>Clear button</strong>: Manual clear via trash icon in header</li>
</ul>
<p>This is a pragmatic choice over server-side storage (Cloudflare KV) because:</p>
<ul>
<li>Most visitors have one-time conversations</li>
<li>No user identification needed</li>
<li>Zero additional cost</li>
<li>Simpler implementation</li>
</ul>
<h2>Lessons Learned</h2>
<ol>
<li><p><strong>Environment variables &gt; hardcoded prompts</strong>: Storing <code>system_prompt</code> as a Cloudflare environment variable allows prompt iteration without code deployment.</p>
</li>
<li><p><strong>Reuse existing search infrastructure</strong>: Building RAG on top of an existing hybrid search system saved significant effort.</p>
</li>
<li><p><strong>Nuanced content filtering</strong>: Initial attempts at sensitive topic filtering were too aggressive (blocking questions about myself). The key is explicitly whitelisting allowed topics.</p>
</li>
<li><p><strong>Markdown in chat</strong>: Simple regex-based markdown parsing is sufficient for bold and links. No need for heavy libraries.</p>
</li>
<li><p><strong>Start simple with state management</strong>: localStorage is sufficient for conversation history. Server-side storage (KV) adds complexity without clear benefit for a personal site.</p>
</li>
</ol>
<h2>Cost Analysis</h2>
<p>With <code>gpt-4o-mini</code> and <code>text-embedding-3-small</code>:</p>
<ul>
<li>Embedding: ~$0.00002 per query</li>
<li>Chat: ~$0.0001-0.0005 per response (depending on context length)</li>
<li>Estimated monthly cost: &lt; $5 for moderate traffic</li>
</ul>
<h2>Next Steps</h2>
<ul>
<li><input disabled="" type="checkbox"> Add streaming responses for better UX</li>
<li><input checked="" disabled="" type="checkbox"> <del>Implement conversation memory</del> (Done with localStorage)</li>
<li><input disabled="" type="checkbox"> Add usage analytics</li>
<li><input disabled="" type="checkbox"> Support image understanding for blog screenshots</li>
</ul>
<p>The complete implementation is open source in my website repository. Feel free to adapt it for your own projects!</p>

</body>
</html>