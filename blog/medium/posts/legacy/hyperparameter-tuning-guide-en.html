<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Comprehensive Guide to Hyperparameter Tuning in Machine Learning: From Theory to Practice</title>
    <style>
        body { font-family: Georgia, serif; max-width: 700px; margin: 2rem auto; padding: 0 1rem; line-height: 1.7; color: #333; }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; }
        h2 { font-size: 1.4rem; margin-top: 2rem; }
        pre { background: #f4f4f4; padding: 1rem; overflow-x: auto; border-radius: 4px; }
        code { font-family: Menlo, Monaco, monospace; font-size: 0.9em; }
        p code { background: #f4f4f4; padding: 0.2em 0.4em; border-radius: 3px; }
        img { max-width: 100%; }
        blockquote { border-left: 3px solid #ddd; margin-left: 0; padding-left: 1rem; color: #666; }
        ul { padding-left: 1.5rem; }
        hr { border: none; border-top: 1px solid #ddd; margin: 2rem 0; }
        a { color: #1a8917; }
    </style>
</head>
<body>
<hr>
<h2>date: 2024-01-01
tags: [ai, machine learning, hyperparameter tuning, python, pytorch, optuna, scikit-learn]
legacy: true</h2>
<h1>A Comprehensive Guide to Hyperparameter Tuning in Machine Learning: From Theory to Practice</h1>
<h2>2. Tuning Methods: Overview and Code Examples</h2>
<p>Choosing the right tuning strategy is an art of balancing exploration (covering a wider search space) and exploitation (finer search in promising areas). Let&#39;s delve into the mainstream methods one by one, complete with runnable code snippets.</p>
<h3>2.1 Grid Search</h3>
<p><strong>Core Idea</strong>: A brute-force enumeration of all possible combinations (Cartesian product) of the provided hyperparameter values. It&#39;s the most reliable method when the number of dimensions is small.</p>
<blockquote><code># pip install scikit-learn</code><br>
<code>from sklearn.model_selection import GridSearchCV</code><br>
<code>from sklearn.svm import SVC</code><br>
<code></code><br>
<code># Assume X_train, y_train are loaded</code><br>
<code># from sklearn.datasets import make_classification</code><br>
<code># X_train, y_train = make_classification(n_samples=1000, n_features=20, n_informative=10, random_state=42)</code><br>
<code></code><br>
<code># ... (15 more lines)</code></blockquote>
<p><em>Full code available in the <a href="https://github.com/geyuxu">GitHub repository</a>.</em></p><p><strong>Tip</strong>: Start with a coarse grid to scan a wide range, then define a finer, more localized grid around the best-performing region.</p>
<h3>2.2 Random Search</h3>
<p><strong>Core Idea</strong>: Unlike Grid Search, Random Search samples a fixed number of parameter combinations from specified distributions. It is generally more efficient than Grid Search, especially when dealing with a large number of hyperparameters.</p>
<blockquote><code># pip install scikit-learn scipy</code><br>
<code>from sklearn.model_selection import RandomizedSearchCV</code><br>
<code>from sklearn.ensemble import GradientBoostingClassifier</code><br>
<code>from scipy.stats import loguniform, randint</code><br>
<code></code><br>
<code># Assume X_train, y_train are loaded</code><br>
<code>param_dist = {</code><br>
<code>    &quot;learning_rate&quot;: loguniform(1e-4, 1e-1),</code><br>
<code># ... (14 more lines)</code></blockquote>
<p><em>Full code available in the <a href="https://github.com/geyuxu">GitHub repository</a>.</em></p><p><strong>Key Insight</strong>: For hyperparameters sensitive to their order of magnitude, like learning rate and regularization strength, sampling from a log-uniform distribution (<code>loguniform</code>) is more effective than a linear uniform distribution.</p>
<h3>2.3 Bayesian Optimization</h3>
<p><strong>Core Idea</strong>: This is a more intelligent search strategy. It uses a probabilistic model (a surrogate) to model the hyperparameter-to-performance function and leverages historical evaluation results to select the next most promising point to evaluate. This allows it to approach the optimal solution in fewer iterations. <code>Optuna</code> is a popular library for this.</p>
<blockquote><code># pip install optuna torch torchvision</code><br>
<code>import torch</code><br>
<code>import torch.nn as nn</code><br>
<code>import optuna</code><br>
<code>from torchvision.datasets import MNIST</code><br>
<code>from torch.utils.data import DataLoader</code><br>
<code>from torchvision import transforms</code><br>
<code></code><br>
<code># ... (50 more lines)</code></blockquote>
<p><em>Full code available in the <a href="https://github.com/geyuxu">GitHub repository</a>.</em></p><p><strong>Pro Tip</strong>: Combine this with <code>optuna.pruners</code> to terminate unpromising trials early, significantly saving computational resources.</p>
<h3>2.4 Early-Stopping Based Algorithms (Successive Halving / Hyperband)</h3>
<p><strong>Core Idea</strong>: These algorithms aim to speed up the search through dynamic resource allocation. They start by allocating a small amount of resources (e.g., a few training epochs) to many configurations, then eliminate the poor performers and allocate more resources only to the &quot;survivors.&quot;</p>
<blockquote><code># pip install scikit-learn</code><br>
<code>from sklearn.experimental import enable_halving_search_cv</code><br>
<code>from sklearn.model_selection import HalvingGridSearchCV</code><br>
<code>from sklearn.ensemble import RandomForestClassifier</code><br>
<code></code><br>
<code># Assume X_train, y_train are loaded</code><br>
<code>param_grid = {&quot;max_depth&quot;: [5, 10, 15, None],</code><br>
<code>              &quot;min_samples_leaf&quot;: [1, 2, 4]}</code><br>
<code># ... (11 more lines)</code></blockquote>
<p><em>Full code available in the <a href="https://github.com/geyuxu">GitHub repository</a>.</em></p><p>For deep learning, <code>KerasTuner</code>&#39;s <code>Hyperband</code> or <code>Optuna</code>&#39;s <code>SuccessiveHalvingPruner</code> are more natural choices, as they can use <code>epoch</code> as the resource dimension.</p>
<h3>2.5 Population-Based Training (PBT)</h3>
<p><strong>Core Idea</strong>: This is an advanced hybrid strategy, often used in large-scale distributed training. It trains a group of models (a &quot;population&quot;) in parallel. Periodically, it replaces the weights of poor-performing models with those of high-performing models, while also applying small random perturbations (&quot;mutations&quot;) to their hyperparameters.</p>
<blockquote><code># pip install &quot;ray[tune]&quot; torch</code><br>
<code>from ray import tune</code><br>
<code>from ray.tune.schedulers import PopulationBasedTraining</code><br>
<code></code><br>
<code>def train_model(config):</code><br>
<code>    # Model, data loading, and training loop definition omitted here</code><br>
<code>    # The training loop needs to report validation metrics via tune.report()</code><br>
<code>    # e.g., tune.report(mean_accuracy=acc)</code><br>
<code># ... (21 more lines)</code></blockquote>
<p><em>Full code available in the <a href="https://github.com/geyuxu">GitHub repository</a>.</em></p><p>The power of PBT lies in its ability not only to optimize hyperparameters but also to learn effective learning rate schedules online.</p>
<h2>3. Common Hyperparameters and Their Impact</h2>
<p>Understanding how different hyperparameters affect model behavior is key to making informed tuning decisions.</p>
<p><img src="/blog/images/tables/table--38cad9a.png" alt="Table"></p><h2>4. Practical Tuning Tips</h2>
<ol>
<li><strong>Set the Main Direction</strong>: Have a lot of data? Prioritize increasing model capacity. Limited data? Focus on regularization or data augmentation first.</li>
<li><strong>Tune in Groups</strong>: Don&#39;t try to tune everything at once. Start with optimization-related parameters (like learning rate, batch size), then move to model architecture, and finally, tune regularization.</li>
<li><strong>Use a Logarithmic Scale</strong>: For hyperparameters like learning rate and regularization strength, searching on a log scale (e.g., from <code>1e-5</code> to <code>1e-1</code>) is far more efficient than a linear scale.</li>
<li><strong>Visualize and Use Early Stopping</strong>: Monitor training/validation curves with tools like TensorBoard or WandB. If the validation loss stops decreasing or starts to rise, consider stopping the training early or increasing regularization.</li>
<li><strong>Leverage Existing Work</strong>: Start with the default configurations from relevant papers or open-source repositories. They are often a great baseline. Fine-tune within the same order of magnitude first.</li>
<li><strong>Prioritize Resources</strong>: First, determine the maximum batch size and model size your hardware (especially GPU memory) can handle. Then, fine-tune other parameters within these constraints.</li>
</ol>
<h2>5. Summary and Decision Tree</h2>
<p>How to choose the right tuning method?</p>
<ol>
<li><strong>Dimensions ≤ 3, small dataset, CPU training?</strong> → Go straight for <code>GridSearchCV</code>.</li>
<li><strong>Want a quick 80% solution?</strong> → <code>RandomizedSearchCV</code> with log-uniform sampling is your friend.</li>
<li><strong>Training on a GPU with a limited budget?</strong> → <code>Optuna</code> (TPE) with pruning or <code>Hyperband</code> is the most efficient choice.</li>
<li><strong>Have a large distributed cluster?</strong> → <code>Ray Tune</code>&#39;s PBT will unleash its full power.</li>
<li><strong>Doing academic research or need extreme fine-tuning?</strong> → You can explore hypergradient optimization, but be prepared for its complexity.</li>
</ol>
<p>By combining this theoretical knowledge with hands-on code examples, you will be able to perform hyperparameter tuning more systematically and efficiently, leading to significant improvements in your model&#39;s performance.</p>

</body>
</html>