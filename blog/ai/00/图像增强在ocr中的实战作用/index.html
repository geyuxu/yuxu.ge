<!DOCTYPE html><html lang="zh" data-astro-cid-bvzihdzo> <head><!-- Global Metadata --><meta charset="utf-8"><!--<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">--><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"><link rel="shortcut icon" href="favicon.png" type="image/png"><link rel="sitemap" href="/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" title="Ge Yuxu • AI &#38; Engineering" href="https://geyuxu.com/rss.xml"><meta name="generator" content="Astro v5.7.5"><!-- Font preloads --><link rel="preload" href="/fonts/atkinson-regular.woff" as="font" type="font/woff" crossorigin><link rel="preload" href="/fonts/atkinson-bold.woff" as="font" type="font/woff" crossorigin><!-- Canonical URL --><link rel="canonical" href="https://geyuxu.com/blog/ai/00/%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA%E5%9C%A8ocr%E4%B8%AD%E7%9A%84%E5%AE%9E%E6%88%98%E4%BD%9C%E7%94%A8/"><!-- Primary Meta Tags --><title>图像增强在 OCR 中的实战作用</title><meta name="title" content="图像增强在 OCR 中的实战作用"><meta name="description"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://geyuxu.com/blog/ai/00/%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA%E5%9C%A8ocr%E4%B8%AD%E7%9A%84%E5%AE%9E%E6%88%98%E4%BD%9C%E7%94%A8/"><meta property="og:title" content="图像增强在 OCR 中的实战作用"><meta property="og:description"><meta property="og:image" content="https://geyuxu.com/blog-placeholder-1.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://geyuxu.com/blog/ai/00/%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA%E5%9C%A8ocr%E4%B8%AD%E7%9A%84%E5%AE%9E%E6%88%98%E4%BD%9C%E7%94%A8/"><meta property="twitter:title" content="图像增强在 OCR 中的实战作用"><meta property="twitter:description"><meta property="twitter:image" content="https://geyuxu.com/blog-placeholder-1.jpg"><script src="/js/jquery-3.7.1.min.js"></script><script is:global>
	window.addEventListener('DOMContentLoaded', () => {
		$('.toc ol').css({
		'list-style': 'none',   // 隐藏 1. 2. 3.
		'margin': 0,
		'padding-left': 0,       // 可按需调整
		});
		$('.toc ol > li').css({
		'list-style': 'none',   // 隐藏 1. 2. 3.
		'padding-left': 10  
		});
        $('.sidebar').append($('.toc'));
      });
	</script><style>main[data-astro-cid-bvzihdzo].page{display:grid;grid-template-columns:260px minmax(0,1fr);width:100%;margin:0}aside[data-astro-cid-bvzihdzo].sidebar{box-sizing:border-box;width:260px;padding:2rem 1rem;font-size:.95rem;position:sticky;top:4rem;align-self:start}.sidebar[data-astro-cid-bvzihdzo] .meta[data-astro-cid-bvzihdzo] p[data-astro-cid-bvzihdzo]{margin:.25rem 0}nav[data-astro-cid-bvzihdzo].toc li[data-astro-cid-bvzihdzo]{margin:.35rem 0 .35rem 1rem}nav[data-astro-cid-bvzihdzo].toc a[data-astro-cid-bvzihdzo]{color:var(--gray-dark,#444);text-decoration:none}nav[data-astro-cid-bvzihdzo].toc a[data-astro-cid-bvzihdzo]:hover{text-decoration:underline}.content-wrapper[data-astro-cid-bvzihdzo]{display:flex;justify-content:center;padding:2rem 1rem}article[data-astro-cid-bvzihdzo].prose{max-width:740px;width:100%}@media (max-width: 768px){main[data-astro-cid-bvzihdzo].page{grid-template-columns:1fr}aside[data-astro-cid-bvzihdzo].sidebar{position:static;width:100%;padding:1rem}.content-wrapper[data-astro-cid-bvzihdzo]{justify-content:flex-start}article[data-astro-cid-bvzihdzo].prose{max-width:100%}}.series-list[data-astro-cid-bvzihdzo]{list-style:none;margin:0;padding-left:10px}.series-list[data-astro-cid-bvzihdzo]{list-style:none;margin:0;padding-left:10px;max-height:calc(16em + .5rem);overflow-y:auto}.series-list[data-astro-cid-bvzihdzo]::-webkit-scrollbar{width:6px}.series-list[data-astro-cid-bvzihdzo]::-webkit-scrollbar-thumb{background:#0003;border-radius:3px}.content[data-astro-cid-7jjqptxk]{max-width:720px;margin:0 auto;padding:2rem 1rem;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,sans-serif;font-size:1.05rem;line-height:1.75;color:#333}.content[data-astro-cid-7jjqptxk] h1[data-astro-cid-7jjqptxk],.content[data-astro-cid-7jjqptxk] h2[data-astro-cid-7jjqptxk],.content[data-astro-cid-7jjqptxk] h3[data-astro-cid-7jjqptxk]{font-weight:600;margin-top:2rem;margin-bottom:1rem;line-height:1.3}.content[data-astro-cid-7jjqptxk] p[data-astro-cid-7jjqptxk]{margin-bottom:1.25rem}.content[data-astro-cid-7jjqptxk] a[data-astro-cid-7jjqptxk]{color:var(--accent, #0070f3);text-decoration:underline}.content[data-astro-cid-7jjqptxk] img[data-astro-cid-7jjqptxk]{max-width:100%;border-radius:6px;margin:1.5rem 0}.content[data-astro-cid-7jjqptxk] pre[data-astro-cid-7jjqptxk],.content[data-astro-cid-7jjqptxk] code[data-astro-cid-7jjqptxk]{font-family:Menlo,Monaco,Consolas,Courier New,monospace;background:#f4f4f4;padding:.2em .4em;border-radius:4px}.content[data-astro-cid-7jjqptxk] pre[data-astro-cid-7jjqptxk]{padding:1em;overflow-x:auto}
:root{--accent: #2337ff;--accent-dark: #000d8a;--black: 15, 18, 25;--gray: 96, 115, 159;--gray-light: 229, 233, 240;--gray-dark: 34, 41, 57;--gray-gradient: rgba(var(--gray-light), 50%), #fff;--box-shadow: 0 2px 6px rgba(var(--gray), 25%), 0 8px 24px rgba(var(--gray), 33%), 0 16px 32px rgba(var(--gray), 33%)}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-regular.woff) format("woff");font-weight:400;font-style:normal;font-display:swap}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-bold.woff) format("woff");font-weight:700;font-style:normal;font-display:swap}body{font-family:Atkinson,sans-serif;margin:0;padding:0;text-align:left;background:linear-gradient(var(--gray-gradient)) no-repeat;background-size:100% 600px;word-wrap:break-word;overflow-wrap:break-word;color:rgb(var(--gray-dark));font-size:20px;line-height:1.7}main{width:720px;max-width:calc(100% - 2em);margin:auto;padding:3em 1em}h1,h2,h3,h4,h5,h6{margin:0 0 .5rem;color:rgb(var(--black));line-height:1.2}h1{font-size:3.052em}h2{font-size:2.441em}h3{font-size:1.953em}h4{font-size:1.563em}h5{font-size:1.25em}strong,b{font-weight:700}a,a:hover{color:var(--accent)}p{margin-bottom:1em}.prose p{margin-bottom:2em}textarea{width:100%;font-size:16px}input{font-size:16px}table{width:100%}img{max-width:100%;height:auto;border-radius:8px}code{padding:2px 5px;background-color:rgb(var(--gray-light));border-radius:2px}pre{padding:1.5em;border-radius:8px}pre>code{all:unset}blockquote{border-left:4px solid var(--accent);padding:0 0 0 20px;margin:0;font-size:1.333em}hr{border:none;border-top:1px solid rgb(var(--gray-light))}@media (max-width: 720px){body{font-size:18px}main{padding:1em}}.sr-only{border:0;padding:0;margin:0;position:absolute!important;height:1px;width:1px;overflow:hidden;clip:rect(1px 1px 1px 1px);clip:rect(1px,1px,1px,1px);clip-path:inset(50%);white-space:nowrap}
a[data-astro-cid-eimmu3lg]{display:inline-block;text-decoration:none}a[data-astro-cid-eimmu3lg].active{font-weight:bolder;text-decoration:underline}header[data-astro-cid-3ef6ksr2]{margin:0;padding:0 1em;background:#fff;box-shadow:0 2px 8px rgba(var(--black),5%)}h2[data-astro-cid-3ef6ksr2]{margin:0;font-size:1em}h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2],h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none}nav[data-astro-cid-3ef6ksr2]{display:flex;align-items:center;justify-content:space-between}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{padding:1em .5em;color:var(--black);border-bottom:4px solid transparent;text-decoration:none}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none;border-bottom-color:var(--accent)}.social-links[data-astro-cid-3ef6ksr2],.social-links[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{display:flex}@media (max-width: 720px){.social-links[data-astro-cid-3ef6ksr2]{display:none}}footer[data-astro-cid-sz7xmlte]{padding:2em 1em 6em;background:linear-gradient(var(--gray-gradient)) no-repeat;color:rgb(var(--gray));text-align:center}.social-links[data-astro-cid-sz7xmlte]{display:flex;justify-content:center;gap:1em;margin-top:1em}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]{text-decoration:none;color:rgb(var(--gray))}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]:hover{color:rgb(var(--gray-dark))}
</style></head> <body data-astro-cid-bvzihdzo> <header data-astro-cid-3ef6ksr2> <nav data-astro-cid-3ef6ksr2> <!--<h2><a href="/">{SITE_TITLE}</a></h2>--> <h2 data-astro-cid-3ef6ksr2><a style="padding-left:0" href="/" data-astro-cid-3ef6ksr2>Ge Yuxu<br data-astro-cid-3ef6ksr2>AI & Engineering</a></h2> <div class="internal-links" data-astro-cid-3ef6ksr2> <a href="/" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Home </a>  <a href="/blog/1" class="active" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Blog </a>  <a href="/series" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Series </a>  <a href="/projects" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Projects </a>  </div> <div class="social-links" data-astro-cid-3ef6ksr2> <a href="https://github.com/geyuxu" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Go to Ge Yuxu's GitHub repo</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" data-astro-cid-3ef6ksr2><path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-astro-cid-3ef6ksr2></path></svg> </a> <a href="https://www.linkedin.com/in/geyuxu/" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Go to Ge Yuxu's LinkedIn profile</span> <svg viewBox="0 0 24 24" width="32" height="32" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" data-astro-cid-3ef6ksr2> <path fill="currentColor" d="M20.447 20.452H17.2v-5.569c0-1.328-.025-3.039-1.852-3.039-1.853 0-2.136 1.447-2.136 2.942v5.666h-3.248V9h3.122v1.561h.045c.435-.823 1.498-1.688 3.083-1.688 3.295 0 3.903 2.17 3.903 4.989v6.59zM5.337 7.433a1.882 1.882 0 110-3.764 1.882 1.882 0 010 3.764zm1.626 13.019H3.708V9h3.255v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.226.792 24 1.771 24h20.451C23.2 24 24 23.226 24 22.271V1.729C24 .774 23.2 0 22.222 0z" data-astro-cid-3ef6ksr2></path> </svg> </a> </div> </nav> </header>  <main class="page" data-astro-cid-bvzihdzo> <!-- 左侧栏 --> <aside class="sidebar" data-astro-cid-bvzihdzo> <div class="meta" data-astro-cid-bvzihdzo> <b data-astro-cid-bvzihdzo>图像增强在 OCR 中的实战作用</b> <p data-astro-cid-bvzihdzo><time datetime="2025-04-01T00:00:00.000Z"> Apr 1, 2025 </time></p>   </div> <hr data-astro-cid-bvzihdzo> <br data-astro-cid-bvzihdzo> </aside> <!-- 右侧正文区域（flex 居中） --> <div class="content-wrapper" data-astro-cid-bvzihdzo> <article class="prose" data-astro-cid-bvzihdzo>  <article class="content" data-astro-cid-7jjqptxk> <nav class="toc"><ol class="toc-level toc-level-1"></ol></nav><h1 id="图像增强在-ocr-中的实战作用"><a aria-hidden="true" tabindex="-1" href="#图像增强在-ocr-中的实战作用"><span class="icon icon-link"></span></a>图像增强在 OCR 中的实战作用</h1>
<p><strong>1. 模型没变，结果却差一大截？</strong></p>
<p>训练模型的人，可能都碰到过一种“迷之崩溃”：你用着一套大家都在用的网络结构，参数也调得差不多，甚至 loss 下降也挺稳，结果一到实际场景一测——识别结果惨不忍睹。</p>
<p>我第一次做 OCR 项目的时候就栽了这个坑。模型是网上开源的 CRNN+CTC，训练 loss 跌得飞快，我当时都以为自己要起飞了，结果一上线：夜间图片识别率直接掉了 20 个点，还被运维同事追着问“是不是模型坏了”。</p>
<p>后来我才发现问题根本不是模型，而是<strong>训练集太干净、太理想化了</strong>。现实世界比训练数据脏得多，而模型从来只会“学你喂的东西”，不会“脑补”。</p>
<p>于是我们决定做一波数据增强——一开始只是试试，没想到效果提升非常明显，甚至不动模型结构都能救回来。</p>
<p>这篇文章就来分享一下我们实际做过的图像增强方式、效果对比、还有一些实验记录。</p>
<hr>
<p><strong>2. 为啥 OCR 更依赖图像增强？</strong></p>
<p>图像增强这件事，其实很多做分类模型的朋友可能觉得“锦上添花”，不增强也能训。但 OCR 是另一个物种。</p>
<p><strong>OCR 有几个天然硬伤：</strong></p>
<p>• <strong>输入信息量少</strong>：一张车牌、小票、票据，本来就只有几个字符，容不得你漏一个；</p>
<p>• <strong>字符很敏感</strong>：一个像素模糊都可能把 8 看成 B，或者 3 看成 5；</p>
<p>• <strong>数据变化范围大</strong>：旋转、曝光、遮挡、脏污、低清晰度……这些都是现实 OCR 系统每天要打交道的；</p>
<p>所以 OCR 模型不光得会识别清晰的字符，更得能扛住“脏图”。</p>
<p>而图像增强，恰好就是把这些“坏场景”模拟出来的最好方式。它不会直接提升你训练集的容量，但会<strong>拉高模型的认知范围和抗噪能力</strong>，很多时候，<strong>模型训练得不好，不一定是网络不够，而是数据太“单纯”。</strong></p>
<p><strong>3. 我们用过的图像增强方法（按类型分类）</strong></p>
<p>为了让模型识别能力更贴近真实环境，我们当时尝试了几种图像增强方式，基本可以分为以下几类：</p>
<p><strong>🌀 3.1 几何变换类</strong></p>
<blockquote>
<p>模拟各种角度拍摄、图像倾斜、歪斜摆放等场景。</p>
</blockquote>
<p>• <strong>轻微旋转（±10°~15°）</strong></p>
<p>模拟车辆歪着进场、小票拍摄角度不正的情况。</p>
<p>• <strong>缩放 + 裁剪</strong></p>
<p>模拟摄像头或用户拍摄时框得不准，字符被切边或者缩小。</p>
<p>• <strong>仿射/透视变换</strong></p>
<p>稍复杂，但能很好模拟斜拍、侧拍，特别实用。</p>
<p><strong>💡 3.2 光照和色彩类</strong></p>
<blockquote>
<p>现实中拍照会遇到强光、阴影、曝光不足，必须要模拟。</p>
</blockquote>
<p>• <strong>亮度增强/降低</strong></p>
<p>模拟白天阳光强烈或夜间昏暗的图像。</p>
<p>• <strong>对比度调整</strong></p>
<p>提升或压低图像对比度，模仿脏污/强光干扰。</p>
<p>• <strong>颜色扰动（HSV 偏移）</strong></p>
<p>轻微改动色调，增加模型的颜色鲁棒性。</p>
<p><strong>🌫️ 3.3 模糊/噪声类</strong></p>
<blockquote>
<p>很多图模糊并不是摄像头坏，而是运动中拍的。</p>
</blockquote>
<p>• <strong>高斯模糊</strong></p>
<p>模拟镜头没对好焦或车牌快速移动时的拖影。</p>
<p>• <strong>椒盐噪声</strong></p>
<p>模拟图像传输压缩时的噪点，车牌压缩图经常出问题。</p>
<p>• <strong>运动模糊（Motion Blur）</strong></p>
<p>模拟车辆进出过快导致的拉丝感。</p>
<p><strong>🔧 3.4 自定义遮挡/拼接类（我们项目场景特有）</strong></p>
<p>• <strong>添加局部遮挡（半透明灰块、贴条）</strong></p>
<p>模拟有灰尘、车牌被装饰物遮挡的情况。</p>
<p>• <strong>拼接错位字符图</strong></p>
<p>把不同车牌字符截出来拼成新图，制造字符间距异常的样本。</p>
<p>这些操作虽然听着都挺“基础”的，但组合起来就是一套硬核训练法——不夸张地说，模型识别能力从“只会认干净图”变成了“脏图也能八九不离十”。</p>
<hr>
<p><strong>4. 每种增强方法对效果的影响（我们实测的结果）</strong></p>
<p>我们在一批验证集中分别测试了“不开增强”和“单种增强后训练”的结果，下面是一些实测数据（基于 CRNN+CTC OCR 结构，在 5k 训练样本上训练，验证集 1k 张）：</p>













































<table><thead><tr><th><strong>增强方式</strong></th><th><strong>识别准确率提升（对比无增强）</strong></th><th><strong>备注说明</strong></th></tr></thead><tbody><tr><td>轻微旋转</td><td>+4.8%</td><td>模型能容忍倾斜字符了</td></tr><tr><td>高斯模糊</td><td>+3.2%</td><td>对夜间图表现特别明显</td></tr><tr><td>色调扰动（HSV）</td><td>+2.9%</td><td>提高对不同车牌底色/背景适应性</td></tr><tr><td>对比度调整</td><td>+1.5%</td><td>作用不大，但不会掉精度</td></tr><tr><td>添加噪声（椒盐）</td><td>+1.8%</td><td>稳定性略提升</td></tr><tr><td>添加遮挡（随机块）</td><td>+6.1%</td><td>模型能“推测”部分被挡住的字符了</td></tr><tr><td>增强全部组合（混合）</td><td><strong>+9.3%</strong></td><td>效果最明显，但训练时间稍长</td></tr></tbody></table>
<p>最明显的提升来自两个方向：</p>
<ol>
<li>
<p><strong>旋转 + 遮挡</strong>：因为现实中这两种情况最常见；</p>
</li>
<li>
<p><strong>增强组合使用，而不是单独用一两种</strong>。</p>
</li>
</ol>
<p>而且我们还发现一个现象：<strong>增强带来的提升，在训练数据量较小时特别明显</strong>。小数据集 + 强增强，比大数据 + 弱增强的提升还要直接。</p>
<p><strong>5. 增强组合策略与使用建议</strong></p>
<p>虽然增强方式很多，但不是所有增强都适合一起乱加，尤其在 OCR 场景里，一不小心加猛了，模型反而“学乱了”。</p>
<p>以下是我们做实验后踩出来的一些组合策略建议，适合 OCR 或小样本图像任务使用：</p>
<p><strong>✅ 实用组合一：基础耐受增强</strong></p>
<p>适合任何模型做基础鲁棒性打底。</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="sql"><code><span class="line"><span style="color:#E1E4E8">✔ 轻微旋转（±</span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">°）  </span></span>
<span class="line"><span style="color:#E1E4E8">✔ 亮度调节（</span><span style="color:#F97583">+/-</span><span style="color:#79B8FF">20</span><span style="color:#E1E4E8">%）  </span></span>
<span class="line"><span style="color:#E1E4E8">✔ 色调扰动（HSV 小幅偏移）  </span></span>
<span class="line"><span style="color:#E1E4E8">✔ 高斯模糊（</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">~2px）</span></span></code></pre>
<p>✅ 效果稳定、几乎不会破坏字符形状，是我们默认打开的一组增强。</p>
<p><strong>✅ 实用组合二：脏图专用抗压训练</strong></p>
<p>适合针对实际部署环境“画面脏、反光多、模糊严重”的场景。</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="sql"><code><span class="line"><span style="color:#E1E4E8">✔ 遮挡模拟（随机加灰块）  </span></span>
<span class="line"><span style="color:#E1E4E8">✔ 运动模糊（水平或竖直）  </span></span>
<span class="line"><span style="color:#E1E4E8">✔ 对比度压缩  </span></span>
<span class="line"><span style="color:#E1E4E8">✔ 背景扰动（局部添加噪声）</span></span></code></pre>
<p>⚠ 这组增强会加大模型训练难度，建议在前面训练过一轮、或者模型已经“稳住”之后再加入。</p>
<p><strong>⚙ 增强策略建议（我们自己的经验）：</strong></p>
<p>• <strong>前 10 轮训练不要加太重的增强</strong></p>
<p>模型刚开始还没学会基本形状，加太多干扰会迷糊；</p>
<p>• <strong>中期可以分批加增强</strong></p>
<p>我们是每 5~10 个 epoch 动态开启一种新增强，逐步加压；</p>
<p>• <strong>后期增强可适当减弱，避免过拟合在异常图上</strong></p>
<p>最终测试集效果不好，有时不是模型退化，而是训练图太“花”了。</p>
<hr>
<p><strong>6. 增强不是万能，但在 OCR 里它几乎是刚需</strong></p>
<p>最后总结几句。</p>
<p>说实话，图像增强这个事，在很多模型训练工程师眼里都算“杂活”，不够高大上，也不像搞 Transformer 那样显得很厉害。但我们项目做完之后最大的感受是：</p>
<blockquote>
<p>在 OCR 这类任务里，增强不只是提升模型的一种方式，很多时候是它能不能「活下来」的前提。</p>
</blockquote>
<p>我们曾经尝试不加任何增强，只用“干净数据”训练，模型在验证集上表现很好，但上线不到一小时就开始疯狂识错。最后一轮带增强训练后的模型，虽然训练 loss 跌得慢了一点，但实际稳定得多——尤其在夜间、雨天、边缘图像这些“真实世界的黑暗面”。</p>
<p>所以，如果你也正在做 OCR 项目，或者是小样本图像分类，不妨认真想一想：</p>
<p>与其花三天调网络结构，可能还不如花一天，好好做做数据增强。</p> </article>   </article> </div> </main> <footer data-astro-cid-sz7xmlte>
&copy; 2025 All rights reserved.
</footer>  </body></html>