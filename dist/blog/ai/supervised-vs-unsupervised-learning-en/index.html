<!DOCTYPE html><html lang="zh"> <head><!-- Global Metadata --><meta charset="utf-8"><!--<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">--><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"><link rel="shortcut icon" href="/favicon.png" type="image/png"><link rel="sitemap" href="/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" title="Ge Yuxu • AI &#38; Engineering" href="https://geyuxu.com/rss.xml"><meta name="generator" content="Astro v5.5.6"><!-- Font preloads --><link rel="preload" href="/fonts/atkinson-regular.woff" as="font" type="font/woff" crossorigin><link rel="preload" href="/fonts/atkinson-bold.woff" as="font" type="font/woff" crossorigin><!-- Canonical URL --><link rel="canonical" href="https://geyuxu.com/blog/ai/supervised-vs-unsupervised-learning-en/"><!-- Primary Meta Tags --><title>Supervised vs. Unsupervised Learning: Concepts, Algorithms, and Practice</title><meta name="title" content="Supervised vs. Unsupervised Learning: Concepts, Algorithms, and Practice"><meta name="description"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://geyuxu.com/blog/ai/supervised-vs-unsupervised-learning-en/"><meta property="og:title" content="Supervised vs. Unsupervised Learning: Concepts, Algorithms, and Practice"><meta property="og:description"><meta property="og:image" content="https://geyuxu.com/blog-placeholder-1.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://geyuxu.com/blog/ai/supervised-vs-unsupervised-learning-en/"><meta property="twitter:title" content="Supervised vs. Unsupervised Learning: Concepts, Algorithms, and Practice"><meta property="twitter:description"><meta property="twitter:image" content="https://geyuxu.com/blog-placeholder-1.jpg"><script src="/js/jquery-3.7.1.min.js"></script><script src="/js/copy-code-button.js" defer></script><!-- Dify Chatbot Configuration --><script>
 window.difyChatbotConfig = {
  token: 'VgrjCAto93bjE0MW',
  systemVariables: {
    // user_id: 'YOU CAN DEFINE USER ID HERE',
    // conversation_id: 'YOU CAN DEFINE CONVERSATION ID HERE, IT MUST BE A VALID UUID',
  },
  userVariables: {
    // avatar_url: 'YOU CAN DEFINE USER AVATAR URL HERE',
    // name: 'YOU CAN DEFINE USER NAME HERE',
  },
 }
</script><!-- Dify Chatbot Script --><script src="https://udify.app/embed.min.js" id="VgrjCAto93bjE0MW" defer>
</script><!-- Dify Chatbot Custom Styles --><script is:global>
	window.addEventListener('DOMContentLoaded', () => {
		$('.toc ol').css({
		'list-style': 'none',   // 隐藏 1. 2. 3.
		'margin': 0,
		'padding-left': 0,       // 可按需调整
		});
		$('.toc ol > li').css({
		'list-style': 'none',   // 隐藏 1. 2. 3.
		'padding-left': 10
		});
        $('.sidebar').append($('.toc'));
      });
	</script><link rel="stylesheet" href="/_astro/assets-slug_.BFPovWfH.css">
<style>a[data-astro-cid-eimmu3lg]{display:inline-block;text-decoration:none}a[data-astro-cid-eimmu3lg].active{font-weight:bolder;text-decoration:underline}header[data-astro-cid-3ef6ksr2]{margin:0;padding:0 1em;background:#fff;box-shadow:0 2px 8px rgba(var(--black),5%)}h2[data-astro-cid-3ef6ksr2]{margin:0;font-size:1em}h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2],h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none}nav[data-astro-cid-3ef6ksr2]{display:flex;align-items:center;justify-content:space-between}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{padding:1em .5em;color:var(--black);border-bottom:4px solid transparent;text-decoration:none}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none;border-bottom-color:var(--accent)}.social-links[data-astro-cid-3ef6ksr2],.social-links[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{display:flex}@media (max-width: 720px){.social-links[data-astro-cid-3ef6ksr2]{display:none}}footer[data-astro-cid-sz7xmlte]{padding:2em 1em 6em;background:linear-gradient(var(--gray-gradient)) no-repeat;color:rgb(var(--gray));text-align:center}.social-links[data-astro-cid-sz7xmlte]{display:flex;justify-content:center;gap:1em;margin-top:1em}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]{text-decoration:none;color:rgb(var(--gray))}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]:hover{color:rgb(var(--gray-dark))}
.statement[data-astro-cid-uffxixac]{font-size:10px;color:gray}@media (max-width: 640px){.statement[data-astro-cid-uffxixac]{font-size:6px;color:gray}}
</style></head> <body> <header data-astro-cid-3ef6ksr2> <nav data-astro-cid-3ef6ksr2> <!--<h2><a href="/">{SITE_TITLE}</a></h2>--> <h2 data-astro-cid-3ef6ksr2><a style="padding-left:0;color:blue;" href="/" data-astro-cid-3ef6ksr2>Ge Yuxu<br data-astro-cid-3ef6ksr2>AI & Engineering</a></h2> <div class="internal-links" data-astro-cid-3ef6ksr2> <a href="/" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Home </a>  <a href="/blog/1" class="active" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Blog </a>  <a href="/series" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Series </a>  <a href="/projects" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Projects </a>  </div> <div class="social-links" data-astro-cid-3ef6ksr2> <a href="https://github.com/geyuxu" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Go to Ge Yuxu's GitHub repo</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" data-astro-cid-3ef6ksr2><path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-astro-cid-3ef6ksr2></path></svg> </a> <a href="https://www.linkedin.com/in/geyuxu/" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Go to Ge Yuxu's LinkedIn profile</span> <svg viewBox="0 0 24 24" width="32" height="32" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" data-astro-cid-3ef6ksr2> <path fill="currentColor" d="M20.447 20.452H17.2v-5.569c0-1.328-.025-3.039-1.852-3.039-1.853 0-2.136 1.447-2.136 2.942v5.666h-3.248V9h3.122v1.561h.045c.435-.823 1.498-1.688 3.083-1.688 3.295 0 3.903 2.17 3.903 4.989v6.59zM5.337 7.433a1.882 1.882 0 110-3.764 1.882 1.882 0 010 3.764zm1.626 13.019H3.708V9h3.255v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.226.792 24 1.771 24h20.451C23.2 24 24 23.226 24 22.271V1.729C24 .774 23.2 0 22.222 0z" data-astro-cid-3ef6ksr2></path> </svg> </a> <a href="mailto:ngzerone@hotmail.com" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Send email to Ge Yuxu</span> <svg viewBox="0 0 24 24" width="32" height="32" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" data-astro-cid-3ef6ksr2> <path fill="currentColor" d="M20 4H4c-1.1 0-1.99.9-1.99 2L2 18c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z" data-astro-cid-3ef6ksr2></path> </svg> </a> </div> </nav> <!-- Microsoft Clarity --> <script type="text/javascript">
		(function(c,l,a,r,i,t,y){
			c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
			t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
			y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
		})(window, document, "clarity", "script", "rc2w96osp6");
	</script> </header>  <main class="page"> <!-- 左侧栏 --> <aside class="sidebar"> <div class="meta"> <b>Supervised vs. Unsupervised Learning: Concepts, Algorithms, and Practice</b> <p><time datetime="2025-07-25T10:52:14.000Z"> 2025/07/25 18:52:14 </time></p>   </div> <hr> <br> </aside> <!-- 右侧正文区域（flex 居中） --> <div class="content-wrapper"> <article class="prose">  <nav class="toc"><ol class="toc-level toc-level-1"><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#1-introduction">1. Introduction</a></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#2-core-concepts-and-differences">2. Core Concepts and Differences</a><ol class="toc-level toc-level-2"><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#21-supervised-learning">2.1 Supervised Learning</a></li><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#22-unsupervised-learning">2.2 Unsupervised Learning</a></li><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#23-at-a-glance-comparison">2.3 At-a-Glance Comparison</a></li></ol></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#3-workflow-comparison">3. Workflow Comparison</a><ol class="toc-level toc-level-2"><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#31-supervised-learning-pipeline">3.1 Supervised Learning Pipeline</a></li><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#32-unsupervised-learning-pipeline">3.2 Unsupervised Learning Pipeline</a></li></ol></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#4-a-quick-look-at-typical-algorithms">4. A Quick Look at Typical Algorithms</a><ol class="toc-level toc-level-2"><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#41-supervised-learning-algorithms">4.1 Supervised Learning Algorithms</a></li><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#42-unsupervised-learning-algorithms">4.2 Unsupervised Learning Algorithms</a></li></ol></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#5-scenarios-and-case-studies">5. Scenarios and Case Studies</a></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#6-extended-paradigms">6. Extended Paradigms</a></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#7-selection-guide--practical-tips">7. Selection Guide &#x26; Practical Tips</a></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#8-conclusion">8. Conclusion</a></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#9-code-examples">9. Code Examples</a><ol class="toc-level toc-level-2"><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#91-environment-setup">9.1 Environment Setup</a></li><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#92-supervised-learning-examples">9.2 Supervised Learning Examples</a></li><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#93-unsupervised-learning-examples">9.3 Unsupervised Learning Examples</a></li><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#94-simple-gan-skeleton-pytorch">9.4 Simple GAN Skeleton (PyTorch)</a></li></ol></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#10-references">10. References</a></li></ol></nav><h2 id="1-introduction"><a aria-hidden="true" tabindex="-1" href="#1-introduction"><span class="icon icon-link"></span></a>1. Introduction</h2>
<p>In the vast landscape of machine learning, Supervised and Unsupervised Learning are two fundamental paradigms that form the bedrock of modern artificial intelligence systems. Their influence is ubiquitous, powering everything from precise medical diagnostics and personalized e-commerce recommendations to financial risk control and autonomous driving.</p>
<p>However, when faced with a real-world problem, we often encounter a core dilemma: <strong>When should we choose supervised learning? And when should we turn to unsupervised learning?</strong></p>
<p>This blog post will guide you through these two concepts in a clear and accessible way. By comparing their core ideas, workflows, and typical algorithms, we will provide you with a clear selection guide and practical tips. After reading this article, you will gain:</p>
<ul>
<li><strong>A clear conceptual understanding</strong>: No more confusion about the true meaning of “labeled” vs. “unlabeled” data.</li>
<li><strong>Mastery of core algorithms</strong>: Quickly grasp the use cases for classic algorithms like Linear Regression, K-means, SVM, and PCA.</li>
<li><strong>Practical selection skills</strong>: Be able to make reasonable model choices based on your data and goals in real-world projects.</li>
</ul>
<hr>
<h2 id="2-core-concepts-and-differences"><a aria-hidden="true" tabindex="-1" href="#2-core-concepts-and-differences"><span class="icon icon-link"></span></a>2. Core Concepts and Differences</h2>
<h3 id="21-supervised-learning"><a aria-hidden="true" tabindex="-1" href="#21-supervised-learning"><span class="icon icon-link"></span></a>2.1 Supervised Learning</h3>
<p>The core idea of supervised learning is to <strong>learn from “labeled” data</strong>. The “label” here is the correct answer we want the model to predict.</p>
<ul>
<li><strong>Definition</strong>: Given a set of input data <code>X</code> and its corresponding output labels <code>y</code>, the goal of the algorithm is to learn a mapping function <code>f</code> such that <code>f(x) ≈ y</code>.</li>
<li><strong>Objective</strong>: To minimize a <strong>measurable error</strong> between the model’s predictions and the true labels, such as Mean Squared Error (MSE) for regression or Cross-Entropy for classification.</li>
<li><strong>Typical Tasks</strong>:
<ul>
<li><strong>Classification</strong>: Predicting a discrete class label. For example, determining if an email is spam or not (binary classification), or identifying an animal in a picture as a cat, dog, or bird (multi-class classification).</li>
<li><strong>Regression</strong>: Predicting a continuous numerical value. For example, predicting the price of a house based on its features, or forecasting the temperature for the next day based on historical data.</li>
</ul>
</li>
</ul>
<h3 id="22-unsupervised-learning"><a aria-hidden="true" tabindex="-1" href="#22-unsupervised-learning"><span class="icon icon-link"></span></a>2.2 Unsupervised Learning</h3>
<p>In contrast to supervised learning, unsupervised learning deals with <strong>“unlabeled” data</strong>. It does not require manually annotated answers; instead, it strives to discover the inherent structure and patterns within the data itself.</p>
<ul>
<li><strong>Definition</strong>: Given only input data <code>X</code>, the algorithm’s goal is to uncover hidden structures in the data.</li>
<li><strong>Objective</strong>: To explore the intrinsic patterns of the data, such as <strong>similarity, density, or latent factors</strong>.</li>
<li><strong>Typical Tasks</strong>:
<ul>
<li><strong>Clustering</strong>: Grouping similar data points into the same cluster. For example, segmenting customers into different groups (high-value, potential, etc.) based on their purchasing behavior.</li>
<li><strong>Dimensionality Reduction/Visualization</strong>: Reducing the number of features in the data while preserving its core information. For example, compressing high-dimensional user profiles into a 2D plane for visualization.</li>
<li><strong>Density Estimation/Generative Modeling</strong>: Learning the distribution of the data to generate new, similar samples. For example, creating realistic human face images.</li>
</ul>
</li>
</ul>
<h3 id="23-at-a-glance-comparison"><a aria-hidden="true" tabindex="-1" href="#23-at-a-glance-comparison"><span class="icon icon-link"></span></a>2.3 At-a-Glance Comparison</h3>






























<table><thead><tr><th align="left">Dimension</th><th align="left">Supervised Learning</th><th align="left">Unsupervised Learning</th></tr></thead><tbody><tr><td align="left"><strong>Training Data</strong></td><td align="left">Labeled (X, y)</td><td align="left">Unlabeled (X)</td></tr><tr><td align="left"><strong>Primary Goal</strong></td><td align="left">Predict a clear output</td><td align="left">Discover hidden structures</td></tr><tr><td align="left"><strong>Evaluation</strong></td><td align="left">Compare against true labels (Accuracy, RMSE, F1-Score…)</td><td align="left">Indirect metrics (Silhouette Score, Reconstruction Error…)</td></tr><tr><td align="left"><strong>Common Risks</strong></td><td align="left">Overfitting, high annotation costs</td><td align="left">Difficult to interpret results, ambiguous evaluation</td></tr></tbody></table>
<hr>
<h2 id="3-workflow-comparison"><a aria-hidden="true" tabindex="-1" href="#3-workflow-comparison"><span class="icon icon-link"></span></a>3. Workflow Comparison</h2>
<h3 id="31-supervised-learning-pipeline"><a aria-hidden="true" tabindex="-1" href="#31-supervised-learning-pipeline"><span class="icon icon-link"></span></a>3.1 Supervised Learning Pipeline</h3>
<p>A typical supervised learning project follows a relatively standardized process:</p>
<ol>
<li><strong>Data Annotation and Splitting</strong>: Obtain or annotate high-quality labeled data and split it into training, validation, and test sets.</li>
<li><strong>Feature Engineering and Model Selection</strong>: Extract effective features based on business understanding and choose a suitable model (e.g., linear model, tree-based model, or neural network).</li>
<li><strong>Training and Tuning</strong>: Train the model on the training set and tune hyperparameters (e.g., learning rate, tree depth) on the validation set.</li>
<li><strong>Evaluation and Deployment</strong>: Evaluate the final model’s performance on the test set and deploy it to production if it meets the criteria.</li>
<li><strong>Monitoring and Iteration</strong>: Continuously monitor the model’s online performance, watch for “concept drift” (changes in data distribution), and periodically retrain it with new data.</li>
</ol>
<h3 id="32-unsupervised-learning-pipeline"><a aria-hidden="true" tabindex="-1" href="#32-unsupervised-learning-pipeline"><span class="icon icon-link"></span></a>3.2 Unsupervised Learning Pipeline</h3>
<p>The unsupervised learning process is more exploratory:</p>
<ol>
<li><strong>Data Preprocessing</strong>: Data standardization or normalization is crucial, as many algorithms (like K-means, PCA) are sensitive to scale. An appropriate distance metric (e.g., Euclidean distance, cosine similarity) must also be chosen.</li>
<li><strong>Algorithm and Hyperparameter Exploration</strong>: Select a suitable algorithm (e.g., K-means, DBSCAN) and explore its key hyperparameters (e.g., number of clusters <code>k</code>, neighborhood radius <code>ε</code>).</li>
<li><strong>Result Visualization and Business Validation</strong>: Since there is no “correct answer,” results (like clusters or dimensionality reduction plots) often need to be visualized and validated with business knowledge for effectiveness and interpretability.</li>
<li><strong>Downstream Applications</strong>: The results of unsupervised learning often serve as input for downstream tasks. For example, using cluster assignments as user tags or using reduced-dimension features for a subsequent supervised learning model.</li>
</ol>
<hr>
<h2 id="4-a-quick-look-at-typical-algorithms"><a aria-hidden="true" tabindex="-1" href="#4-a-quick-look-at-typical-algorithms"><span class="icon icon-link"></span></a>4. A Quick Look at Typical Algorithms</h2>
<h3 id="41-supervised-learning-algorithms"><a aria-hidden="true" tabindex="-1" href="#41-supervised-learning-algorithms"><span class="icon icon-link"></span></a>4.1 Supervised Learning Algorithms</h3>













































<table><thead><tr><th align="left">Algorithm</th><th align="left">One-Sentence Summary</th><th align="left">Key Features / Use Cases</th></tr></thead><tbody><tr><td align="left"><strong>Linear Regression</strong></td><td align="left">Fits a straight line by minimizing the squared error between predicted and actual values.</td><td align="left">Highly interpretable, serves as a baseline for complex models; used for house price/sales prediction.</td></tr><tr><td align="left"><strong>Logistic Regression</strong></td><td align="left">Maps a linear output to the (0,1) interval using the Sigmoid function for binary classification.</td><td align="left">Outputs probabilities, easy to understand and implement; widely used for CTR prediction, credit scoring.</td></tr><tr><td align="left"><strong>Decision Tree (CART)</strong></td><td align="left">Builds a tree by recursively partitioning data into nodes to increase “purity.”</td><td align="left">Intuitive rules, handles non-linearity and missing values, but prone to overfitting.</td></tr><tr><td align="left"><strong>Random Forest</strong></td><td align="left">Improves performance by building and combining the votes of multiple decision trees.</td><td align="left">Effectively combats overfitting, can assess feature importance, a strong baseline model.</td></tr><tr><td align="left"><strong>Support Vector Machine (SVM)</strong></td><td align="left">Finds a hyperplane that separates classes with the maximum margin, using the kernel trick for non-linear problems.</td><td align="left">Performs well on small, high-dimensional datasets; used for text classification, image recognition.</td></tr><tr><td align="left"><strong>Boosting (XGBoost/LightGBM)</strong></td><td align="left">Iteratively fits the residuals of the previous round, combining weak learners into a strong model.</td><td align="left">State-of-the-art on tabular data, feature engineering-friendly.</td></tr><tr><td align="left"><strong>Deep Networks (CNN/Transformer)</strong></td><td align="left">Automatically learns hierarchical features from data through multiple non-linear transformations.</td><td align="left"><strong>CNNs</strong> excel at capturing local spatial features (images), while <strong>Transformers</strong> handle global dependencies in sequential data (text, speech).</td></tr></tbody></table>
<h3 id="42-unsupervised-learning-algorithms"><a aria-hidden="true" tabindex="-1" href="#42-unsupervised-learning-algorithms"><span class="icon icon-link"></span></a>4.2 Unsupervised Learning Algorithms</h3>























































<table><thead><tr><th align="left">Algorithm</th><th align="left">One-Sentence Summary</th><th align="left">Key Features / Typical Scenarios</th></tr></thead><tbody><tr><td align="left"><strong>K-means</strong></td><td align="left">Iteratively updates cluster centroids to minimize the sum of squared distances from each point to its assigned centroid.</td><td align="left">Simple and efficient, but requires pre-specifying <code>k</code> and is sensitive to initialization; used for user segmentation.</td></tr><tr><td align="left"><strong>DBSCAN</strong></td><td align="left">Defines clusters based on density, automatically identifying noise and discovering arbitrarily shaped clusters.</td><td align="left">Does not require pre-setting <code>k</code>, robust to noise; suitable for geospatial data analysis.</td></tr><tr><td align="left"><strong>Hierarchical Clustering</strong></td><td align="left">Forms a tree-like hierarchy of clusters by successively merging (bottom-up) or splitting (top-down) data points.</td><td align="left">No need to pre-set <code>k</code>, provides a dendrogram for understanding data hierarchy; used in phylogenetic analysis.</td></tr><tr><td align="left"><strong>PCA</strong></td><td align="left">Linearly transforms data onto a few orthogonal directions that capture the maximum variance.</td><td align="left">The most classic dimensionality reduction method, used for data compression, denoising, and visualization.</td></tr><tr><td align="left"><strong>t-SNE / UMAP</strong></td><td align="left">Preserves the local neighborhood structure of high-dimensional data in a low-dimensional space via non-linear embedding.</td><td align="left">Excellent for visualizing high-dimensional data (like text, genes), often superior to PCA for this purpose.</td></tr><tr><td align="left"><strong>Gaussian Mixture Model (GMM)</strong></td><td align="left">Assumes data is a mixture of several Gaussian distributions and uses the EM algorithm for soft clustering.</td><td align="left">Can handle more complex (elliptical) cluster shapes and outputs probabilities of cluster membership.</td></tr><tr><td align="left"><strong>Kernel Density Estimation (KDE)</strong></td><td align="left">Smoothly estimates the probability density function of data by placing a kernel (e.g., Gaussian) on each data point.</td><td align="left">Used for data distribution visualization and anomaly detection.</td></tr><tr><td align="left"><strong>Generative Adversarial Network (GAN)</strong></td><td align="left">A generator and a discriminator compete, with the generator trying to create realistic data and the discriminator trying to tell it apart from real data.</td><td align="left">Achieves stunning results in image synthesis and data augmentation.</td></tr><tr><td align="left"><strong>Variational Autoencoder (VAE)</strong></td><td align="left">Encodes input into a latent distribution, then samples from it to reconstruct the input, serving as a generative model.</td><td align="left">Can generate controllable new samples, and its latent variables have some semantic meaning.</td></tr></tbody></table>
<hr>
<h2 id="5-scenarios-and-case-studies"><a aria-hidden="true" tabindex="-1" href="#5-scenarios-and-case-studies"><span class="icon icon-link"></span></a>5. Scenarios and Case Studies</h2>






























<table><thead><tr><th align="left">Task</th><th align="left">Method Paradigm</th><th align="left">Example</th></tr></thead><tbody><tr><td align="left"><strong>Medical Image Diagnosis</strong></td><td align="left"><strong>Supervised Learning</strong> → CNN/Transformer</td><td align="left">Input a CT scan, and the model automatically classifies lesion areas (e.g., tumors, nodules).</td></tr><tr><td align="left"><strong>E-commerce User Segmentation</strong></td><td align="left"><strong>Unsupervised Learning</strong> → K-means/DBSCAN</td><td align="left">Segment users into different value groups based on their browsing, carting, and purchasing behavior logs.</td></tr><tr><td align="left"><strong>Stylized Image Generation</strong></td><td align="left"><strong>Unsupervised Learning</strong> → GAN/VAE</td><td align="left">Input a regular photo and generate an artistic image in the style of Van Gogh or ink wash painting.</td></tr><tr><td align="left"><strong>Semi-Supervised Text Classification</strong></td><td align="left"><strong>Self-Supervised Pre-training + Supervised Fine-tuning</strong></td><td align="left">Use massive unlabeled text for self-supervised learning (like BERT), then fine-tune with a small amount of labeled data to achieve high classification accuracy. This is the dominant paradigm in modern NLP.</td></tr></tbody></table>
<hr>
<h2 id="6-extended-paradigms"><a aria-hidden="true" tabindex="-1" href="#6-extended-paradigms"><span class="icon icon-link"></span></a>6. Extended Paradigms</h2>
<p>The line between supervised and unsupervised learning is not always sharp. In practice, many powerful hybrid paradigms have emerged:</p>
<ul>
<li><strong>Semi-supervised Learning</strong>: When you have a small amount of labeled data and a large amount of unlabeled data, techniques like Pseudo-Labeling and Consistency Regularization can leverage the unlabeled data to improve model performance.</li>
<li><strong>Weakly Supervised Learning</strong>: The labels are not entirely accurate or complete (e.g., you know an image contains a cat, but not its exact location).</li>
<li><strong>Self-supervised Learning</strong>: Creates “pseudo-tasks” from the data itself to generate labels. For example, randomly masking a word in a text (Masked Language Model) and training the model to predict it, which is the core idea behind pre-trained language models like BERT.</li>
<li><strong>Reinforcement Learning (RL)</strong>: An agent learns the optimal policy by interacting with an environment and receiving rewards or penalties. It is often combined with supervised learning, as in AlphaGo.</li>
</ul>
<hr>
<h2 id="7-selection-guide--practical-tips"><a aria-hidden="true" tabindex="-1" href="#7-selection-guide--practical-tips"><span class="icon icon-link"></span></a>7. Selection Guide &#x26; Practical Tips</h2>
<ol>
<li>
<p><strong>Start with Your Data and Labels</strong>:</p>
<ul>
<li><strong>Have high-quality labels?</strong> Go with supervised learning.</li>
<li><strong>Labeling is expensive?</strong> Prioritize unsupervised learning for data exploration (clustering, visualization) or use self-supervised/semi-supervised methods to reduce label dependency.</li>
</ul>
</li>
<li>
<p><strong>Consider Model Scale and Data Complexity</strong>:</p>
<ul>
<li><strong>Large-scale perceptual tasks (images, speech, text)?</strong> Deep learning networks are the best choice.</li>
<li><strong>Small, high-dimensional datasets?</strong> SVMs or tree-based models (like Random Forest) might perform better.</li>
<li><strong>Structured/tabular data?</strong> XGBoost/LightGBM are often the performance kings.</li>
</ul>
</li>
<li>
<p><strong>Balance Interpretability and Accuracy</strong>:</p>
<ul>
<li><strong>High-stakes or regulated scenarios (finance, healthcare)?</strong> Linear models, logistic regression, or decision trees are favored for their interpretability.</li>
<li><strong>Scenarios where performance is paramount (online ads, recommendations)?</strong> More accurate, complex models (like deep networks) are preferred.</li>
</ul>
</li>
<li>
<p><strong>Combine Offline Exploration with Online Application</strong>:</p>
<ul>
<li>A common pattern is to first use <strong>unsupervised learning</strong> for exploratory analysis on offline data to discover potential user segments or data patterns. Then, use these findings as features or targets to build a <strong>supervised learning</strong> model for real-time prediction online.</li>
</ul>
</li>
</ol>
<hr>
<h2 id="8-conclusion"><a aria-hidden="true" tabindex="-1" href="#8-conclusion"><span class="icon icon-link"></span></a>8. Conclusion</h2>
<p>Supervised and unsupervised learning are two powerful tools for solving different kinds of problems, with the core distinction being their reliance on “ground truth” answers.</p>
<ul>
<li><strong>Supervised learning excels at “answering known questions.”</strong> Driven by clear goals and high-quality labels, it can make accurate predictions.</li>
<li><strong>Unsupervised learning excels at “discovering unknown questions.”</strong> Without prior knowledge, it can reveal hidden structures, patterns, and insights in the data.</li>
</ul>
<p>In real-world machine learning projects, the two are often not isolated. The most powerful solutions frequently combine them: <strong>first exploring the possibilities in the data with unsupervised learning, then building a precise model for a specific goal with supervised learning, creating a complete cycle from data insight to value creation.</strong></p>
<hr>
<h2 id="9-code-examples"><a aria-hidden="true" tabindex="-1" href="#9-code-examples"><span class="icon icon-link"></span></a>9. Code Examples</h2>
<h3 id="91-environment-setup"><a aria-hidden="true" tabindex="-1" href="#91-environment-setup"><span class="icon icon-link"></span></a>9.1 Environment Setup</h3>
<pre class="astro-code github-light" style="background-color:#fff;color:#24292e; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6F42C1">pip</span><span style="color:#032F62"> install</span><span style="color:#032F62"> scikit-learn</span><span style="color:#032F62"> matplotlib</span><span style="color:#032F62"> torch</span><span style="color:#032F62"> torchvision</span></span></code></pre>
<h3 id="92-supervised-learning-examples"><a aria-hidden="true" tabindex="-1" href="#92-supervised-learning-examples"><span class="icon icon-link"></span></a>9.2 Supervised Learning Examples</h3>
<h4 id="linear-regression-california-housing"><a aria-hidden="true" tabindex="-1" href="#linear-regression-california-housing"><span class="icon icon-link"></span></a>Linear Regression (California Housing)</h4>
<pre class="astro-code github-light" style="background-color:#fff;color:#24292e; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#D73A49">from</span><span style="color:#24292E"> sklearn.datasets </span><span style="color:#D73A49">import</span><span style="color:#24292E"> fetch_california_housing</span></span>
<span class="line"><span style="color:#D73A49">from</span><span style="color:#24292E"> sklearn.model_selection </span><span style="color:#D73A49">import</span><span style="color:#24292E"> train_test_split</span></span>
<span class="line"><span style="color:#D73A49">from</span><span style="color:#24292E"> sklearn.linear_model </span><span style="color:#D73A49">import</span><span style="color:#24292E"> LinearRegression</span></span>
<span class="line"><span style="color:#D73A49">from</span><span style="color:#24292E"> sklearn.metrics </span><span style="color:#D73A49">import</span><span style="color:#24292E"> mean_squared_error</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># California Housing dataset</span></span>
<span class="line"><span style="color:#24292E">X, y </span><span style="color:#D73A49">=</span><span style="color:#24292E"> fetch_california_housing(</span><span style="color:#E36209">return_X_y</span><span style="color:#D73A49">=</span><span style="color:#005CC5">True</span><span style="color:#24292E">)</span></span>
<span class="line"><span style="color:#24292E">X_train, X_test, y_train, y_test </span><span style="color:#D73A49">=</span><span style="color:#24292E"> train_test_split(X, y, </span><span style="color:#E36209">test_size</span><span style="color:#D73A49">=</span><span style="color:#005CC5">0.2</span><span style="color:#24292E">, </span><span style="color:#E36209">random_state</span><span style="color:#D73A49">=</span><span style="color:#005CC5">42</span><span style="color:#24292E">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E">model </span><span style="color:#D73A49">=</span><span style="color:#24292E"> LinearRegression().fit(X_train, y_train)</span></span>
<span class="line"><span style="color:#24292E">pred </span><span style="color:#D73A49">=</span><span style="color:#24292E"> model.predict(X_test)</span></span>
<span class="line"><span style="color:#005CC5">print</span><span style="color:#24292E">(</span><span style="color:#D73A49">f</span><span style="color:#032F62">"RMSE on California Housing: </span><span style="color:#005CC5">{</span><span style="color:#24292E">mean_squared_error(y_test, pred, </span><span style="color:#E36209">squared</span><span style="color:#D73A49">=</span><span style="color:#005CC5">False</span><span style="color:#24292E">)</span><span style="color:#D73A49">:.2f</span><span style="color:#005CC5">}</span><span style="color:#032F62">"</span><span style="color:#24292E">)</span></span></code></pre>
<h4 id="logistic-regression-breast-cancer-binary-classification"><a aria-hidden="true" tabindex="-1" href="#logistic-regression-breast-cancer-binary-classification"><span class="icon icon-link"></span></a>Logistic Regression (Breast Cancer Binary Classification)</h4>
<pre class="astro-code github-light" style="background-color:#fff;color:#24292e; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#D73A49">from</span><span style="color:#24292E"> sklearn.datasets </span><span style="color:#D73A49">import</span><span style="color:#24292E"> load_breast_cancer</span></span>
<span class="line"><span style="color:#D73A49">from</span><span style="color:#24292E"> sklearn.linear_model </span><span style="color:#D73A49">import</span><span style="color:#24292E"> LogisticRegression</span></span>
<span class="line"><span style="color:#D73A49">from</span><span style="color:#24292E"> sklearn.preprocessing </span><span style="color:#D73A49">import</span><span style="color:#24292E"> StandardScaler</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E">X, y </span><span style="color:#D73A49">=</span><span style="color:#24292E"> load_breast_cancer(</span><span style="color:#E36209">return_X_y</span><span style="color:#D73A49">=</span><span style="color:#005CC5">True</span><span style="color:#24292E">)</span></span>
<span class="line"><span style="color:#6A737D"># Scaling improves performance</span></span>
<span class="line"><span style="color:#24292E">scaler </span><span style="color:#D73A49">=</span><span style="color:#24292E"> StandardScaler()</span></span>
<span class="line"><span style="color:#24292E">X_scaled </span><span style="color:#D73A49">=</span><span style="color:#24292E"> scaler.fit_transform(X)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E">clf </span><span style="color:#D73A49">=</span><span style="color:#24292E"> LogisticRegression(</span><span style="color:#E36209">max_iter</span><span style="color:#D73A49">=</span><span style="color:#005CC5">1000</span><span style="color:#24292E">).fit(X_scaled, y)</span></span>
<span class="line"><span style="color:#005CC5">print</span><span style="color:#24292E">(</span><span style="color:#D73A49">f</span><span style="color:#032F62">"Accuracy on Breast Cancer: </span><span style="color:#005CC5">{</span><span style="color:#24292E">clf.score(X_scaled, y)</span><span style="color:#D73A49">:.3f</span><span style="color:#005CC5">}</span><span style="color:#032F62">"</span><span style="color:#24292E">)</span></span></code></pre>
<h3 id="93-unsupervised-learning-examples"><a aria-hidden="true" tabindex="-1" href="#93-unsupervised-learning-examples"><span class="icon icon-link"></span></a>9.3 Unsupervised Learning Examples</h3>
<h4 id="k-means-clustering--visualization"><a aria-hidden="true" tabindex="-1" href="#k-means-clustering--visualization"><span class="icon icon-link"></span></a>K-means Clustering + Visualization</h4>
<pre class="astro-code github-light" style="background-color:#fff;color:#24292e; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#D73A49">from</span><span style="color:#24292E"> sklearn.datasets </span><span style="color:#D73A49">import</span><span style="color:#24292E"> load_iris</span></span>
<span class="line"><span style="color:#D73A49">from</span><span style="color:#24292E"> sklearn.cluster </span><span style="color:#D73A49">import</span><span style="color:#24292E"> KMeans</span></span>
<span class="line"><span style="color:#D73A49">import</span><span style="color:#24292E"> matplotlib.pyplot </span><span style="color:#D73A49">as</span><span style="color:#24292E"> plt</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E">X, y </span><span style="color:#D73A49">=</span><span style="color:#24292E"> load_iris(</span><span style="color:#E36209">return_X_y</span><span style="color:#D73A49">=</span><span style="color:#005CC5">True</span><span style="color:#24292E">) </span><span style="color:#6A737D"># y is used here only for comparison; K-means itself doesn't use it</span></span>
<span class="line"><span style="color:#24292E">kmeans </span><span style="color:#D73A49">=</span><span style="color:#24292E"> KMeans(</span><span style="color:#E36209">n_clusters</span><span style="color:#D73A49">=</span><span style="color:#005CC5">3</span><span style="color:#24292E">, </span><span style="color:#E36209">random_state</span><span style="color:#D73A49">=</span><span style="color:#005CC5">42</span><span style="color:#24292E">, </span><span style="color:#E36209">n_init</span><span style="color:#D73A49">=</span><span style="color:#005CC5">10</span><span style="color:#24292E">).fit(X) </span><span style="color:#6A737D"># n_init='auto' in future</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Visualize the first two features</span></span>
<span class="line"><span style="color:#24292E">plt.scatter(X[:, </span><span style="color:#005CC5">0</span><span style="color:#24292E">], X[:, </span><span style="color:#005CC5">1</span><span style="color:#24292E">], </span><span style="color:#E36209">c</span><span style="color:#D73A49">=</span><span style="color:#24292E">kmeans.labels_, </span><span style="color:#E36209">cmap</span><span style="color:#D73A49">=</span><span style="color:#032F62">'viridis'</span><span style="color:#24292E">)</span></span>
<span class="line"><span style="color:#24292E">plt.title(</span><span style="color:#032F62">'K-means Clustering on Iris Dataset'</span><span style="color:#24292E">)</span></span>
<span class="line"><span style="color:#24292E">plt.xlabel(</span><span style="color:#032F62">'Sepal Length'</span><span style="color:#24292E">)</span></span>
<span class="line"><span style="color:#24292E">plt.ylabel(</span><span style="color:#032F62">'Sepal Width'</span><span style="color:#24292E">)</span></span>
<span class="line"><span style="color:#24292E">plt.show()</span></span></code></pre>
<h4 id="pca--t-sne-visualization"><a aria-hidden="true" tabindex="-1" href="#pca--t-sne-visualization"><span class="icon icon-link"></span></a>PCA + t-SNE Visualization</h4>
<pre class="astro-code github-light" style="background-color:#fff;color:#24292e; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#D73A49">from</span><span style="color:#24292E"> sklearn.decomposition </span><span style="color:#D73A49">import</span><span style="color:#005CC5"> PCA</span></span>
<span class="line"><span style="color:#D73A49">from</span><span style="color:#24292E"> sklearn.manifold </span><span style="color:#D73A49">import</span><span style="color:#005CC5"> TSNE</span></span>
<span class="line"><span style="color:#D73A49">import</span><span style="color:#24292E"> matplotlib.pyplot </span><span style="color:#D73A49">as</span><span style="color:#24292E"> plt</span></span>
<span class="line"><span style="color:#D73A49">from</span><span style="color:#24292E"> sklearn.datasets </span><span style="color:#D73A49">import</span><span style="color:#24292E"> load_iris</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E">X, y </span><span style="color:#D73A49">=</span><span style="color:#24292E"> load_iris(</span><span style="color:#E36209">return_X_y</span><span style="color:#D73A49">=</span><span style="color:#005CC5">True</span><span style="color:#24292E">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># First, reduce dimensions with PCA to a reasonable intermediate number</span></span>
<span class="line"><span style="color:#24292E">X_reduced </span><span style="color:#D73A49">=</span><span style="color:#24292E"> PCA(</span><span style="color:#E36209">n_components</span><span style="color:#D73A49">=</span><span style="color:#005CC5">50</span><span style="color:#24292E">, </span><span style="color:#E36209">random_state</span><span style="color:#D73A49">=</span><span style="color:#005CC5">42</span><span style="color:#24292E">).fit_transform(X) </span><span style="color:#D73A49">if</span><span style="color:#24292E"> X.shape[</span><span style="color:#005CC5">1</span><span style="color:#24292E">] </span><span style="color:#D73A49">></span><span style="color:#005CC5"> 50</span><span style="color:#D73A49"> else</span><span style="color:#24292E"> X</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Then, use t-SNE for non-linear dimensionality reduction for visualization</span></span>
<span class="line"><span style="color:#24292E">X_embedded </span><span style="color:#D73A49">=</span><span style="color:#24292E"> TSNE(</span><span style="color:#E36209">n_components</span><span style="color:#D73A49">=</span><span style="color:#005CC5">2</span><span style="color:#24292E">, </span><span style="color:#E36209">learning_rate</span><span style="color:#D73A49">=</span><span style="color:#032F62">'auto'</span><span style="color:#24292E">, </span><span style="color:#E36209">init</span><span style="color:#D73A49">=</span><span style="color:#032F62">'pca'</span><span style="color:#24292E">, </span><span style="color:#E36209">random_state</span><span style="color:#D73A49">=</span><span style="color:#005CC5">42</span><span style="color:#24292E">).fit_transform(X_reduced)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E">plt.scatter(X_embedded[:, </span><span style="color:#005CC5">0</span><span style="color:#24292E">], X_embedded[:, </span><span style="color:#005CC5">1</span><span style="color:#24292E">], </span><span style="color:#E36209">c</span><span style="color:#D73A49">=</span><span style="color:#24292E">y, </span><span style="color:#E36209">cmap</span><span style="color:#D73A49">=</span><span style="color:#032F62">'viridis'</span><span style="color:#24292E">) </span><span style="color:#6A737D"># Color by true labels to verify</span></span>
<span class="line"><span style="color:#24292E">plt.title(</span><span style="color:#032F62">'t-SNE Visualization of Iris Dataset'</span><span style="color:#24292E">)</span></span>
<span class="line"><span style="color:#24292E">plt.xlabel(</span><span style="color:#032F62">'t-SNE feature 1'</span><span style="color:#24292E">)</span></span>
<span class="line"><span style="color:#24292E">plt.ylabel(</span><span style="color:#032F62">'t-SNE feature 2'</span><span style="color:#24292E">)</span></span>
<span class="line"><span style="color:#24292E">plt.show()</span></span></code></pre>
<h3 id="94-simple-gan-skeleton-pytorch"><a aria-hidden="true" tabindex="-1" href="#94-simple-gan-skeleton-pytorch"><span class="icon icon-link"></span></a>9.4 Simple GAN Skeleton (PyTorch)</h3>
<p>This is a minimal GAN structure to demonstrate its core components, not a complete training script.</p>
<pre class="astro-code github-light" style="background-color:#fff;color:#24292e; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#D73A49">import</span><span style="color:#24292E"> torch</span></span>
<span class="line"><span style="color:#D73A49">from</span><span style="color:#24292E"> torch </span><span style="color:#D73A49">import</span><span style="color:#24292E"> nn</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Define the Generator</span></span>
<span class="line"><span style="color:#D73A49">class</span><span style="color:#6F42C1"> Generator</span><span style="color:#24292E">(</span><span style="color:#6F42C1">nn</span><span style="color:#24292E">.</span><span style="color:#6F42C1">Module</span><span style="color:#24292E">):</span></span>
<span class="line"><span style="color:#D73A49">    def</span><span style="color:#005CC5"> __init__</span><span style="color:#24292E">(self, z_dim</span><span style="color:#D73A49">=</span><span style="color:#005CC5">100</span><span style="color:#24292E">, img_dim</span><span style="color:#D73A49">=</span><span style="color:#005CC5">784</span><span style="color:#24292E">):</span></span>
<span class="line"><span style="color:#005CC5">        super</span><span style="color:#24292E">().</span><span style="color:#005CC5">__init__</span><span style="color:#24292E">()</span></span>
<span class="line"><span style="color:#005CC5">        self</span><span style="color:#24292E">.net </span><span style="color:#D73A49">=</span><span style="color:#24292E"> nn.Sequential(</span></span>
<span class="line"><span style="color:#24292E">            nn.Linear(z_dim, </span><span style="color:#005CC5">256</span><span style="color:#24292E">),</span></span>
<span class="line"><span style="color:#24292E">            nn.ReLU(</span><span style="color:#005CC5">True</span><span style="color:#24292E">),</span></span>
<span class="line"><span style="color:#24292E">            nn.Linear(</span><span style="color:#005CC5">256</span><span style="color:#24292E">, </span><span style="color:#005CC5">512</span><span style="color:#24292E">),</span></span>
<span class="line"><span style="color:#24292E">            nn.ReLU(</span><span style="color:#005CC5">True</span><span style="color:#24292E">),</span></span>
<span class="line"><span style="color:#24292E">            nn.Linear(</span><span style="color:#005CC5">512</span><span style="color:#24292E">, img_dim),</span></span>
<span class="line"><span style="color:#24292E">            nn.Tanh()  </span><span style="color:#6A737D"># Normalize output to [-1, 1]</span></span>
<span class="line"><span style="color:#24292E">        )</span></span>
<span class="line"><span style="color:#D73A49">    def</span><span style="color:#6F42C1"> forward</span><span style="color:#24292E">(self, z):</span></span>
<span class="line"><span style="color:#D73A49">        return</span><span style="color:#005CC5"> self</span><span style="color:#24292E">.net(z)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Define the Discriminator</span></span>
<span class="line"><span style="color:#D73A49">class</span><span style="color:#6F42C1"> Discriminator</span><span style="color:#24292E">(</span><span style="color:#6F42C1">nn</span><span style="color:#24292E">.</span><span style="color:#6F42C1">Module</span><span style="color:#24292E">):</span></span>
<span class="line"><span style="color:#D73A49">    def</span><span style="color:#005CC5"> __init__</span><span style="color:#24292E">(self, img_dim</span><span style="color:#D73A49">=</span><span style="color:#005CC5">784</span><span style="color:#24292E">):</span></span>
<span class="line"><span style="color:#005CC5">        super</span><span style="color:#24292E">().</span><span style="color:#005CC5">__init__</span><span style="color:#24292E">()</span></span>
<span class="line"><span style="color:#005CC5">        self</span><span style="color:#24292E">.net </span><span style="color:#D73A49">=</span><span style="color:#24292E"> nn.Sequential(</span></span>
<span class="line"><span style="color:#24292E">            nn.Linear(img_dim, </span><span style="color:#005CC5">512</span><span style="color:#24292E">),</span></span>
<span class="line"><span style="color:#24292E">            nn.LeakyReLU(</span><span style="color:#005CC5">0.2</span><span style="color:#24292E">, </span><span style="color:#E36209">inplace</span><span style="color:#D73A49">=</span><span style="color:#005CC5">True</span><span style="color:#24292E">),</span></span>
<span class="line"><span style="color:#24292E">            nn.Linear(</span><span style="color:#005CC5">512</span><span style="color:#24292E">, </span><span style="color:#005CC5">256</span><span style="color:#24292E">),</span></span>
<span class="line"><span style="color:#24292E">            nn.LeakyReLU(</span><span style="color:#005CC5">0.2</span><span style="color:#24292E">, </span><span style="color:#E36209">inplace</span><span style="color:#D73A49">=</span><span style="color:#005CC5">True</span><span style="color:#24292E">),</span></span>
<span class="line"><span style="color:#24292E">            nn.Linear(</span><span style="color:#005CC5">256</span><span style="color:#24292E">, </span><span style="color:#005CC5">1</span><span style="color:#24292E">),</span></span>
<span class="line"><span style="color:#24292E">            nn.Sigmoid() </span><span style="color:#6A737D"># Output a probability value [0, 1]</span></span>
<span class="line"><span style="color:#24292E">        )</span></span>
<span class="line"><span style="color:#D73A49">    def</span><span style="color:#6F42C1"> forward</span><span style="color:#24292E">(self, x):</span></span>
<span class="line"><span style="color:#D73A49">        return</span><span style="color:#005CC5"> self</span><span style="color:#24292E">.net(x)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Initialize models, optimizers, and loss function</span></span>
<span class="line"><span style="color:#24292E">G </span><span style="color:#D73A49">=</span><span style="color:#24292E"> Generator()</span></span>
<span class="line"><span style="color:#24292E">D </span><span style="color:#D73A49">=</span><span style="color:#24292E"> Discriminator()</span></span>
<span class="line"><span style="color:#24292E">g_opt </span><span style="color:#D73A49">=</span><span style="color:#24292E"> torch.optim.Adam(G.parameters(), </span><span style="color:#E36209">lr</span><span style="color:#D73A49">=</span><span style="color:#005CC5">2e-4</span><span style="color:#24292E">)</span></span>
<span class="line"><span style="color:#24292E">d_opt </span><span style="color:#D73A49">=</span><span style="color:#24292E"> torch.optim.Adam(D.parameters(), </span><span style="color:#E36209">lr</span><span style="color:#D73A49">=</span><span style="color:#005CC5">2e-4</span><span style="color:#24292E">)</span></span>
<span class="line"><span style="color:#24292E">criterion </span><span style="color:#D73A49">=</span><span style="color:#24292E"> nn.BCELoss()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#005CC5">print</span><span style="color:#24292E">(</span><span style="color:#032F62">"GAN components initialized successfully."</span><span style="color:#24292E">)</span></span></code></pre>
<hr>
<h2 id="10-references"><a aria-hidden="true" tabindex="-1" href="#10-references"><span class="icon icon-link"></span></a>10. References</h2>
<ul>
<li><em>Pattern Recognition and Machine Learning</em> — Christopher M. Bishop</li>
<li><em>Deep Learning</em> — Ian Goodfellow, Yoshua Bengio, and Aaron Courville</li>
<li><a href="https://scikit-learn.org/stable/documentation.html">Scikit-learn Official Documentation</a></li>
<li><a href="https://pytorch.org/docs/stable/index.html">PyTorch Official Documentation</a></li>
</ul> <html lang="en" data-astro-cid-uffxixac> <head><!-- Global Metadata --><meta charset="utf-8"><!--<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">--><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"><link rel="shortcut icon" href="/favicon.png" type="image/png"><link rel="sitemap" href="/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" title="Ge Yuxu • AI &#38; Engineering" href="https://geyuxu.com/rss.xml"><meta name="generator" content="Astro v5.5.6"><!-- Font preloads --><link rel="preload" href="/fonts/atkinson-regular.woff" as="font" type="font/woff" crossorigin><link rel="preload" href="/fonts/atkinson-bold.woff" as="font" type="font/woff" crossorigin><!-- Canonical URL --><link rel="canonical" href="https://geyuxu.com/blog/ai/supervised-vs-unsupervised-learning-en/"><!-- Primary Meta Tags --><title>Ge Yuxu • AI &amp; Engineering</title><meta name="title" content="Ge Yuxu • AI &#38; Engineering"><meta name="description" content="Welcome to my blog!"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://geyuxu.com/blog/ai/supervised-vs-unsupervised-learning-en/"><meta property="og:title" content="Ge Yuxu • AI &#38; Engineering"><meta property="og:description" content="Welcome to my blog!"><meta property="og:image" content="https://geyuxu.com/blog-placeholder-1.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://geyuxu.com/blog/ai/supervised-vs-unsupervised-learning-en/"><meta property="twitter:title" content="Ge Yuxu • AI &#38; Engineering"><meta property="twitter:description" content="Welcome to my blog!"><meta property="twitter:image" content="https://geyuxu.com/blog-placeholder-1.jpg"><script src="/js/jquery-3.7.1.min.js"></script><script src="/js/copy-code-button.js" defer></script><!-- Dify Chatbot Configuration --><script>
 window.difyChatbotConfig = {
  token: 'VgrjCAto93bjE0MW',
  systemVariables: {
    // user_id: 'YOU CAN DEFINE USER ID HERE',
    // conversation_id: 'YOU CAN DEFINE CONVERSATION ID HERE, IT MUST BE A VALID UUID',
  },
  userVariables: {
    // avatar_url: 'YOU CAN DEFINE USER AVATAR URL HERE',
    // name: 'YOU CAN DEFINE USER NAME HERE',
  },
 }
</script><!-- Dify Chatbot Script --><script src="https://udify.app/embed.min.js" id="VgrjCAto93bjE0MW" defer>
</script><!-- Dify Chatbot Custom Styles --></head> <body data-astro-cid-uffxixac>  <div class="statement" data-astro-cid-uffxixac> <blockquote data-astro-cid-uffxixac> <p data-astro-cid-uffxixac><strong data-astro-cid-uffxixac>脱敏说明</strong>：本文所有出现的表名、字段名、接口地址、变量名、IP地址及示例数据等均非真实，仅用于阐述技术思路与实现步骤，示例代码亦非公司真实代码。示例方案亦非公司真实完整方案，仅为本人记忆总结，用于技术学习探讨。<br data-astro-cid-uffxixac>
&nbsp;&nbsp;&nbsp;&nbsp;•&nbsp;文中所示任何标识符并不对应实际生产环境中的名称或编号。<br data-astro-cid-uffxixac>
&nbsp;&nbsp;&nbsp;&nbsp;•&nbsp;示例&nbsp;SQL、脚本、代码及数据等均为演示用途，不含真实业务数据，也不具备直接运行或复现的完整上下文。<br data-astro-cid-uffxixac>
&nbsp;&nbsp;&nbsp;&nbsp;•&nbsp;读者若需在实际项目中参考本文方案，请结合自身业务场景及数据安全规范，使用符合内部命名和权限控制的配置。</p> <p data-astro-cid-uffxixac><strong data-astro-cid-uffxixac>Data Desensitization Notice</strong>: All table names, field names, API endpoints, variable names, IP addresses, and sample data appearing in this article are fictitious and intended solely to illustrate technical concepts and implementation steps. The sample code is not actual company code. The proposed solutions are not complete or actual company solutions but are summarized from the author's memory for technical learning and discussion.<br data-astro-cid-uffxixac>
&nbsp;&nbsp;&nbsp;&nbsp;•&nbsp;Any identifiers shown in the text do not correspond to names or numbers in any actual production environment.<br data-astro-cid-uffxixac>
&nbsp;&nbsp;&nbsp;&nbsp;•&nbsp;Sample SQL, scripts, code, and data are for demonstration purposes only, do not contain real business data, and lack the full context required for direct execution or reproduction.<br data-astro-cid-uffxixac>
&nbsp;&nbsp;&nbsp;&nbsp;•&nbsp;Readers who wish to reference the solutions in this article for actual projects should adapt them to their own business scenarios and data security standards, using configurations that comply with internal naming and access control policies.</p> <p data-astro-cid-uffxixac><strong data-astro-cid-uffxixac>版权声明</strong>：本文版权归原作者所有，未经作者事先书面许可，任何单位或个人不得以任何方式复制、转载、摘编或用于商业用途。<br data-astro-cid-uffxixac>
&nbsp;&nbsp;&nbsp;&nbsp;•&nbsp;若需非商业性引用或转载本文内容，请务必注明出处并保持内容完整。<br data-astro-cid-uffxixac>
&nbsp;&nbsp;&nbsp;&nbsp;•&nbsp;对因商业使用、篡改或不当引用本文内容所产生的法律纠纷，作者保留追究法律责任的权利。</p> <p data-astro-cid-uffxixac><strong data-astro-cid-uffxixac>Copyright Notice</strong>: The copyright of this article belongs to the original author. Without prior written permission from the author, no entity or individual may copy, reproduce, excerpt, or use it for commercial purposes in any way.<br data-astro-cid-uffxixac>
&nbsp;&nbsp;&nbsp;&nbsp;•&nbsp;For non-commercial citation or reproduction of this content, attribution must be given, and the integrity of the content must be maintained.<br data-astro-cid-uffxixac>
&nbsp;&nbsp;&nbsp;&nbsp;•&nbsp;The author reserves the right to pursue legal action against any legal disputes arising from the commercial use, alteration, or improper citation of this article's content.</p> <p data-astro-cid-uffxixac><em data-astro-cid-uffxixac>Copyright&nbsp;©&nbsp;1989–Present&nbsp;Ge&nbsp;Yuxu.&nbsp;All&nbsp;Rights&nbsp;Reserved.</em></p> </blockquote> </div></body></html>  </article> </div> </main> <footer data-astro-cid-sz7xmlte>
&copy; 2025 All rights reserved.
</footer>  </body></html>