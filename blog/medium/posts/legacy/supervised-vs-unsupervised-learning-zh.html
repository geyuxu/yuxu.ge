<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>监督学习 vs 非监督学习：概念、算法与实践</title>
    <style>
        body { font-family: Georgia, serif; max-width: 700px; margin: 2rem auto; padding: 0 1rem; line-height: 1.7; color: #333; }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; }
        h2 { font-size: 1.4rem; margin-top: 2rem; }
        pre { background: #f4f4f4; padding: 1rem; overflow-x: auto; border-radius: 4px; }
        code { font-family: Menlo, Monaco, monospace; font-size: 0.9em; }
        p code { background: #f4f4f4; padding: 0.2em 0.4em; border-radius: 3px; }
        img { max-width: 100%; }
        blockquote { border-left: 3px solid #ddd; margin-left: 0; padding-left: 1rem; color: #666; }
        ul { padding-left: 1.5rem; }
        hr { border: none; border-top: 1px solid #ddd; margin: 2rem 0; }
        a { color: #1a8917; }
    </style>
</head>
<body>
<hr>
<h2>date: 2024-01-01
tags: [ai, 机器学习, 监督学习, 非监督学习, ai]
legacy: true</h2>
<h1>监督学习 vs 非监督学习：概念、算法与实践</h1>
<hr>
<h2>2. 核心概念与差异</h2>
<h3>2.1 监督学习（Supervised Learning）</h3>
<p>监督学习的核心思想是<strong>从“有标签”的数据中学习</strong>。这里的“标签”（Label）是我们希望模型预测的正确答案。</p>
<ul>
<li><strong>定义</strong>：给定一组输入数据 <code>X</code> 和其对应的输出标签 <code>y</code>，算法的目标是学习一个映射函数 <code>f</code>，使得 <code>f(x) ≈ y</code>。</li>
<li><strong>目标</strong>：最小化模型预测值与真实标签之间的<strong>可度量误差</strong>，例如均方误差（MSE）用于回归，交叉熵（Cross-Entropy）用于分类。</li>
<li><strong>典型任务</strong>：<ul>
<li><strong>分类（Classification）</strong>：预测离散的类别标签。例如，判断一封邮件是否为垃圾邮件（二分类），或者识别一张图片中的动物是猫、狗还是鸟（多分类）。</li>
<li><strong>回归（Regression）</strong>：预测连续的数值。例如，根据房屋特征预测其售价，或者根据历史数据预测未来一天的气温。</li>
</ul>
</li>
</ul>
<h3>2.2 非监督学习（Unsupervised Learning）</h3>
<p>与监督学习相反，非监督学习处理的是**“无标签”的数据**。它不需要人工标注的答案，而是致力于发现数据自身内在的结构和模式。</p>
<ul>
<li><strong>定义</strong>：仅给定输入数据 <code>X</code>，算法的目标是挖掘数据中隐藏的结构。</li>
<li><strong>目标</strong>：探索数据的内在规律，如<strong>相似性、密度、潜在因子</strong>等。</li>
<li><strong>典型任务</strong>：<ul>
<li><strong>聚类（Clustering）</strong>：将相似的数据点分到同一个簇（Cluster）。例如，根据用户的购买行为将其划分为不同群体（高价值用户、潜力用户等）。</li>
<li><strong>降维（Dimensionality Reduction）/可视化</strong>：在保留核心信息的前提下，减少数据的特征数量。例如，将高维的用户画像数据压缩到二维平面上进行可视化。</li>
<li><strong>密度估计（Density Estimation）/生成建模</strong>：学习数据的分布，从而可以生成新的、与原始数据相似的样本。例如，生成逼真的人脸图像。</li>
</ul>
</li>
</ul>
<h3>2.3 差异一览表</h3>
<p><img src="/blog/images/tables/table-89640ea2.png" alt="Table"></p><hr>
<h2>3. 工作流程对比</h2>
<h3>3.1 监督学习流水线</h3>
<p>一个典型的监督学习项目遵循一个相对标准化的流程：</p>
<ol>
<li><strong>数据标注与划分</strong>：获取或标注高质量的标签数据，并将其划分为训练集、验证集和测试集。</li>
<li><strong>特征工程与模型选择</strong>：根据业务理解提取有效特征，并选择合适的模型（如线性模型、树模型或神经网络）。</li>
<li><strong>训练与调优</strong>：在训练集上训练模型，并在验证集上调整超参数（如学习率、树的深度）。</li>
<li><strong>评估与上线</strong>：在测试集上评估最终模型的性能，达到标准后部署上线。</li>
<li><strong>监控与迭代</strong>：持续监控线上模型的表现，警惕“概念漂移”（数据分布变化），并定期使用新数据进行再训练。</li>
</ol>
<h3>3.2 非监督学习流水线</h3>
<p>非监督学习的流程更具探索性：</p>
<ol>
<li><strong>数据预处理</strong>：数据标准化或归一化至关重要，因为许多算法（如 K-means、PCA）对尺度敏感。同时需要选择合适的距离度量方式（如欧氏距离、余弦相似度）。</li>
<li><strong>算法与超参数探索</strong>：选择合适的算法（如 K-means、DBSCAN），并探索其关键超参数（如簇的数量 <code>k</code>、邻域半径 <code>ε</code>）。</li>
<li><strong>结果可视化与业务验证</strong>：由于没有“正确答案”，通常需要将结果（如聚类簇、降维图）可视化，并结合业务知识来验证其有效性和可解释性。</li>
<li><strong>下游应用</strong>：非监督学习的结果往往作为下游任务的输入。例如，将聚类结果作为用户标签，或将降维后的特征用于后续的监督学习模型。</li>
</ol>
<hr>
<h2>4. 典型算法速览</h2>
<h3>4.1 监督学习算法</h3>
<p><img src="/blog/images/tables/table-304267f0.png" alt="Table"></p><h3>4.2 非监督学习算法</h3>
<p><img src="/blog/images/tables/table-edc74622.png" alt="Table"></p><hr>
<h2>5. 场景与案例</h2>
<p><img src="/blog/images/tables/table-201316f9.png" alt="Table"></p><hr>
<h2>6. 拓展范式</h2>
<p>监督与非监督并非泾渭分明，实践中涌现了许多强大的混合范式：</p>
<ul>
<li><strong>半监督学习（Semi-supervised Learning）</strong>：当拥有少量有标签数据和大量无标签数据时，通过伪标签（Pseudo-Labeling）、一致性正则化等技术，利用无标签数据提升模型性能。</li>
<li><strong>弱监督学习（Weakly Supervised Learning）</strong>：标签不完全准确或不完整（例如，只知道一张图里有猫，但不知道猫的具体位置）。</li>
<li><strong>自监督学习（Self-supervised Learning）</strong>：从数据自身构造“伪任务”来生成标签。例如，在文本中随机遮盖一个词（Masked Language Model），让模型去预测它，这是 BERT 等预训练语言模型的核心思想。</li>
<li><strong>强化学习（Reinforcement Learning）</strong>：智能体（Agent）通过与环境交互，根据获得的奖励或惩罚来学习最优策略。它常与监督学习结合使用，如 AlphaGo。</li>
</ul>
<hr>
<h2>7. 选型指南 &amp; 实战技巧</h2>
<ol>
<li><p><strong>从数据和标签出发</strong>：</p>
<ul>
<li><strong>有高质量标签</strong>：首选监督学习。</li>
<li><strong>标签获取成本高昂</strong>：优先考虑非监督学习进行数据探索（聚类、可视化），或使用自监督/半监督方法减少对标签的依赖。</li>
</ul>
</li>
<li><p><strong>考虑模型规模与数据复杂度</strong>：</p>
<ul>
<li><strong>大规模感知任务（图像、语音、文本）</strong>：深度学习网络是最佳选择。</li>
<li><strong>小样本、高维度数据</strong>：SVM 或树模型（如随机森林）可能表现更佳。</li>
<li><strong>结构化/表格数据</strong>：XGBoost/LightGBM 通常是性能之王。</li>
</ul>
</li>
<li><p><strong>平衡可解释性与精度</strong>：</p>
<ul>
<li><strong>金融风控、医疗等高风险或需合规的场景</strong>：线性模型、逻辑回归或决策树因其良好的可解释性而备受青睐。</li>
<li><strong>互联网广告、推荐等追求极致效果的场景</strong>：精度更高的复杂模型（如深度网络）是首选。</li>
</ul>
</li>
<li><p><strong>结合离线探索与在线应用</strong>：</p>
<ul>
<li>一个常见的模式是：先用<strong>非监督学习</strong>在离线数据上进行探索性分析，发现潜在的用户群体或数据模式。然后，将这些发现作为特征或目标，用于构建<strong>监督学习</strong>模型，并部署到线上提供实时预测服务。</li>
</ul>
</li>
</ol>
<hr>
<h2>8. 总结</h2>
<p>监督学习与非监督学习是解决不同问题的两种强大工具，它们的核心区别在于是否依赖“标准答案”。</p>
<ul>
<li><strong>监督学习擅长“回答已知问题”</strong>：在明确的目标和高质量的标签驱动下，它能做出精准的预测。</li>
<li><strong>非监督学习擅长“发现未知问题”</strong>：在没有先验知识的情况下，它能揭示数据中隐藏的结构、模式和洞见。</li>
</ul>
<p>在真实的机器学习项目中，两者往往不是孤立的。最强大的解决方案常常是将它们结合起来：<strong>先用非监督学习探索数据的可能性，再用监督学习实现精准的目标建模，形成一个从数据洞察到价值创造的完整闭环。</strong></p>
<hr>
<h2>9. 代码示例</h2>
<h3>9.1 环境准备</h3>
<p><code>pip install scikit-learn matplotlib torch torchvision</code></p><h3>9.2 监督学习示例</h3>
<h4>线性回归 (Boston Housing)</h4>
<p><em>注意：<code>load_boston</code> 在 scikit-learn 1.2 版本后被移除，这里仅作经典示例。可替换为其他回归数据集。</em></p>
<blockquote><code>from sklearn.datasets import fetch_california_housing</code><br>
<code>from sklearn.model_selection import train_test_split</code><br>
<code>from sklearn.linear_model import LinearRegression</code><br>
<code>from sklearn.metrics import mean_squared_error</code><br>
<code></code><br>
<code># 加州房价数据集</code><br>
<code>X, y = fetch_california_housing(return_X_y=True)</code><br>
<code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</code><br>
<code></code><br>
<code>model = LinearRegression().fit(X_train, y_train)</code><br>
<code>pred = model.predict(X_test)</code><br>
<code>print(f&quot;RMSE on California Housing: {mean_squared_error(y_test, pred, squared=False):.2f}&quot;)</code></blockquote><h4>逻辑回归 (乳腺癌二分类)</h4>
<blockquote><code>from sklearn.datasets import load_breast_cancer</code><br>
<code>from sklearn.linear_model import LogisticRegression</code><br>
<code>from sklearn.preprocessing import StandardScaler</code><br>
<code></code><br>
<code>X, y = load_breast_cancer(return_X_y=True)</code><br>
<code># 归一化提升性能</code><br>
<code>scaler = StandardScaler()</code><br>
<code>X_scaled = scaler.fit_transform(X)</code><br>
<code></code><br>
<code>clf = LogisticRegression(max_iter=1000).fit(X_scaled, y)</code><br>
<code>print(f&quot;Accuracy on Breast Cancer: {clf.score(X_scaled, y):.3f}&quot;)</code></blockquote><h3>9.3 非监督学习示例</h3>
<h4>K-means 聚类 + 可视化</h4>
<blockquote><code>from sklearn.datasets import load_iris</code><br>
<code>from sklearn.cluster import KMeans</code><br>
<code>import matplotlib.pyplot as plt</code><br>
<code></code><br>
<code>X, y = load_iris(return_X_y=True) # y 在这里只用于后续对比，K-means本身不用</code><br>
<code>kmeans = KMeans(n_clusters=3, random_state=42, n_init=10).fit(X) # n_init=&#039;auto&#039; in future</code><br>
<code></code><br>
<code># 可视化前两个特征</code><br>
<code>plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap=&#039;viridis&#039;)</code><br>
<code>plt.title(&#039;K-means Clustering on Iris Dataset&#039;)</code><br>
<code>plt.xlabel(&#039;Sepal Length&#039;)</code><br>
<code>plt.ylabel(&#039;Sepal Width&#039;)</code><br>
<code>plt.show()</code></blockquote><h4>PCA + t-SNE 可视化</h4>
<blockquote><code>from sklearn.decomposition import PCA</code><br>
<code>from sklearn.manifold import TSNE</code><br>
<code>import matplotlib.pyplot as plt</code><br>
<code>from sklearn.datasets import load_iris</code><br>
<code></code><br>
<code>X, y = load_iris(return_X_y=True)</code><br>
<code></code><br>
<code># 先用PCA降到合理的中间维度</code><br>
<code># ... (10 more lines)</code></blockquote>
<p><em>View full code: <a href="https://gist.github.com/geyuxu/135a4befad72fd93c7c3b40015a57b48">GitHub Gist</a></em></p><h3>9.4 简易 GAN 骨架 (PyTorch)</h3>
<p>这是一个极简的 GAN 结构，用于演示其核心组件，并非一个完整的训练脚本。</p>
<blockquote><code>import torch</code><br>
<code>from torch import nn</code><br>
<code></code><br>
<code># 定义生成器</code><br>
<code>class Generator(nn.Module):</code><br>
<code>    def __init__(self, z_dim=100, img_dim=784):</code><br>
<code>        super().__init__()</code><br>
<code>        self.net = nn.Sequential(</code><br>
<code># ... (33 more lines)</code></blockquote>
<p><em>View full code: <a href="https://gist.github.com/geyuxu/384791015b6be550f955c1b3ec968778">GitHub Gist</a></em></p><hr>
<h2>10. 参考资料</h2>
<ul>
<li><em>Pattern Recognition and Machine Learning</em> — Christopher M. Bishop</li>
<li><em>Deep Learning</em> — Ian Goodfellow, Yoshua Bengio, and Aaron Courville</li>
<li><a href="https://scikit-learn.org/stable/documentation.html">Scikit-learn 官方文档</a></li>
<li><a href="https://pytorch.org/docs/stable/index.html">PyTorch 官方文档</a></li>
</ul>

</body>
</html>