<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Supervised vs. Unsupervised Learning: Concepts, Algorithms, and Practice | Yuxu Ge</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1/themes/prism-tomorrow.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <style>
        :root {
            --black: #111;
            --dark-grey: #444;
            --off-white: #f4f4f4;
            --vermilion: #C41E3A;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }

        html {
            font-size: 16px;
            scroll-behavior: smooth;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: var(--black);
            background-color: var(--off-white);
        }

        a { color: var(--vermilion); text-decoration: none; }
        a:hover { opacity: 0.75; }

        .layout { min-height: 100vh; }

        /* Sidebar */
        .sidebar {
            position: fixed; top: 0; left: 0;
            width: 320px; height: 100vh;
            background: var(--black); color: var(--off-white);
            padding: 3rem 2rem;
            display: flex; flex-direction: column; justify-content: space-between;
        }
        .sidebar-top { display: flex; flex-direction: column; align-items: center; text-align: center; }
        .avatar { display: block; width: 120px; height: 120px; border-radius: 50%; border: 3px solid var(--vermilion); margin-bottom: 1.5rem; overflow: hidden; transition: transform 0.2s ease; }
        .avatar:hover { opacity: 1; transform: scale(1.05); }
        .avatar img { width: 100%; height: 100%; object-fit: cover; }
        .name-block { margin-bottom: 0.5rem; }
        h1 { font-size: 1.8rem; font-weight: 700; letter-spacing: 1px; color: var(--off-white); }
        .title-role { font-size: 0.9rem; color: #888; margin-top: 0.25rem; font-weight: 400; }
        .contact-block { margin-top: 2.5rem; width: 100%; }
        .social-links { display: flex; flex-direction: column; align-items: center; gap: 0.75rem; }
        .social-links a { display: flex; align-items: center; gap: 0.75rem; width: 160px; color: var(--off-white); opacity: 0.7; padding: 0.5rem 0.75rem; border-radius: 4px; font-size: 0.9rem; transition: all 0.2s; }
        .social-links a:hover { color: var(--vermilion); opacity: 1; background: rgba(255,255,255,0.05); }
        .social-links svg { width: 20px; height: 20px; flex-shrink: 0; }
        .social-links .orcid-link { font-size: 0.68rem; gap: 0.5rem; white-space: nowrap; }
        .sidebar-footer { text-align: center; font-size: 0.75rem; color: #555; }

        /* Content */
        .content { margin-left: 320px; padding: 3rem 4rem; max-width: 900px; }
        .back-link { display: inline-block; font-size: 0.85rem; margin-bottom: 2rem; color: var(--dark-grey); }
        .back-link:hover { color: var(--vermilion); }

        /* Article */
        article h1 { font-size: 2rem; font-weight: 700; margin-bottom: 1.5rem; line-height: 1.3; color: var(--black); }
        article h2 { font-size: 1.4rem; font-weight: 600; margin: 2rem 0 1rem; color: var(--black); }
        article h3 { font-size: 1.1rem; font-weight: 600; margin: 1.5rem 0 0.75rem; color: var(--black); }
        article p { margin-bottom: 1rem; color: var(--dark-grey); }
        article ul, article ol { margin: 1rem 0 1rem 1.5rem; color: var(--dark-grey); }
        article li { margin-bottom: 0.5rem; }
        article strong { color: var(--black); }
        article code { font-family: "SF Mono", Monaco, monospace; font-size: 0.9em; background: #e8e8e8; padding: 0.15em 0.4em; border-radius: 3px; }
        article pre { margin: 1rem 0; border-radius: 6px; overflow-x: auto; background: #2d2d2d; padding: 1rem; }
        article pre code { background: none; padding: 0; font-size: 0.75em; white-space: pre !important; counter-reset: line; display: block; color: #ccc; }
        article pre code .line { counter-increment: line; }
        article pre code .line::before { content: counter(line); display: inline-block; width: 2.5em; margin-right: 1em; text-align: right; color: #666; user-select: none; }
        .code-toolbar { display: flex; justify-content: flex-start; padding: 0.35rem 0.5rem; background: #3a3a3a; border-radius: 6px 6px 0 0; }
        .code-toolbar + pre { margin-top: 0; border-radius: 0 0 6px 6px; }
        .copy-btn { padding: 0.2rem 0.5rem; font-size: 0.7rem; background: #555; color: #fff; border: none; border-radius: 3px; cursor: pointer; transition: all 0.2s; }
        .copy-btn:hover, .copy-btn:active { background: #777; }
        .copy-btn.copied { background: #2a2; }
        article blockquote { border-left: 3px solid var(--vermilion); padding-left: 1rem; margin: 1rem 0; color: #666; font-style: italic; }
        article hr { border: none; border-top: 1px solid #ddd; margin: 2rem 0; }
        article table { width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem; }
        article th, article td { border: 1px solid #ddd; padding: 0.5rem 0.75rem; text-align: left; }
        article th { background: #e8e8e8; font-weight: 600; }
        article img { max-width: 100%; height: auto; display: block; margin: 1.5rem 0; border-radius: 6px; }

        .loading { text-align: center; padding: 3rem; color: #888; }
        .error { color: var(--vermilion); }

        /* KaTeX math styles */
        .katex-display { overflow-x: auto; overflow-y: hidden; padding: 0.5rem 0; }
        .katex { font-size: 1.1em; }

        /* Responsive */
        @media (max-width: 900px) {
            .sidebar { position: relative; width: 100%; height: auto; padding: 2rem 1.5rem 1rem; }
            .social-links { flex-direction: row; flex-wrap: wrap; justify-content: center; }
            .social-links a, .social-links .orcid-link { width: auto; padding: 0.5rem; font-size: 0; gap: 0; }
            .social-links svg { width: 24px; height: 24px; }
            .social-links .hide-mobile { display: none; }
            .sidebar-footer { margin-top: 1rem; }
            .content { margin-left: 0; padding: 2rem 1rem; }
        }
        @media (max-width: 480px) {
            .sidebar { padding: 1.5rem 1rem 1rem; }
            .avatar { width: 80px; height: 80px; }
            h1 { font-size: 1.4rem; }
            article h1 { font-size: 1.5rem; }
        }
    </style>
</head>
<body>

<div class="layout">
    <aside class="sidebar" id="sidebar-content"></aside>
    <script src="/components/sidebar.js"></script>

    <main class="content">
        <a href="/blog/" class="back-link">← Back to Blog</a>
        <article>
<hr>
<h2>date: 2024-01-01
tags: [ai, machine learning, supervised learning, unsupervised learning, ai]
legacy: true</h2>
<h1>Supervised vs. Unsupervised Learning: Concepts, Algorithms, and Practice</h1>
<hr>
<h2>2. Core Concepts and Differences</h2>
<h3>2.1 Supervised Learning</h3>
<p>The core idea of supervised learning is to <strong>learn from &quot;labeled&quot; data</strong>. The &quot;label&quot; here is the correct answer we want the model to predict.</p>
<ul>
<li><strong>Definition</strong>: Given a set of input data <code>X</code> and its corresponding output labels <code>y</code>, the goal of the algorithm is to learn a mapping function <code>f</code> such that <code>f(x) ≈ y</code>.</li>
<li><strong>Objective</strong>: To minimize a <strong>measurable error</strong> between the model&#39;s predictions and the true labels, such as Mean Squared Error (MSE) for regression or Cross-Entropy for classification.</li>
<li><strong>Typical Tasks</strong>:<ul>
<li><strong>Classification</strong>: Predicting a discrete class label. For example, determining if an email is spam or not (binary classification), or identifying an animal in a picture as a cat, dog, or bird (multi-class classification).</li>
<li><strong>Regression</strong>: Predicting a continuous numerical value. For example, predicting the price of a house based on its features, or forecasting the temperature for the next day based on historical data.</li>
</ul>
</li>
</ul>
<h3>2.2 Unsupervised Learning</h3>
<p>In contrast to supervised learning, unsupervised learning deals with <strong>&quot;unlabeled&quot; data</strong>. It does not require manually annotated answers; instead, it strives to discover the inherent structure and patterns within the data itself.</p>
<ul>
<li><strong>Definition</strong>: Given only input data <code>X</code>, the algorithm&#39;s goal is to uncover hidden structures in the data.</li>
<li><strong>Objective</strong>: To explore the intrinsic patterns of the data, such as <strong>similarity, density, or latent factors</strong>.</li>
<li><strong>Typical Tasks</strong>:<ul>
<li><strong>Clustering</strong>: Grouping similar data points into the same cluster. For example, segmenting customers into different groups (high-value, potential, etc.) based on their purchasing behavior.</li>
<li><strong>Dimensionality Reduction/Visualization</strong>: Reducing the number of features in the data while preserving its core information. For example, compressing high-dimensional user profiles into a 2D plane for visualization.</li>
<li><strong>Density Estimation/Generative Modeling</strong>: Learning the distribution of the data to generate new, similar samples. For example, creating realistic human face images.</li>
</ul>
</li>
</ul>
<h3>2.3 At-a-Glance Comparison</h3>
<table>
<thead>
<tr>
<th align="left">Dimension</th>
<th align="left">Supervised Learning</th>
<th align="left">Unsupervised Learning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>Training Data</strong></td>
<td align="left">Labeled (X, y)</td>
<td align="left">Unlabeled (X)</td>
</tr>
<tr>
<td align="left"><strong>Primary Goal</strong></td>
<td align="left">Predict a clear output</td>
<td align="left">Discover hidden structures</td>
</tr>
<tr>
<td align="left"><strong>Evaluation</strong></td>
<td align="left">Compare against true labels (Accuracy, RMSE, F1-Score...)</td>
<td align="left">Indirect metrics (Silhouette Score, Reconstruction Error...)</td>
</tr>
<tr>
<td align="left"><strong>Common Risks</strong></td>
<td align="left">Overfitting, high annotation costs</td>
<td align="left">Difficult to interpret results, ambiguous evaluation</td>
</tr>
</tbody></table>
<hr>
<h2>3. Workflow Comparison</h2>
<h3>3.1 Supervised Learning Pipeline</h3>
<p>A typical supervised learning project follows a relatively standardized process:</p>
<ol>
<li><strong>Data Annotation and Splitting</strong>: Obtain or annotate high-quality labeled data and split it into training, validation, and test sets.</li>
<li><strong>Feature Engineering and Model Selection</strong>: Extract effective features based on business understanding and choose a suitable model (e.g., linear model, tree-based model, or neural network).</li>
<li><strong>Training and Tuning</strong>: Train the model on the training set and tune hyperparameters (e.g., learning rate, tree depth) on the validation set.</li>
<li><strong>Evaluation and Deployment</strong>: Evaluate the final model&#39;s performance on the test set and deploy it to production if it meets the criteria.</li>
<li><strong>Monitoring and Iteration</strong>: Continuously monitor the model&#39;s online performance, watch for &quot;concept drift&quot; (changes in data distribution), and periodically retrain it with new data.</li>
</ol>
<h3>3.2 Unsupervised Learning Pipeline</h3>
<p>The unsupervised learning process is more exploratory:</p>
<ol>
<li><strong>Data Preprocessing</strong>: Data standardization or normalization is crucial, as many algorithms (like K-means, PCA) are sensitive to scale. An appropriate distance metric (e.g., Euclidean distance, cosine similarity) must also be chosen.</li>
<li><strong>Algorithm and Hyperparameter Exploration</strong>: Select a suitable algorithm (e.g., K-means, DBSCAN) and explore its key hyperparameters (e.g., number of clusters <code>k</code>, neighborhood radius <code>ε</code>).</li>
<li><strong>Result Visualization and Business Validation</strong>: Since there is no &quot;correct answer,&quot; results (like clusters or dimensionality reduction plots) often need to be visualized and validated with business knowledge for effectiveness and interpretability.</li>
<li><strong>Downstream Applications</strong>: The results of unsupervised learning often serve as input for downstream tasks. For example, using cluster assignments as user tags or using reduced-dimension features for a subsequent supervised learning model.</li>
</ol>
<hr>
<h2>4. A Quick Look at Typical Algorithms</h2>
<h3>4.1 Supervised Learning Algorithms</h3>
<table>
<thead>
<tr>
<th align="left">Algorithm</th>
<th align="left">One-Sentence Summary</th>
<th align="left">Key Features / Use Cases</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>Linear Regression</strong></td>
<td align="left">Fits a straight line by minimizing the squared error between predicted and actual values.</td>
<td align="left">Highly interpretable, serves as a baseline for complex models; used for house price/sales prediction.</td>
</tr>
<tr>
<td align="left"><strong>Logistic Regression</strong></td>
<td align="left">Maps a linear output to the (0,1) interval using the Sigmoid function for binary classification.</td>
<td align="left">Outputs probabilities, easy to understand and implement; widely used for CTR prediction, credit scoring.</td>
</tr>
<tr>
<td align="left"><strong>Decision Tree (CART)</strong></td>
<td align="left">Builds a tree by recursively partitioning data into nodes to increase &quot;purity.&quot;</td>
<td align="left">Intuitive rules, handles non-linearity and missing values, but prone to overfitting.</td>
</tr>
<tr>
<td align="left"><strong>Random Forest</strong></td>
<td align="left">Improves performance by building and combining the votes of multiple decision trees.</td>
<td align="left">Effectively combats overfitting, can assess feature importance, a strong baseline model.</td>
</tr>
<tr>
<td align="left"><strong>Support Vector Machine (SVM)</strong></td>
<td align="left">Finds a hyperplane that separates classes with the maximum margin, using the kernel trick for non-linear problems.</td>
<td align="left">Performs well on small, high-dimensional datasets; used for text classification, image recognition.</td>
</tr>
<tr>
<td align="left"><strong>Boosting (XGBoost/LightGBM)</strong></td>
<td align="left">Iteratively fits the residuals of the previous round, combining weak learners into a strong model.</td>
<td align="left">State-of-the-art on tabular data, feature engineering-friendly.</td>
</tr>
<tr>
<td align="left"><strong>Deep Networks (CNN/Transformer)</strong></td>
<td align="left">Automatically learns hierarchical features from data through multiple non-linear transformations.</td>
<td align="left"><strong>CNNs</strong> excel at capturing local spatial features (images), while <strong>Transformers</strong> handle global dependencies in sequential data (text, speech).</td>
</tr>
</tbody></table>
<h3>4.2 Unsupervised Learning Algorithms</h3>
<table>
<thead>
<tr>
<th align="left">Algorithm</th>
<th align="left">One-Sentence Summary</th>
<th align="left">Key Features / Typical Scenarios</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>K-means</strong></td>
<td align="left">Iteratively updates cluster centroids to minimize the sum of squared distances from each point to its assigned centroid.</td>
<td align="left">Simple and efficient, but requires pre-specifying <code>k</code> and is sensitive to initialization; used for user segmentation.</td>
</tr>
<tr>
<td align="left"><strong>DBSCAN</strong></td>
<td align="left">Defines clusters based on density, automatically identifying noise and discovering arbitrarily shaped clusters.</td>
<td align="left">Does not require pre-setting <code>k</code>, robust to noise; suitable for geospatial data analysis.</td>
</tr>
<tr>
<td align="left"><strong>Hierarchical Clustering</strong></td>
<td align="left">Forms a tree-like hierarchy of clusters by successively merging (bottom-up) or splitting (top-down) data points.</td>
<td align="left">No need to pre-set <code>k</code>, provides a dendrogram for understanding data hierarchy; used in phylogenetic analysis.</td>
</tr>
<tr>
<td align="left"><strong>PCA</strong></td>
<td align="left">Linearly transforms data onto a few orthogonal directions that capture the maximum variance.</td>
<td align="left">The most classic dimensionality reduction method, used for data compression, denoising, and visualization.</td>
</tr>
<tr>
<td align="left"><strong>t-SNE / UMAP</strong></td>
<td align="left">Preserves the local neighborhood structure of high-dimensional data in a low-dimensional space via non-linear embedding.</td>
<td align="left">Excellent for visualizing high-dimensional data (like text, genes), often superior to PCA for this purpose.</td>
</tr>
<tr>
<td align="left"><strong>Gaussian Mixture Model (GMM)</strong></td>
<td align="left">Assumes data is a mixture of several Gaussian distributions and uses the EM algorithm for soft clustering.</td>
<td align="left">Can handle more complex (elliptical) cluster shapes and outputs probabilities of cluster membership.</td>
</tr>
<tr>
<td align="left"><strong>Kernel Density Estimation (KDE)</strong></td>
<td align="left">Smoothly estimates the probability density function of data by placing a kernel (e.g., Gaussian) on each data point.</td>
<td align="left">Used for data distribution visualization and anomaly detection.</td>
</tr>
<tr>
<td align="left"><strong>Generative Adversarial Network (GAN)</strong></td>
<td align="left">A generator and a discriminator compete, with the generator trying to create realistic data and the discriminator trying to tell it apart from real data.</td>
<td align="left">Achieves stunning results in image synthesis and data augmentation.</td>
</tr>
<tr>
<td align="left"><strong>Variational Autoencoder (VAE)</strong></td>
<td align="left">Encodes input into a latent distribution, then samples from it to reconstruct the input, serving as a generative model.</td>
<td align="left">Can generate controllable new samples, and its latent variables have some semantic meaning.</td>
</tr>
</tbody></table>
<hr>
<h2>5. Scenarios and Case Studies</h2>
<table>
<thead>
<tr>
<th align="left">Task</th>
<th align="left">Method Paradigm</th>
<th align="left">Example</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>Medical Image Diagnosis</strong></td>
<td align="left"><strong>Supervised Learning</strong> → CNN/Transformer</td>
<td align="left">Input a CT scan, and the model automatically classifies lesion areas (e.g., tumors, nodules).</td>
</tr>
<tr>
<td align="left"><strong>E-commerce User Segmentation</strong></td>
<td align="left"><strong>Unsupervised Learning</strong> → K-means/DBSCAN</td>
<td align="left">Segment users into different value groups based on their browsing, carting, and purchasing behavior logs.</td>
</tr>
<tr>
<td align="left"><strong>Stylized Image Generation</strong></td>
<td align="left"><strong>Unsupervised Learning</strong> → GAN/VAE</td>
<td align="left">Input a regular photo and generate an artistic image in the style of Van Gogh or ink wash painting.</td>
</tr>
<tr>
<td align="left"><strong>Semi-Supervised Text Classification</strong></td>
<td align="left"><strong>Self-Supervised Pre-training + Supervised Fine-tuning</strong></td>
<td align="left">Use massive unlabeled text for self-supervised learning (like BERT), then fine-tune with a small amount of labeled data to achieve high classification accuracy. This is the dominant paradigm in modern NLP.</td>
</tr>
</tbody></table>
<hr>
<h2>6. Extended Paradigms</h2>
<p>The line between supervised and unsupervised learning is not always sharp. In practice, many powerful hybrid paradigms have emerged:</p>
<ul>
<li><strong>Semi-supervised Learning</strong>: When you have a small amount of labeled data and a large amount of unlabeled data, techniques like Pseudo-Labeling and Consistency Regularization can leverage the unlabeled data to improve model performance.</li>
<li><strong>Weakly Supervised Learning</strong>: The labels are not entirely accurate or complete (e.g., you know an image contains a cat, but not its exact location).</li>
<li><strong>Self-supervised Learning</strong>: Creates &quot;pseudo-tasks&quot; from the data itself to generate labels. For example, randomly masking a word in a text (Masked Language Model) and training the model to predict it, which is the core idea behind pre-trained language models like BERT.</li>
<li><strong>Reinforcement Learning (RL)</strong>: An agent learns the optimal policy by interacting with an environment and receiving rewards or penalties. It is often combined with supervised learning, as in AlphaGo.</li>
</ul>
<hr>
<h2>7. Selection Guide &amp; Practical Tips</h2>
<ol>
<li><p><strong>Start with Your Data and Labels</strong>:</p>
<ul>
<li><strong>Have high-quality labels?</strong> Go with supervised learning.</li>
<li><strong>Labeling is expensive?</strong> Prioritize unsupervised learning for data exploration (clustering, visualization) or use self-supervised/semi-supervised methods to reduce label dependency.</li>
</ul>
</li>
<li><p><strong>Consider Model Scale and Data Complexity</strong>:</p>
<ul>
<li><strong>Large-scale perceptual tasks (images, speech, text)?</strong> Deep learning networks are the best choice.</li>
<li><strong>Small, high-dimensional datasets?</strong> SVMs or tree-based models (like Random Forest) might perform better.</li>
<li><strong>Structured/tabular data?</strong> XGBoost/LightGBM are often the performance kings.</li>
</ul>
</li>
<li><p><strong>Balance Interpretability and Accuracy</strong>:</p>
<ul>
<li><strong>High-stakes or regulated scenarios (finance, healthcare)?</strong> Linear models, logistic regression, or decision trees are favored for their interpretability.</li>
<li><strong>Scenarios where performance is paramount (online ads, recommendations)?</strong> More accurate, complex models (like deep networks) are preferred.</li>
</ul>
</li>
<li><p><strong>Combine Offline Exploration with Online Application</strong>:</p>
<ul>
<li>A common pattern is to first use <strong>unsupervised learning</strong> for exploratory analysis on offline data to discover potential user segments or data patterns. Then, use these findings as features or targets to build a <strong>supervised learning</strong> model for real-time prediction online.</li>
</ul>
</li>
</ol>
<hr>
<h2>8. Conclusion</h2>
<p>Supervised and unsupervised learning are two powerful tools for solving different kinds of problems, with the core distinction being their reliance on &quot;ground truth&quot; answers.</p>
<ul>
<li><strong>Supervised learning excels at &quot;answering known questions.&quot;</strong> Driven by clear goals and high-quality labels, it can make accurate predictions.</li>
<li><strong>Unsupervised learning excels at &quot;discovering unknown questions.&quot;</strong> Without prior knowledge, it can reveal hidden structures, patterns, and insights in the data.</li>
</ul>
<p>In real-world machine learning projects, the two are often not isolated. The most powerful solutions frequently combine them: <strong>first exploring the possibilities in the data with unsupervised learning, then building a precise model for a specific goal with supervised learning, creating a complete cycle from data insight to value creation.</strong></p>
<hr>
<h2>9. Code Examples</h2>
<h3>9.1 Environment Setup</h3>
<pre><code class="language-bash"><span class="line">pip install scikit-learn matplotlib torch torchvision</span></code></pre>
<h3>9.2 Supervised Learning Examples</h3>
<h4>Linear Regression (California Housing)</h4>
<pre><code class="language-python"><span class="line">from sklearn.datasets import fetch_california_housing</span>
<span class="line">from sklearn.model_selection import train_test_split</span>
<span class="line">from sklearn.linear_model import LinearRegression</span>
<span class="line">from sklearn.metrics import mean_squared_error</span>
<span class="line"></span>
<span class="line"># California Housing dataset</span>
<span class="line">X, y = fetch_california_housing(return_X_y=True)</span>
<span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span>
<span class="line"></span>
<span class="line">model = LinearRegression().fit(X_train, y_train)</span>
<span class="line">pred = model.predict(X_test)</span>
<span class="line">print(f&quot;RMSE on California Housing: {mean_squared_error(y_test, pred, squared=False):.2f}&quot;)</span></code></pre>
<h4>Logistic Regression (Breast Cancer Binary Classification)</h4>
<pre><code class="language-python"><span class="line">from sklearn.datasets import load_breast_cancer</span>
<span class="line">from sklearn.linear_model import LogisticRegression</span>
<span class="line">from sklearn.preprocessing import StandardScaler</span>
<span class="line"></span>
<span class="line">X, y = load_breast_cancer(return_X_y=True)</span>
<span class="line"># Scaling improves performance</span>
<span class="line">scaler = StandardScaler()</span>
<span class="line">X_scaled = scaler.fit_transform(X)</span>
<span class="line"></span>
<span class="line">clf = LogisticRegression(max_iter=1000).fit(X_scaled, y)</span>
<span class="line">print(f&quot;Accuracy on Breast Cancer: {clf.score(X_scaled, y):.3f}&quot;)</span></code></pre>
<h3>9.3 Unsupervised Learning Examples</h3>
<h4>K-means Clustering + Visualization</h4>
<pre><code class="language-python"><span class="line">from sklearn.datasets import load_iris</span>
<span class="line">from sklearn.cluster import KMeans</span>
<span class="line">import matplotlib.pyplot as plt</span>
<span class="line"></span>
<span class="line">X, y = load_iris(return_X_y=True) # y is used here only for comparison; K-means itself doesn&#39;t use it</span>
<span class="line">kmeans = KMeans(n_clusters=3, random_state=42, n_init=10).fit(X) # n_init=&#39;auto&#39; in future</span>
<span class="line"></span>
<span class="line"># Visualize the first two features</span>
<span class="line">plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap=&#39;viridis&#39;)</span>
<span class="line">plt.title(&#39;K-means Clustering on Iris Dataset&#39;)</span>
<span class="line">plt.xlabel(&#39;Sepal Length&#39;)</span>
<span class="line">plt.ylabel(&#39;Sepal Width&#39;)</span>
<span class="line">plt.show()</span></code></pre>
<h4>PCA + t-SNE Visualization</h4>
<pre><code class="language-python"><span class="line">from sklearn.decomposition import PCA</span>
<span class="line">from sklearn.manifold import TSNE</span>
<span class="line">import matplotlib.pyplot as plt</span>
<span class="line">from sklearn.datasets import load_iris</span>
<span class="line"></span>
<span class="line">X, y = load_iris(return_X_y=True)</span>
<span class="line"></span>
<span class="line"># First, reduce dimensions with PCA to a reasonable intermediate number</span>
<span class="line">X_reduced = PCA(n_components=50, random_state=42).fit_transform(X) if X.shape[1] &gt; 50 else X</span>
<span class="line"></span>
<span class="line"># Then, use t-SNE for non-linear dimensionality reduction for visualization</span>
<span class="line">X_embedded = TSNE(n_components=2, learning_rate=&#39;auto&#39;, init=&#39;pca&#39;, random_state=42).fit_transform(X_reduced)</span>
<span class="line"></span>
<span class="line">plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, cmap=&#39;viridis&#39;) # Color by true labels to verify</span>
<span class="line">plt.title(&#39;t-SNE Visualization of Iris Dataset&#39;)</span>
<span class="line">plt.xlabel(&#39;t-SNE feature 1&#39;)</span>
<span class="line">plt.ylabel(&#39;t-SNE feature 2&#39;)</span>
<span class="line">plt.show()</span></code></pre>
<h3>9.4 Simple GAN Skeleton (PyTorch)</h3>
<p>This is a minimal GAN structure to demonstrate its core components, not a complete training script.</p>
<pre><code class="language-python"><span class="line">import torch</span>
<span class="line">from torch import nn</span>
<span class="line"></span>
<span class="line"># Define the Generator</span>
<span class="line">class Generator(nn.Module):</span>
<span class="line">    def __init__(self, z_dim=100, img_dim=784):</span>
<span class="line">        super().__init__()</span>
<span class="line">        self.net = nn.Sequential(</span>
<span class="line">            nn.Linear(z_dim, 256),</span>
<span class="line">            nn.ReLU(True),</span>
<span class="line">            nn.Linear(256, 512),</span>
<span class="line">            nn.ReLU(True),</span>
<span class="line">            nn.Linear(512, img_dim),</span>
<span class="line">            nn.Tanh()  # Normalize output to [-1, 1]</span>
<span class="line">        )</span>
<span class="line">    def forward(self, z):</span>
<span class="line">        return self.net(z)</span>
<span class="line"></span>
<span class="line"># Define the Discriminator</span>
<span class="line">class Discriminator(nn.Module):</span>
<span class="line">    def __init__(self, img_dim=784):</span>
<span class="line">        super().__init__()</span>
<span class="line">        self.net = nn.Sequential(</span>
<span class="line">            nn.Linear(img_dim, 512),</span>
<span class="line">            nn.LeakyReLU(0.2, inplace=True),</span>
<span class="line">            nn.Linear(512, 256),</span>
<span class="line">            nn.LeakyReLU(0.2, inplace=True),</span>
<span class="line">            nn.Linear(256, 1),</span>
<span class="line">            nn.Sigmoid() # Output a probability value [0, 1]</span>
<span class="line">        )</span>
<span class="line">    def forward(self, x):</span>
<span class="line">        return self.net(x)</span>
<span class="line"></span>
<span class="line"># Initialize models, optimizers, and loss function</span>
<span class="line">G = Generator()</span>
<span class="line">D = Discriminator()</span>
<span class="line">g_opt = torch.optim.Adam(G.parameters(), lr=2e-4)</span>
<span class="line">d_opt = torch.optim.Adam(D.parameters(), lr=2e-4)</span>
<span class="line">criterion = nn.BCELoss()</span>
<span class="line"></span>
<span class="line">print(&quot;GAN components initialized successfully.&quot;)</span></code></pre>
<hr>
<h2>10. References</h2>
<ul>
<li><em>Pattern Recognition and Machine Learning</em> — Christopher M. Bishop</li>
<li><em>Deep Learning</em> — Ian Goodfellow, Yoshua Bengio, and Aaron Courville</li>
<li><a href="https://scikit-learn.org/stable/documentation.html">Scikit-learn Official Documentation</a></li>
<li><a href="https://pytorch.org/docs/stable/index.html">PyTorch Official Documentation</a></li>
</ul>

    </article>
    </main>
</div>

<script src="https://cdn.jsdelivr.net/npm/prismjs@1/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-python.min.js"></script>
<script>
Prism.highlightAll();
document.querySelectorAll('article pre').forEach(pre => {
    const code = pre.querySelector('code');
    if (!code) return;
    const toolbar = document.createElement('div');
    toolbar.className = 'code-toolbar';
    const btn = document.createElement('button');
    btn.className = 'copy-btn';
    btn.textContent = 'Copy';
    btn.onclick = () => {
        navigator.clipboard.writeText(code.textContent).then(() => {
            btn.textContent = 'Copied!';
            btn.classList.add('copied');
            setTimeout(() => { btn.textContent = 'Copy'; btn.classList.remove('copied'); }, 2000);
        });
    };
    toolbar.appendChild(btn);
    pre.parentNode.insertBefore(toolbar, pre);
});
</script>
</body>
</html>