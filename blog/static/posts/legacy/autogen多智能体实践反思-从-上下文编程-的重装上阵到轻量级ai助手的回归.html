<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AutoGen多智能体实践反思：从"上下文编程"的重装上阵到轻量级AI助手的回归 | Yuxu Ge</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1/themes/prism-tomorrow.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <style>
        :root {
            --black: #111;
            --dark-grey: #444;
            --off-white: #f4f4f4;
            --vermilion: #C41E3A;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }

        html {
            font-size: 16px;
            scroll-behavior: smooth;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: var(--black);
            background-color: var(--off-white);
        }

        a { color: var(--vermilion); text-decoration: none; }
        a:hover { opacity: 0.75; }

        .layout { min-height: 100vh; }

        /* Sidebar */
        .sidebar {
            position: fixed; top: 0; left: 0;
            width: 320px; height: 100vh;
            background: var(--black); color: var(--off-white);
            padding: 3rem 2rem;
            display: flex; flex-direction: column; justify-content: space-between;
        }
        .sidebar-top { display: flex; flex-direction: column; align-items: center; text-align: center; }
        .avatar { display: block; width: 120px; height: 120px; border-radius: 50%; border: 3px solid var(--vermilion); margin-bottom: 1.5rem; overflow: hidden; transition: transform 0.2s ease; }
        .avatar:hover { opacity: 1; transform: scale(1.05); }
        .avatar img { width: 100%; height: 100%; object-fit: cover; }
        .name-block { margin-bottom: 0.5rem; }
        h1 { font-size: 1.8rem; font-weight: 700; letter-spacing: 1px; color: var(--off-white); }
        .title-role { font-size: 0.9rem; color: #888; margin-top: 0.25rem; font-weight: 400; }
        .contact-block { margin-top: 2.5rem; width: 100%; }
        .social-links { display: flex; flex-direction: column; align-items: center; gap: 0.75rem; }
        .social-links a { display: flex; align-items: center; gap: 0.75rem; width: 160px; color: var(--off-white); opacity: 0.7; padding: 0.5rem 0.75rem; border-radius: 4px; font-size: 0.9rem; transition: all 0.2s; }
        .social-links a:hover { color: var(--vermilion); opacity: 1; background: rgba(255,255,255,0.05); }
        .social-links svg { width: 20px; height: 20px; flex-shrink: 0; }
        .social-links .orcid-link { font-size: 0.68rem; gap: 0.5rem; white-space: nowrap; }
        .sidebar-footer { text-align: center; font-size: 0.75rem; color: #555; }

        /* Content */
        .content { margin-left: 320px; padding: 3rem 4rem; max-width: 900px; }
        .back-link { display: inline-block; font-size: 0.85rem; margin-bottom: 2rem; color: var(--dark-grey); }
        .back-link:hover { color: var(--vermilion); }

        /* Article */
        article h1 { font-size: 2rem; font-weight: 700; margin-bottom: 1.5rem; line-height: 1.3; color: var(--black); }
        article h2 { font-size: 1.4rem; font-weight: 600; margin: 2rem 0 1rem; color: var(--black); }
        article h3 { font-size: 1.1rem; font-weight: 600; margin: 1.5rem 0 0.75rem; color: var(--black); }
        article p { margin-bottom: 1rem; color: var(--dark-grey); }
        article ul, article ol { margin: 1rem 0 1rem 1.5rem; color: var(--dark-grey); }
        article li { margin-bottom: 0.5rem; }
        article strong { color: var(--black); }
        article code { font-family: "SF Mono", Monaco, monospace; font-size: 0.9em; background: #e8e8e8; padding: 0.15em 0.4em; border-radius: 3px; }
        article pre { margin: 1rem 0; border-radius: 6px; overflow-x: auto; background: #2d2d2d; padding: 1rem; }
        article pre code { background: none; padding: 0; font-size: 0.75em; white-space: pre !important; counter-reset: line; display: block; color: #ccc; }
        article pre code .line { counter-increment: line; }
        article pre code .line::before { content: counter(line); display: inline-block; width: 2.5em; margin-right: 1em; text-align: right; color: #666; user-select: none; }
        .code-toolbar { display: flex; justify-content: flex-start; padding: 0.35rem 0.5rem; background: #3a3a3a; border-radius: 6px 6px 0 0; }
        .code-toolbar + pre { margin-top: 0; border-radius: 0 0 6px 6px; }
        .copy-btn { padding: 0.2rem 0.5rem; font-size: 0.7rem; background: #555; color: #fff; border: none; border-radius: 3px; cursor: pointer; transition: all 0.2s; }
        .copy-btn:hover, .copy-btn:active { background: #777; }
        .copy-btn.copied { background: #2a2; }
        article blockquote { border-left: 3px solid var(--vermilion); padding-left: 1rem; margin: 1rem 0; color: #666; font-style: italic; }
        article hr { border: none; border-top: 1px solid #ddd; margin: 2rem 0; }
        article table { width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem; }
        article th, article td { border: 1px solid #ddd; padding: 0.5rem 0.75rem; text-align: left; }
        article th { background: #e8e8e8; font-weight: 600; }
        article img { max-width: 100%; height: auto; display: block; margin: 1.5rem 0; border-radius: 6px; }

        .loading { text-align: center; padding: 3rem; color: #888; }
        .error { color: var(--vermilion); }

        /* KaTeX math styles */
        .katex-display { overflow-x: auto; overflow-y: hidden; padding: 0.5rem 0; }
        .katex { font-size: 1.1em; }

        /* Responsive */
        @media (max-width: 900px) {
            .sidebar { position: relative; width: 100%; height: auto; padding: 2rem 1.5rem 1rem; }
            .social-links { flex-direction: row; flex-wrap: wrap; justify-content: center; }
            .social-links a, .social-links .orcid-link { width: auto; padding: 0.5rem; font-size: 0; gap: 0; }
            .social-links svg { width: 24px; height: 24px; }
            .social-links .hide-mobile { display: none; }
            .sidebar-footer { margin-top: 1rem; }
            .content { margin-left: 0; padding: 2rem 1rem; }
        }
        @media (max-width: 480px) {
            .sidebar { padding: 1.5rem 1rem 1rem; }
            .avatar { width: 80px; height: 80px; }
            h1 { font-size: 1.4rem; }
            article h1 { font-size: 1.5rem; }
        }
    </style>
</head>
<body>

<div class="layout">
    <aside class="sidebar" id="sidebar-content"></aside>
    <script src="/components/sidebar.js"></script>

    <main class="content">
        <a href="/blog/" class="back-link">← Back to Blog</a>
        <article>
<hr>
<h2>date: 2025-07-24
tags: [ai]
legacy: true</h2>
<h1>AutoGen多智能体实践反思：从&quot;上下文编程&quot;的重装上阵到轻量级AI助手的回归</h1>
<p>我的核心目标是解决当前AI代码助手的一个痛点：它们通常是&quot;一次性&quot;的，缺乏对代码质量和后续优化的持续关注。我希望我的系统能模拟一个微型的开发小组。</p>
<h3>1. 系统设计：三位一体的AI开发团队</h3>
<p>我设计了三个高度专业化的智能体：</p>
<ol>
<li><strong>CoderAgent (编码工程师):</strong> 负责根据用户需求生成初始的Python代码。它的核心职责是快速实现功能。</li>
<li><strong>QualityAnalyzerAgent (质量分析师):</strong> 负责审查<code>CoderAgent</code>生成的代码。它会使用静态分析工具（如<code>pylint</code>）检查代码的风格、潜在错误和不规范的写法，并提出具体的修改建议。</li>
<li><strong>OptimizerAgent (性能优化师):</strong> 在代码功能正确、质量达标后，它会从更高层面审视代码，提出关于算法效率、代码结构、可读性等方面的优化建议。</li>
</ol>
<p>为了让这三个智能体能&quot;智能地&quot;协同工作，我选择了AutoGen中强大的 <code>GroupChat</code> 模式，并特别使用了<code>SelectorGroupChat</code>，期望它能像一个项目经理，根据当前的对话上下文，自动选择最合适的智能体发言。</p>
<h3>2. 技术实现：用AutoGen组建团队</h3>
<p>以下是系统设置的核心代码片段：</p>
<pre><code class="language-python"><span class="line">import autogen</span>
<span class="line"></span>
<span class="line"># 配置LLM</span>
<span class="line">config_list = autogen.config_list_from_json(...) </span>
<span class="line">llm_config = {&quot;config_list&quot;: config_list}</span>
<span class="line"></span>
<span class="line"># 1. 定义智能体</span>
<span class="line">coder = autogen.AssistantAgent(</span>
<span class="line">    name=&quot;CoderAgent&quot;,</span>
<span class="line">    system_message=&quot;You are a helpful AI assistant that writes Python code to solve tasks. Return the code in a markdown code block.&quot;,</span>
<span class="line">    llm_config=llm_config,</span>
<span class="line">)</span>
<span class="line"></span>
<span class="line">quality_analyzer = autogen.AssistantAgent(</span>
<span class="line">    name=&quot;QualityAnalyzerAgent&quot;,</span>
<span class="line">    system_message=&quot;You are a quality assurance expert. You review the given Python code for style, errors, and best practices. Suggest specific improvements.&quot;,</span>
<span class="line">    llm_config=llm_config,</span>
<span class="line">)</span>
<span class="line"></span>
<span class="line">optimizer = autogen.AssistantAgent(</span>
<span class="line">    name=&quot;OptimizerAgent&quot;,</span>
<span class="line">    system_message=&quot;You are a performance optimization expert. You analyze the Python code for performance bottlenecks and suggest refactoring for better efficiency and readability.&quot;,</span>
<span class="line">    llm_config=llm_config,</span>
<span class="line">)</span>
<span class="line"></span>
<span class="line">user_proxy = autogen.UserProxyAgent(</span>
<span class="line">    name=&quot;UserProxy&quot;,</span>
<span class="line">    human_input_mode=&quot;TERMINATE&quot;,</span>
<span class="line">    code_execution_config={&quot;work_dir&quot;: &quot;coding&quot;},</span>
<span class="line">)</span>
<span class="line"></span>
<span class="line"># 2. 设置SelectorGroupChat</span>
<span class="line"># 使用 &quot;auto&quot; 模式，让LLM来决定下一个发言者</span>
<span class="line">groupchat = autogen.GroupChat(</span>
<span class="line">    agents=[user_proxy, coder, quality_analyzer, optimizer],</span>
<span class="line">    messages=[],</span>
<span class="line">    max_round=15,</span>
<span class="line">    speaker_selection_method=&quot;auto&quot; </span>
<span class="line">)</span>
<span class="line"></span>
<span class="line">manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)</span>
<span class="line"></span>
<span class="line"># 3. 启动任务</span>
<span class="line">user_proxy.initiate_chat(</span>
<span class="line">    manager,</span>
<span class="line">    message=&quot;Write a Python function to find the nth Fibonacci number, then analyze and optimize it.&quot;</span>
<span class="line">)</span></code></pre>
<p>在<code>speaker_selection_method=&quot;auto&quot;</code>的设置下，我期待的理想工作流是：<code>UserProxy</code> -&gt; <code>CoderAgent</code> -&gt; <code>QualityAnalyzerAgent</code> -&gt; <code>OptimizerAgent</code> -&gt; <code>UserProxy</code>。看起来很完美，不是吗？然而，现实很快给了我沉重一击。</p>
<h2>二、实践中的&quot;重&quot;：当理想照进现实</h2>
<p>系统跑起来后，我很快就感受到了那种挥之不去的&quot;沉重感&quot;。这种感觉并非来自单一问题，而是多个因素叠加的结果。</p>
<h3>1. 交互延迟与效率黑洞</h3>
<p>对于一个简单的斐波那契函数，整个流程下来耗时数分钟。每一次智能体之间的交接，都是一次完整的LLM调用。<code>SelectorGroupChat</code>为了决定下一个发言者，本身也需要一次LLM推理。这意味着，完成一个简单任务，背后可能有5-10次甚至更多的LLM调用。</p>
<p>在日常开发中，我需要的是秒级的代码补全和建议，而不是泡杯咖啡等待AI团队&quot;开会讨论&quot;的结果。这种高延迟对于高频、即时的开发辅助场景是致命的。</p>
<h3>2. 不可控的&quot;智能涌现&quot;</h3>
<p><code>speaker_selection_method=&quot;auto&quot;</code> 是一把双刃剑。它确实带来了&quot;智能&quot;，但也带来了混乱。我观察到了几种典型的问题：</p>
<ul>
<li><strong>对话循环：</strong> <code>CoderAgent</code> 和 <code>QualityAnalyzerAgent</code> 之间可能来回&quot;拉扯&quot;，一个修改，一个又挑出新问题，迟迟无法进入优化阶段。</li>
<li><strong>错误调度：</strong> 有时，在<code>CoderAgent</code>刚写完代码后，<code>OptimizerAgent</code>会&quot;抢话&quot;，跳过质量分析环节，直接开始谈优化，打乱了预设的流程。</li>
<li><strong>过早终止：</strong> 系统可能在没有充分优化的情况下，过早地将控制权交还给<code>UserProxy</code>，并认为任务已完成。</li>
</ul>
<p>这种不可预测性，让本应是提效工具的系统，变成了一个需要小心翼翼引导和观察的&quot;黑箱&quot;。</p>
<h3>3. 复杂的状态管理与上下文传递</h3>
<p>多智能体系统的核心挑战之一是状态管理。在这个实验中，&quot;状态&quot;就是那段正在被迭代的代码。理想情况下，<code>QualityAnalyzerAgent</code>应该基于<code>CoderAgent</code>的最新代码进行分析。</p>
<p>但<code>GroupChat</code>的状态是通过不断增长的消息历史来维护的。当对话轮次增多，上下文窗口会迅速膨胀，不仅增加了token成本，还可能因为信息过载导致后续的Agent&quot;注意力不集中&quot;，忽略了关键的代码版本或修改建议。我必须精心设计Prompt，反复提醒Agent&quot;请关注上一轮发言中的代码&quot;，这本身就是一种负担。</p>
<h3>4. 高昂的配置与调试成本</h3>
<p>构建这个系统，我花费了大量时间在&quot;元工作&quot;（meta-work）上：</p>
<ul>
<li><strong>Prompt Engineering:</strong> 为每个Agent编写精确的<code>system_message</code>，定义它们的角色、能力边界和沟通风格。</li>
<li><strong>流程设计：</strong> 思考如何设计终止条件、如何引导对话流向。</li>
<li><strong>调试：</strong> 当系统行为不符合预期时，我需要通读整个对话历史，猜测是哪个Agent的Prompt出了问题，还是<code>Selector</code>的决策逻辑有误。这种调试难度远高于传统代码。</li>
</ul>
<p>这些前期投入和后期维护的成本，对于解决一个&quot;写斐波那契函数&quot;级别的问题来说，显然是不成比例的。</p>
<h2>三、反思：什么场景真正需要&quot;重装上阵&quot;？</h2>
<p>这次失败的尝试并非毫无价值，它让我更深刻地理解了多智能体系统的本质和适用边界。</p>
<p><strong>多智能体系统的核心优势在于：</strong></p>
<ul>
<li><strong>专业分工与模块化：</strong> 能将一个庞大、模糊的任务分解给不同领域的&quot;专家&quot;，实现关注点分离。</li>
<li><strong>模拟复杂工作流：</strong> 非常适合模拟真实世界中需要多角色协作的流程，如产品研发、科学研究等。</li>
<li><strong>&quot;涌现&quot;与创造力：</strong> Agent间的自由讨论有时能碰撞出意想不到的、富有创造力的解决方案。</li>
</ul>
<p><strong>那么，什么场景适合使用这种&quot;重量级&quot;的系统？</strong></p>
<ol>
<li><strong>探索性与研究性任务：</strong> 例如，&quot;调研自动驾驶技术的最新进展，并生成一份包含技术摘要、主要玩家和未来趋势的分析报告&quot;。这类任务没有固定流程，需要信息搜集、整合、分析等多个复杂步骤，且对最终结果的创造性有一定要求。</li>
<li><strong>端到端的自动化项目：</strong> 例如，&quot;根据用户需求文档，自动生成项目框架、编写核心代码、配置部署脚本&quot;。这类任务周期长、步骤多、异步执行，多智能体系统可以像一个自主项目团队一样，在后台默默推进。</li>
<li><strong>复杂决策与模拟：</strong> 例如，模拟一个市场环境，让&quot;消费者Agent&quot;、&quot;竞争对手Agent&quot;和&quot;营销Agent&quot;互动，以预测某个营销策略的效果。</li>
</ol>
<p><strong>而对于以下场景，我们应该果断选择&quot;轻装简行&quot;：</strong></p>
<ul>
<li><strong>高频次的实时交互任务：</strong> 如代码补全、实时问答、文本润色。</li>
<li><strong>流程确定、线性的任务：</strong> 如果一个任务可以被清晰地分解为A-&gt;B-&gt;C的步骤，那么强制使用自由讨论式的<code>GroupChat</code>就是杀鸡用牛刀。</li>
<li><strong>对延迟和成本极其敏感的场景。</strong></li>
</ul>
<h2>四、回归简单：轻量级AI助手的构建思路</h2>
<p>既然重量级的多智能体系统不适合我的日常开发需求，那么什么是更好的替代方案？答案是回归简单，利用AutoGen提供的其他模式，或者转变思路。</p>
<h3>方案一：两阶段智能体流水线 (Sequential Pipeline)</h3>
<p>如果你的流程是确定的，比如&quot;先编码，后审查&quot;，那么完全可以用有序的方式组织Agent。AutoGen的<code>register_nested_chats</code>功能非常适合这个场景。</p>
<pre><code class="language-python"><span class="line"># 这是一个概念性示例，演示如何构建一个有序的流水线</span>
<span class="line"># CoderAgent完成任务后，其结果会自动作为QualityAnalyzerAgent的输入</span>
<span class="line"></span>
<span class="line"># 假设已经定义了 CoderAgent 和 QualityAnalyzerAgent</span>
<span class="line"></span>
<span class="line"># 嵌套聊天设置</span>
<span class="line">review_chat = autogen.GroupChat(</span>
<span class="line">    agents=[quality_analyzer, user_proxy],</span>
<span class="line">    messages=[],</span>
<span class="line">    max_round=2,</span>
<span class="line">    speaker_selection_method=&quot;manual&quot; # 或者其他可控方式</span>
<span class="line">)</span>
<span class="line"></span>
<span class="line"># 注册嵌套聊天，形成流水线</span>
<span class="line">coder.register_nested_chats(</span>
<span class="line">    [{&quot;recipient&quot;: quality_analyzer, &quot;message&quot;: &quot;Please review the following code.&quot;, &quot;summary_method&quot;: &quot;last_msg&quot;}],</span>
<span class="line">    trigger=user_proxy,</span>
<span class="line">)</span>
<span class="line"></span>
<span class="line">user_proxy.initiate_chat(coder, message=&quot;Write a Python function for quick sort.&quot;)</span></code></pre>
<p>这种模式下，控制流是确定的 <code>User -&gt; Coder -&gt; QualityAnalyzer</code>。它保留了Agent专业分工的优点，但消除了<code>SelectorGroupChat</code>的不可预测性和高昂的协调成本。</p>
<h3>方案二：单智能体 + 工具 (Single Agent with Tools)</h3>
<p>这是目前业界在构建AI助手时更为主流和实用的范式，也与OpenAI的Function Calling/Tool Use一脉相承。</p>
<p><strong>核心思想是：<strong>与其创建多个Agent进行对话，不如创建一个&quot;全能&quot;的<code>AssistantAgent</code>，并将&quot;质量分析&quot;、&quot;代码优化&quot;等能力封装成它可以调用的</strong>工具</strong>。</p>
<pre><code class="language-python"><span class="line">import pylint.lint</span>
<span class="line">import io</span>
<span class="line">from pylint.reporters.text import TextReporter</span>
<span class="line"></span>
<span class="line"># 1. 定义工具函数</span>
<span class="line">def lint_code(code: str) -&gt; str:</span>
<span class="line">    &quot;&quot;&quot;Runs pylint on the given Python code and returns the report.&quot;&quot;&quot;</span>
<span class="line">    pylint_opts = [&#39;--disable=all&#39;, &#39;--enable=E,W&#39;]</span>
<span class="line">    reporter = TextReporter(io.StringIO())</span>
<span class="line">    pylint.lint.Run([io.StringIO(code)], reporter=reporter, exit=False, args=pylint_opts)</span>
<span class="line">    return reporter.out.getvalue()</span>
<span class="line"></span>
<span class="line"># 2. 创建一个具备工具调用能力的Agent</span>
<span class="line">super_assistant = autogen.AssistantAgent(</span>
<span class="line">    name=&quot;SuperAssistant&quot;,</span>
<span class="line">    system_message=&quot;You are a super-assistant for Python development. You can write code and use tools to check its quality.&quot;,</span>
<span class="line">    llm_config=llm_config,</span>
<span class="line">)</span>
<span class="line"></span>
<span class="line"># 3. 创建UserProxyAgent并注册工具</span>
<span class="line">user_proxy = autogen.UserProxyAgent(</span>
<span class="line">    name=&quot;UserProxy&quot;,</span>
<span class="line">    human_input_mode=&quot;TERMINATE&quot;,</span>
<span class="line">    code_execution_config=False, # 我们不执行代码，只调用工具</span>
<span class="line">)</span>
<span class="line"></span>
<span class="line">user_proxy.register_function(</span>
<span class="line">    function_map={</span>
<span class="line">        &quot;lint_code&quot;: lint_code</span>
<span class="line">    }</span>
<span class="line">)</span>
<span class="line"></span>
<span class="line"># 4. 让Agent使用工具</span>
<span class="line"># 在LLM的Prompt中，它会被告知有lint_code这个工具可用</span>
<span class="line"># LLM会决定在合适的时机生成调用该工具的请求</span></code></pre>
<p><strong>这种模式的优势是压倒性的：</strong></p>
<ul>
<li><strong>低延迟：</strong> 没有多Agent间的通信开销。</li>
<li><strong>高可控：</strong> 流程由LLM调用工具的决策驱动，比Agent间的自由对话更可预测。</li>
<li><strong>易于扩展和维护：</strong> 增加新能力只需添加新的工具函数，而不是设计一个新的Agent和复杂的交互逻辑。</li>
</ul>
<h2>结论：在复杂性与实用性之间寻找平衡</h2>
<p>我这次从雄心勃勃到回归务实的旅程，让我深刻体会到：<strong>技术选型的第一原则永远是&quot;恰如其分&quot;</strong>。多智能体系统是一个强大而迷人的范式，但它不是解决所有问题的银弹。为了追求&quot;看起来很酷&quot;的架构而忽视了真实场景下的效率、成本和可控性，是典型的技术自嗨。</p>
<p>对于AI应用的构建者而言，我们的目标不应是构建最复杂的系统，而是构建最能解决问题的系统。在AutoGen这样的强大框架中，<code>GroupChat</code>只是众多工具之一。学会根据任务的性质，在&quot;多智能体协作&quot;、&quot;有序流水线&quot;和&quot;单智能体+工具&quot;之间做出明智的选择，才是一名成熟AI工程师的标志。</p>
<p>未来，人与AI的协作、AI与AI的协作，必将更加深入。而我们的任务，就是在不断涌现的新技术中，保持清醒的头脑，找到那个连接技术与价值的最佳平衡点。</p>

    </article>
    </main>
</div>

<script src="https://cdn.jsdelivr.net/npm/prismjs@1/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-python.min.js"></script>
<script>
Prism.highlightAll();
document.querySelectorAll('article pre').forEach(pre => {
    const code = pre.querySelector('code');
    if (!code) return;
    const toolbar = document.createElement('div');
    toolbar.className = 'code-toolbar';
    const btn = document.createElement('button');
    btn.className = 'copy-btn';
    btn.textContent = 'Copy';
    btn.onclick = () => {
        navigator.clipboard.writeText(code.textContent).then(() => {
            btn.textContent = 'Copied!';
            btn.classList.add('copied');
            setTimeout(() => { btn.textContent = 'Copy'; btn.classList.remove('copied'); }, 2000);
        });
    };
    toolbar.appendChild(btn);
    pre.parentNode.insertBefore(toolbar, pre);
});
</script>
</body>
</html>