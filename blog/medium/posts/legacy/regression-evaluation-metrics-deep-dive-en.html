<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Deep Dive into Regression Evaluation Metrics: From MAE to R², Choosing the Right Tool for the Job</title>
    <style>
        body { font-family: Georgia, serif; max-width: 700px; margin: 2rem auto; padding: 0 1rem; line-height: 1.7; color: #333; }
        h1 { font-size: 2rem; margin-bottom: 0.5rem; }
        h2 { font-size: 1.4rem; margin-top: 2rem; }
        pre { background: #f4f4f4; padding: 1rem; overflow-x: auto; border-radius: 4px; }
        code { font-family: Menlo, Monaco, monospace; font-size: 0.9em; }
        p code { background: #f4f4f4; padding: 0.2em 0.4em; border-radius: 3px; }
        img { max-width: 100%; }
        blockquote { border-left: 3px solid #ddd; margin-left: 0; padding-left: 1rem; color: #666; }
        ul { padding-left: 1.5rem; }
        hr { border: none; border-top: 1px solid #ddd; margin: 2rem 0; }
        a { color: #1a8917; }
    </style>
</head>
<body>
<hr>
<h2>date: 2024-01-01
tags: [ai, machine learning, data science, regression, python]
legacy: true</h2>
<h1>A Deep Dive into Regression Evaluation Metrics: From MAE to R², Choosing the Right Tool for the Job</h1>
<p><img src="/blog/images/tables/table-1fbab8e1.png" alt="Table"></p><h3>2. Other Common Metrics</h3>
<p>Beyond the core metrics, the following are extremely useful in specific scenarios.</p>
<p><img src="/blog/images/tables/table-306a4c50.png" alt="Table"></p><h3>3. Quick Calculation with Python</h3>
<p><code>scikit-learn</code> provides convenient tools to calculate most of these metrics. Here is a template you can quickly adapt for your projects.</p>
<blockquote><code>import numpy as np</code><br>
<code>from sklearn.metrics import (mean_absolute_error, mean_squared_error,</code><br>
<code>                             median_absolute_error, r2_score,</code><br>
<code>                             mean_absolute_percentage_error,</code><br>
<code>                             explained_variance_score, max_error)</code><br>
<code></code><br>
<code># Assuming y_true and y_pred are your ground truth and predictions</code><br>
<code>rng = np.random.RandomState(42)</code><br>
<code># ... (19 more lines)</code></blockquote>
<p><em>Full code available in the <a href="https://github.com/geyuxu">GitHub repository</a>.</em></p><p><strong>Example Output</strong> (may vary slightly with each run):</p>
<blockquote><code>--- Regression Metrics ---</code><br>
<code>MAE         : 8.7285</code><br>
<code>MSE         : 105.1905</code><br>
<code>RMSE        : 10.2554</code><br>
<code>MedAE       : 7.6170</code><br>
<code>MAPE(%)     : 7.8522</code><br>
<code>R2          : 0.8925</code><br>
<code>ExplainedVar: 0.8949</code><br>
<code>MaxError    : 28.1245</code></blockquote><h3>4. How Do Outliers Affect Metrics? A Small Experiment</h3>
<p>To intuitively understand the sensitivity of different metrics to outliers, let&#39;s conduct a simple experiment.</p>
<blockquote><code>import numpy as np</code><br>
<code>from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score</code><br>
<code></code><br>
<code># 1) Create a clean dataset</code><br>
<code>y_true_clean = np.linspace(10, 20, 50)</code><br>
<code>y_pred_clean = y_true_clean + np.random.normal(0, 0.8, size=len(y_true_clean))</code><br>
<code></code><br>
<code>def report(title, y_true, y_pred):</code><br>
<code># ... (12 more lines)</code></blockquote>
<p><em>Full code available in the <a href="https://github.com/geyuxu">GitHub repository</a>.</em></p><p><strong>Typical Output</strong>:</p>
<blockquote><code>Without Outlier     MSE= 0.631  MAE=0.642  R2=0.985</code><br>
<code>With Outlier        MSE=82.641  MAE=1.624  R2=-0.250</code></blockquote><p><strong>Interpreting the Experiment</strong>:</p>
<ul>
<li><strong>MSE</strong> skyrockets from 0.631 to 82.641. It&#39;s &quot;detonated&quot; by a single outlier because squaring the error massively amplifies its impact.</li>
<li><strong>MAE</strong> increases gently from 0.642 to 1.624, demonstrating its robustness against outliers.</li>
<li><strong>R²</strong> drops to a negative value. This means that after adding the outlier, the model&#39;s performance is worse than the baseline model of simply predicting the mean of all true values.</li>
</ul>
<p>This experiment shows that when your business cannot tolerate large errors, MSE/RMSE serves as a more sensitive sentinel. However, if you want to assess overall model performance without being skewed by a few anomalies, MAE/MedAE is a more robust choice.</p>
<h3>5. How to Choose the Right Metric for Your Project: A Scenario-Based Guide</h3>
<p>Choosing the right metric begins with understanding your business needs and data characteristics. Here, we provide a scenario-based guide by directly answering the key questions posed earlier.</p>
<h4>Scenario 1: Are we more afraid of being &quot;off by 1° on average&quot; or &quot;off by 5° occasionally&quot;?</h4>
<p>This question gets to the heart of your model&#39;s error tolerance, especially its sensitivity to large errors.</p>
<ul>
<li><p><strong>If you&#39;re more afraid of being &quot;off by 5° occasionally&quot; (High-Risk Aversion):</strong></p>
<ul>
<li><strong>Primary Metrics: MSE / RMSE</strong></li>
<li><strong>Reasoning</strong>: Mean Squared Error (MSE) and its square root (RMSE) square the error term. This means a 5° error ($5^2=25$) contributes 25 times more to the total error than a 1° error ($1^2=1$). This makes MSE/RMSE extremely sensitive to large errors and outliers, acting like an alarm system that penalizes any significant deviation harshly.</li>
<li><strong>Applicable Industries</strong>: Financial risk management (e.g., predicting default losses), industrial manufacturing (e.g., predicting equipment failure times), and weather forecasting. In these fields, a single extreme error can be very costly.</li>
</ul>
</li>
<li><p><strong>If you&#39;re more concerned with being &quot;off by 1° on average&quot; (Stability Priority):</strong></p>
<ul>
<li><strong>Primary Metrics: MAE / MedAE</strong></li>
<li><strong>Reasoning</strong>: Mean Absolute Error (MAE) treats all errors equally, calculating their average linearly. A 5° error is simply 5 times worse than a 1° error. This allows MAE to provide a more robust measure of the model&#39;s general performance, without being skewed by a few extreme values. Median Absolute Error (MedAE) is even more robust, as it is completely insensitive to outliers.</li>
<li><strong>Applicable Industries</strong>: Retail sales forecasting, inventory management, and human resources planning. In these areas, stakeholders often care more about the overall, expected deviation rather than being misled by a few anomalous transactions.</li>
</ul>
</li>
</ul>
<h4>Scenario 2: Does management care about &quot;relative percentages&quot; or &quot;absolute values&quot;?</h4>
<p>This question is about the metric&#39;s audience and its ease of communication.</p>
<ul>
<li><p><strong>If your audience thinks in &quot;relative percentages&quot;:</strong></p>
<ul>
<li><strong>Primary Metrics: MAPE / SMAPE</strong></li>
<li><strong>Reasoning</strong>: Mean Absolute Percentage Error (MAPE) presents the error as a percentage, which is highly intuitive, especially for reporting to non-technical stakeholders (e.g., &quot;Our sales forecast is accurate to within 5%&quot;). It&#39;s also excellent for comparing performance across different scales, such as forecasting sales for a $10 product versus a $1,000 product.</li>
<li><strong>Note</strong>: If your true values can be zero, use Symmetric MAPE (SMAPE) or another metric to avoid division-by-zero errors.</li>
</ul>
</li>
<li><p><strong>If business decisions depend on &quot;absolute values&quot;:</strong></p>
<ul>
<li><strong>Primary Metrics: RMSE / MAE</strong></li>
<li><strong>Reasoning</strong>: The units of these metrics are the same as your target variable (e.g., the RMSE for a housing price model is in &quot;dollars&quot;; the MAE for inventory prediction is in &quot;units&quot;). This allows business teams to directly relate the model&#39;s error to financial metrics like cost and profit, enabling more concrete decisions (e.g., &quot;The model&#39;s average error is $1,000, which is within our acceptable range&quot;).</li>
</ul>
</li>
</ul>
<h4>Scenario 3: What are the characteristics of my data distribution?</h4>
<p>The nature of your data is a critical technical prerequisite for selecting a metric.</p>
<ul>
<li><p><strong>If your data has a long-tail distribution (e.g., housing prices, web traffic, personal income):</strong></p>
<ul>
<li><strong>Primary Metric: RMSLE (Root Mean Squared Log Error)</strong></li>
<li><strong>Reasoning</strong>: For this type of data, we often care more about relative errors than absolute ones. For example, predicting a $1M house as $1.1M (a $100k error) is different from predicting a $10M house as $10.1M (also a $100k error). RMSLE addresses this by taking the logarithm of the predictions and actual values, effectively turning absolute differences into relative ones. It also naturally penalizes underestimation more than overestimation.</li>
</ul>
</li>
<li><p><strong>If your data contains many important zero values:</strong></p>
<ul>
<li><strong>Primary Metrics: MAE / RMSE</strong></li>
<li><strong>Reasoning</strong>: As mentioned, MAPE fails when the actual value is zero. In this case, MAE or RMSE, which directly evaluate the absolute error, are the safest and most straightforward choices.</li>
</ul>
</li>
</ul>
<h4>Summary: A Decision-Making Flowchart</h4>
<ol>
<li><strong>Qualitative Analysis (Business Dialogue)</strong>: Start by talking to stakeholders to clarify error tolerance and reporting habits, addressing Scenarios 1 and 2.</li>
<li><strong>Quantitative Analysis (Data Exploration)</strong>: Plot a histogram of your data to check for long tails, outliers, or zero values, addressing Scenario 3.</li>
<li><strong>Monitor Multiple Metrics (Model Iteration)</strong>: During training and validation, track 2-3 key metrics simultaneously (e.g., a robust metric like MAE, a sensitive one like RMSE, and a business-friendly one like MAPE) to gain a comprehensive understanding of your model&#39;s behavior.</li>
<li><strong>Final Selection (Deployment)</strong>: Based on the analysis, choose 1-2 of the most critical metrics to serve as the final standard for model selection and online monitoring.</li>
</ol>
<h3>Conclusion</h3>
<p>There is no single &quot;best&quot; metric—only the &quot;most appropriate&quot; one for your specific context. A deep understanding of the mathematical principles and business intuition behind each metric is the bridge that connects your model to real-world value. We hope this guide helps you choose and interpret evaluation metrics with more confidence in your future regression projects, leading to more powerful and reliable models.</p>

</body>
</html>