{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713ca1de",
   "metadata": {},
   "source": [
    "# R_002 - Minimal Retrieval (TF-IDF)\n",
    "\n",
    "Goal: build a tiny retrieval pipeline (chunk → index → query → top-k evidence).  \n",
    "No LLM. Focus on reproducibility and evidence display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3955b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "# 如果你在 LABS/notebooks 里运行，repo_root 是 notebooks，需要上两级\n",
    "if (repo_root / \"LABS\").exists() is False:\n",
    "    repo_root = repo_root.parent.parent\n",
    "\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from LABS.src.text_chunker import chunk_text\n",
    "from LABS.src.retrieval_tfidf import TfidfRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a28397",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = {\n",
    "    \"doc_york_ai\": \"\"\"\n",
    "University of York offers an MSc in Artificial Intelligence covering machine learning,\n",
    "deep learning, and autonomous systems. Students often build projects using Python and PyTorch.\n",
    "\"\"\",\n",
    "    \"doc_rag\": \"\"\"\n",
    "Retrieval-Augmented Generation (RAG) improves factual grounding by retrieving relevant documents\n",
    "and injecting them into a model's context. Typical components include chunking, retrieval, reranking,\n",
    "and evaluation for faithfulness.\n",
    "\"\"\",\n",
    "    \"doc_transformer\": \"\"\"\n",
    "Transformers use self-attention to model token interactions. They scale well and are the backbone\n",
    "of modern large language models. Common variants include encoder-only, decoder-only, and encoder-decoder.\n",
    "\"\"\",\n",
    "    \"doc_eval\": \"\"\"\n",
    "Evaluating retrieval systems often uses metrics like precision@k, recall@k, MRR, and nDCG.\n",
    "For RAG, you also care about citation faithfulness and answer correctness on held-out queries.\n",
    "\"\"\".strip(),\n",
    "}\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe8ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "for doc_id, text in docs.items():\n",
    "    cs = chunk_text(text, chunk_size=220, overlap=40)\n",
    "    for i, c in enumerate(cs):\n",
    "        chunks.append((f\"{doc_id}::chunk{i:02d}\", c.text))\n",
    "\n",
    "len(chunks), chunks[0][0], chunks[0][1][:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec479805",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = TfidfRetriever(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c7d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(query: str, top_k: int = 5):\n",
    "    print(\"Query:\", query)\n",
    "    results = retriever.search(query, top_k=top_k)\n",
    "    for r in results:\n",
    "        print(f\"- {r.doc_id}  score={r.score:.3f}\")\n",
    "        print(\"  \", r.text.strip().replace(\"\\n\", \" \"))\n",
    "        print()\n",
    "\n",
    "show_results(\"What is RAG and why is chunking important?\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac6c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(\"Which metrics are used to evaluate retrieval systems?\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f40aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    (\"rag components chunking retrieval\", \"doc_rag\"),\n",
    "    (\"self attention transformer backbone\", \"doc_transformer\"),\n",
    "    (\"precision recall mrr ndcg evaluation\", \"doc_eval\"),\n",
    "]\n",
    "\n",
    "def recall_at_k(k: int = 3) -> float:\n",
    "    hit = 0\n",
    "    for q, target_prefix in queries:\n",
    "        res = retriever.search(q, top_k=k)\n",
    "        ok = any(r.doc_id.startswith(target_prefix) for r in res)\n",
    "        hit += int(ok)\n",
    "    return hit / len(queries)\n",
    "\n",
    "for k in [1, 3, 5]:\n",
    "    print(\"recall@\", k, \"=\", recall_at_k(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca3831a",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- This is a minimal baseline (TF-IDF cosine).\n",
    "- Next step: add a tiny evaluation set + compute recall@k / MRR.\n",
    "- Then: swap retriever to BM25/hybrid or add simple reranking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
