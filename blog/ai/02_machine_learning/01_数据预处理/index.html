<!DOCTYPE html><html lang="zh" data-astro-cid-bvzihdzo> <head><!-- Global Metadata --><meta charset="utf-8"><!--<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">--><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"><link rel="shortcut icon" href="favicon.png" type="image/png"><link rel="sitemap" href="/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" title="Ge Yuxu • AI &#38; Engineering" href="https://geyuxu.com/rss.xml"><meta name="generator" content="Astro v5.7.5"><!-- Font preloads --><link rel="preload" href="/fonts/atkinson-regular.woff" as="font" type="font/woff" crossorigin><link rel="preload" href="/fonts/atkinson-bold.woff" as="font" type="font/woff" crossorigin><!-- Canonical URL --><link rel="canonical" href="https://geyuxu.com/blog/ai/02_machine_learning/01_%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"><!-- Primary Meta Tags --><title>机器学习中的数据预处理</title><meta name="title" content="机器学习中的数据预处理"><meta name="description"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://geyuxu.com/blog/ai/02_machine_learning/01_%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"><meta property="og:title" content="机器学习中的数据预处理"><meta property="og:description"><meta property="og:image" content="https://geyuxu.com/blog-placeholder-1.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://geyuxu.com/blog/ai/02_machine_learning/01_%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"><meta property="twitter:title" content="机器学习中的数据预处理"><meta property="twitter:description"><meta property="twitter:image" content="https://geyuxu.com/blog-placeholder-1.jpg"><script src="/js/jquery-3.7.1.min.js"></script><script is:global>
	window.addEventListener('DOMContentLoaded', () => {
		$('.toc ol').css({
		'list-style': 'none',   // 隐藏 1. 2. 3.
		'margin': 0,
		'padding-left': 0,       // 可按需调整
		});
		$('.toc ol > li').css({
		'list-style': 'none',   // 隐藏 1. 2. 3.
		'padding-left': 10  
		});
        $('.sidebar').append($('.toc'));
      });
	</script><style>main[data-astro-cid-bvzihdzo].page{display:grid;grid-template-columns:260px minmax(0,1fr);width:100%;margin:0}aside[data-astro-cid-bvzihdzo].sidebar{box-sizing:border-box;width:260px;padding:2rem 1rem;font-size:.95rem;position:sticky;top:4rem;align-self:start}.sidebar[data-astro-cid-bvzihdzo] .meta[data-astro-cid-bvzihdzo] p[data-astro-cid-bvzihdzo]{margin:.25rem 0}nav[data-astro-cid-bvzihdzo].toc li[data-astro-cid-bvzihdzo]{margin:.35rem 0 .35rem 1rem}nav[data-astro-cid-bvzihdzo].toc a[data-astro-cid-bvzihdzo]{color:var(--gray-dark,#444);text-decoration:none}nav[data-astro-cid-bvzihdzo].toc a[data-astro-cid-bvzihdzo]:hover{text-decoration:underline}.content-wrapper[data-astro-cid-bvzihdzo]{display:flex;justify-content:center;padding:2rem 1rem}article[data-astro-cid-bvzihdzo].prose{max-width:740px;width:100%}@media (max-width: 768px){main[data-astro-cid-bvzihdzo].page{grid-template-columns:1fr}aside[data-astro-cid-bvzihdzo].sidebar{position:static;width:100%;padding:1rem}.content-wrapper[data-astro-cid-bvzihdzo]{justify-content:flex-start}article[data-astro-cid-bvzihdzo].prose{max-width:100%}}.series-list[data-astro-cid-bvzihdzo]{list-style:none;margin:0;padding-left:10px}.series-list[data-astro-cid-bvzihdzo]{list-style:none;margin:0;padding-left:10px;max-height:calc(16em + .5rem);overflow-y:auto}.series-list[data-astro-cid-bvzihdzo]::-webkit-scrollbar{width:6px}.series-list[data-astro-cid-bvzihdzo]::-webkit-scrollbar-thumb{background:#0003;border-radius:3px}.content[data-astro-cid-7jjqptxk]{max-width:720px;margin:0 auto;padding:2rem 1rem;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,sans-serif;font-size:1.05rem;line-height:1.75;color:#333}.content[data-astro-cid-7jjqptxk] h1[data-astro-cid-7jjqptxk],.content[data-astro-cid-7jjqptxk] h2[data-astro-cid-7jjqptxk],.content[data-astro-cid-7jjqptxk] h3[data-astro-cid-7jjqptxk]{font-weight:600;margin-top:2rem;margin-bottom:1rem;line-height:1.3}.content[data-astro-cid-7jjqptxk] p[data-astro-cid-7jjqptxk]{margin-bottom:1.25rem}.content[data-astro-cid-7jjqptxk] a[data-astro-cid-7jjqptxk]{color:var(--accent, #0070f3);text-decoration:underline}.content[data-astro-cid-7jjqptxk] img[data-astro-cid-7jjqptxk]{max-width:100%;border-radius:6px;margin:1.5rem 0}.content[data-astro-cid-7jjqptxk] pre[data-astro-cid-7jjqptxk],.content[data-astro-cid-7jjqptxk] code[data-astro-cid-7jjqptxk]{font-family:Menlo,Monaco,Consolas,Courier New,monospace;background:#f4f4f4;padding:.2em .4em;border-radius:4px}.content[data-astro-cid-7jjqptxk] pre[data-astro-cid-7jjqptxk]{padding:1em;overflow-x:auto}
:root{--accent: #2337ff;--accent-dark: #000d8a;--black: 15, 18, 25;--gray: 96, 115, 159;--gray-light: 229, 233, 240;--gray-dark: 34, 41, 57;--gray-gradient: rgba(var(--gray-light), 50%), #fff;--box-shadow: 0 2px 6px rgba(var(--gray), 25%), 0 8px 24px rgba(var(--gray), 33%), 0 16px 32px rgba(var(--gray), 33%)}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-regular.woff) format("woff");font-weight:400;font-style:normal;font-display:swap}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-bold.woff) format("woff");font-weight:700;font-style:normal;font-display:swap}body{font-family:Atkinson,sans-serif;margin:0;padding:0;text-align:left;background:linear-gradient(var(--gray-gradient)) no-repeat;background-size:100% 600px;word-wrap:break-word;overflow-wrap:break-word;color:rgb(var(--gray-dark));font-size:20px;line-height:1.7}main{width:720px;max-width:calc(100% - 2em);margin:auto;padding:3em 1em}h1,h2,h3,h4,h5,h6{margin:0 0 .5rem;color:rgb(var(--black));line-height:1.2}h1{font-size:3.052em}h2{font-size:2.441em}h3{font-size:1.953em}h4{font-size:1.563em}h5{font-size:1.25em}strong,b{font-weight:700}a,a:hover{color:var(--accent)}p{margin-bottom:1em}.prose p{margin-bottom:2em}textarea{width:100%;font-size:16px}input{font-size:16px}table{width:100%}img{max-width:100%;height:auto;border-radius:8px}code{padding:2px 5px;background-color:rgb(var(--gray-light));border-radius:2px}pre{padding:1.5em;border-radius:8px}pre>code{all:unset}blockquote{border-left:4px solid var(--accent);padding:0 0 0 20px;margin:0;font-size:1.333em}hr{border:none;border-top:1px solid rgb(var(--gray-light))}@media (max-width: 720px){body{font-size:18px}main{padding:1em}}.sr-only{border:0;padding:0;margin:0;position:absolute!important;height:1px;width:1px;overflow:hidden;clip:rect(1px 1px 1px 1px);clip:rect(1px,1px,1px,1px);clip-path:inset(50%);white-space:nowrap}
a[data-astro-cid-eimmu3lg]{display:inline-block;text-decoration:none}a[data-astro-cid-eimmu3lg].active{font-weight:bolder;text-decoration:underline}header[data-astro-cid-3ef6ksr2]{margin:0;padding:0 1em;background:#fff;box-shadow:0 2px 8px rgba(var(--black),5%)}h2[data-astro-cid-3ef6ksr2]{margin:0;font-size:1em}h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2],h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none}nav[data-astro-cid-3ef6ksr2]{display:flex;align-items:center;justify-content:space-between}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{padding:1em .5em;color:var(--black);border-bottom:4px solid transparent;text-decoration:none}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none;border-bottom-color:var(--accent)}.social-links[data-astro-cid-3ef6ksr2],.social-links[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{display:flex}@media (max-width: 720px){.social-links[data-astro-cid-3ef6ksr2]{display:none}}footer[data-astro-cid-sz7xmlte]{padding:2em 1em 6em;background:linear-gradient(var(--gray-gradient)) no-repeat;color:rgb(var(--gray));text-align:center}.social-links[data-astro-cid-sz7xmlte]{display:flex;justify-content:center;gap:1em;margin-top:1em}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]{text-decoration:none;color:rgb(var(--gray))}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]:hover{color:rgb(var(--gray-dark))}
</style></head> <body data-astro-cid-bvzihdzo> <header data-astro-cid-3ef6ksr2> <nav data-astro-cid-3ef6ksr2> <!--<h2><a href="/">{SITE_TITLE}</a></h2>--> <h2 data-astro-cid-3ef6ksr2><a style="padding-left:0" href="/" data-astro-cid-3ef6ksr2>Ge Yuxu<br data-astro-cid-3ef6ksr2>AI & Engineering</a></h2> <div class="internal-links" data-astro-cid-3ef6ksr2> <a href="/" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Home </a>  <a href="/blog/1" class="active" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Blog </a>  <a href="/series" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Series </a>  <a href="/projects" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Projects </a>  </div> <div class="social-links" data-astro-cid-3ef6ksr2> <a href="https://github.com/geyuxu" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Go to Ge Yuxu's GitHub repo</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" data-astro-cid-3ef6ksr2><path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-astro-cid-3ef6ksr2></path></svg> </a> <a href="https://www.linkedin.com/in/geyuxu/" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Go to Ge Yuxu's LinkedIn profile</span> <svg viewBox="0 0 24 24" width="32" height="32" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" data-astro-cid-3ef6ksr2> <path fill="currentColor" d="M20.447 20.452H17.2v-5.569c0-1.328-.025-3.039-1.852-3.039-1.853 0-2.136 1.447-2.136 2.942v5.666h-3.248V9h3.122v1.561h.045c.435-.823 1.498-1.688 3.083-1.688 3.295 0 3.903 2.17 3.903 4.989v6.59zM5.337 7.433a1.882 1.882 0 110-3.764 1.882 1.882 0 010 3.764zm1.626 13.019H3.708V9h3.255v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.226.792 24 1.771 24h20.451C23.2 24 24 23.226 24 22.271V1.729C24 .774 23.2 0 22.222 0z" data-astro-cid-3ef6ksr2></path> </svg> </a> </div> </nav> </header>  <main class="page" data-astro-cid-bvzihdzo> <!-- 左侧栏 --> <aside class="sidebar" data-astro-cid-bvzihdzo> <div class="meta" data-astro-cid-bvzihdzo> <b data-astro-cid-bvzihdzo>机器学习中的数据预处理</b> <p data-astro-cid-bvzihdzo><time datetime="2023-04-27T00:00:00.000Z"> Apr 27, 2023 </time></p>   <p style="margin-top:1rem;font-weight:bold;" data-astro-cid-bvzihdzo>系列：<a href="/series/人工智能学习笔记/" data-astro-cid-bvzihdzo>人工智能学习笔记</a></p> <ul class="series-list" data-astro-cid-bvzihdzo> <li data-astro-cid-bvzihdzo><a href="/blog/ai/01_numpy_pandas_matplotlib/1_学习笔记-numpy基础与实战入门/" data-astro-cid-bvzihdzo>学习笔记-Numpy基础与实战入门</a></li><li data-astro-cid-bvzihdzo><a href="/blog/ai/01_numpy_pandas_matplotlib/2_pandas-学习笔记核心对象与常用操作/" data-astro-cid-bvzihdzo>Pandas 学习笔记：核心对象与常用操作</a></li><li data-astro-cid-bvzihdzo><a href="/blog/ai/01_numpy_pandas_matplotlib/3_matplotlib学习笔记1/" data-astro-cid-bvzihdzo>matplotlib学习笔记1</a></li><li data-astro-cid-bvzihdzo><a href="/blog/ai/01_numpy_pandas_matplotlib/4_matplotlib学习笔记2/" data-astro-cid-bvzihdzo>matplotlib学习笔记2</a></li><li data-astro-cid-bvzihdzo><a href="/blog/ai/02_machine_learning/01_数据预处理/" data-astro-cid-bvzihdzo>机器学习中的数据预处理</a></li> </ul>  </div> <hr data-astro-cid-bvzihdzo> <br data-astro-cid-bvzihdzo> </aside> <!-- 右侧正文区域（flex 居中） --> <div class="content-wrapper" data-astro-cid-bvzihdzo> <article class="prose" data-astro-cid-bvzihdzo>  <article class="content" data-astro-cid-7jjqptxk> <nav class="toc"><ol class="toc-level toc-level-1"><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#数据预处理的重要性">数据预处理的重要性</a></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#缺失值处理补全遗失的信息">缺失值处理：补全遗失的信息</a></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#数值特征的缩放让特征尺度可比">数值特征的缩放：让特征尺度可比</a><ol class="toc-level toc-level-2"><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#标准化standardization均值归零">标准化（Standardization，均值归零）</a></li><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#min-max-归一化区间缩放">Min-Max 归一化（区间缩放）</a></li><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#按样本归一化normalization-by-norm">按样本归一化（Normalization by norm）</a></li><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#二值化简单粗暴的阈值过滤">二值化：简单粗暴的阈值过滤</a></li></ol></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#分类变量编码独热编码与标签编码">分类变量编码：独热编码与标签编码</a><ol class="toc-level toc-level-2"><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#独热编码one-hot-encoding">独热编码（One-Hot Encoding）</a></li><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#标签编码label-encoding">标签编码（Label Encoding）</a></li></ol></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#总结">总结</a></li></ol></nav><h1 id="机器学习中的数据预处理"><a aria-hidden="true" tabindex="-1" href="#机器学习中的数据预处理"><span class="icon icon-link"></span></a>机器学习中的数据预处理</h1>
<h2 id="数据预处理的重要性"><a aria-hidden="true" tabindex="-1" href="#数据预处理的重要性"><span class="icon icon-link"></span></a>数据预处理的重要性</h2>
<p>现实中的数据常常充满各种“杂质”和不一致之处，例如：
•	缺失值：有的数据样本某些特征项为空缺（就像问卷有的人漏答了一题）。
•	异常值/错误数据：有的值明显不合理，比如人的身高出现了负数，或者原本应该填写年龄却填入了电话号码。
•	尺度不统一：不同特征的取值范围差异巨大，比如一个特征是用户年龄（几十的量级），另一个是收入（上万的量级）；又或者相同的度量单位混用，例如身高有的记录用米，有的用厘米。
•	数据格式不一致：类别型数据可能以文字表示（“男”/“女”）或数字编码表示（0/1），需要统一处理才能供模型使用。</p>
<p>数据预处理的目的就是应对这些问题，具体包括：去除无效或错误的数据、填补缺失值，以及对数据的范围、单位、格式进行规范化处理，把原始数据处理成模型喜欢的样子。良好的预处理能大大提升模型训练的效率和效果。这部分内容往往比模型调参更费时，但也是机器学习工程中最重要的基础之一。</p>
<p>下面，我们分别介绍常见的数据预处理步骤和方法，并解释每一步为何对模型效果至关重要。</p>
<h2 id="缺失值处理补全遗失的信息"><a aria-hidden="true" tabindex="-1" href="#缺失值处理补全遗失的信息"><span class="icon icon-link"></span></a>缺失值处理：补全遗失的信息</h2>
<p>现实数据很少完美无缺。面对缺失值（Missing Values），我们需要合理应对，否则很多算法会因为无法处理空值而报错，或者错误地将缺失当作零值处理，导致偏差。</p>
<p>常见的缺失值处理方法有：
•	删除法：直接丢弃含有缺失值的样本或特征。如果缺失值非常多且无法可靠填补，或该样本本身无意义，可以选择删掉。但删除可能丢弃有用信息，需慎重。
•	填充法：用一个合理的值替换缺失值。简单常用的策略包括用平均值或中位数填充数值型特征的空缺，用众数（出现频率最高的值）填充分类特征的空缺，或者使用固定值（如0或“未知”）标记缺失。填充值应尽量反映数据的总体趋势，避免引入明显偏差。</p>
<p>举个例子，假如某一特征是产品价格，缺失了一些值，我们可以用该特征的平均价格来填补缺失项。这样模型仍能利用大部分样本的信息，且不会因为空值而无法计算。再比如用户填写表单时有时会漏掉“年龄”这一栏，我们可以用所有填写了年龄用户的平均年龄来估计，或者干脆加一个布尔特征“是否缺失年龄”让模型自行学习其影响。</p>
<p>下面用简短的代码演示如何用平均值填充缺失值。我们构造一个简单的数组，其中包含缺失值 np.nan，然后计算均值进行替换：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> numpy </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> np</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 示例数据，包含一个缺失值 np.nan</span></span>
<span class="line"><span style="color:#E1E4E8">data </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.array([</span><span style="color:#79B8FF">1.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">2.5</span><span style="color:#E1E4E8">, np.nan, </span><span style="color:#79B8FF">4.0</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"原始数据："</span><span style="color:#E1E4E8">, data)  </span><span style="color:#6A737D"># 输出原始数组，其中第三个元素为 nan</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 计算非缺失元素的平均值</span></span>
<span class="line"><span style="color:#E1E4E8">mean_val </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.nanmean(data)  </span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"非缺失值平均值："</span><span style="color:#E1E4E8">, mean_val)  </span><span style="color:#6A737D"># 输出计算得到的均值</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 用平均值填充缺失位置</span></span>
<span class="line"><span style="color:#E1E4E8">data[np.isnan(data)] </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> mean_val  </span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"填充缺失值后："</span><span style="color:#E1E4E8">, data)  </span><span style="color:#6A737D"># 输出填补缺失值后的数组</span></span></code></pre>
<p>output</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>原始数据： [1.  2.5 nan 4. ]</span></span>
<span class="line"><span>非缺失值平均值： 2.5</span></span>
<span class="line"><span>填充缺失值后： [1.  2.5 2.5 4. ]</span></span></code></pre>
<p>上述代码中，np.nanmean 会自动忽略 nan 计算平均值。填充后，原本的 nan 被替换为了平均值，使得数据不再有空缺。</p>
<p>为什么缺失值处理影响模型效果？ 一方面，缺失值如果不处理，很多模型算法（比如大部分的 sklearn 算法）会直接报错或无法训练；另一方面，用不当的值填充（比如全部用0填充）可能引入偏差，让模型学到错误的统计规律。因此，我们需要根据业务理解选择合适的处理策略，尽量还原数据的真实分布或提供模型可理解的信号。</p>
<p>在实际项目中，常常需要针对不同特征选择不同的缺失值填充策略。例如，用户年龄缺失可以填充为平均年龄，商品评论缺失可以填充为空字符串或特殊标记。同时要注意记录哪些值是填充得来的，必要时模型可以区别对待这些推测的数据。Scikit-Learn 提供了 SimpleImputer 等工具类方便地处理缺失值，但理解背后的逻辑依然很重要。</p>
<h2 id="数值特征的缩放让特征尺度可比"><a aria-hidden="true" tabindex="-1" href="#数值特征的缩放让特征尺度可比"><span class="icon icon-link"></span></a>数值特征的缩放：让特征尺度可比</h2>
<p>数值型特征往往有不同的量纲和取值范围。如果直接把原始值喂给模型，某些值域特别大的特征会对模型产生不成比例的影响。例如，我们要利用「身高」和「体重」两个特征来预测某人的健康指数。假设身高以厘米记录（范围约150180），体重以千克记录（范围约5080），由于身高的数值普遍比体重大一倍左右，某些对数值大小敏感的算法（比如基于距离的KNN、使用梯度的线性回归/神经网络）就可能更加依赖“身高”这个特征，不是因为身高更重要，而仅仅因为它的数值较大。为避免这种“量纲偏差”，我们需要对特征做缩放变换，使得不同特征处于相近的数值范围。</p>
<p>常用的数值特征缩放方法包括标准化和归一化两大类，下面分别介绍。</p>
<h3 id="标准化standardization均值归零"><a aria-hidden="true" tabindex="-1" href="#标准化standardization均值归零"><span class="icon icon-link"></span></a>标准化（Standardization，均值归零）</h3>
<p>标准化旨在将特征数据调整为均值为0、标准差为1的分布。转换公式是对每个特征列执行：</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>X</mi><mtext>’</mtext><mo>=</mo><mfrac><mrow><mi>X</mi><mo>−</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac></mrow><annotation encoding="application/x-tex">X’ = \frac{X - \mu}{\sigma}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">’</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0463em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">μ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">μ</span></span></span></span> 是该特征的均值，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span> 是该特征的标准差。经过这样的线性变换，所有数据都围绕0上下波动，且大多数落在[-3, 3]范围（对于正态分布数据）。标准化保证每个特征的“基准值”和“波动程度”相似，在许多算法中能防止某个特征因为原始值偏大而**“一家独大”**地主导模型结果。</p>
<p>**举例：**假设我们有一组样本，每个样本有3个特征值：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> numpy </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> np</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sklearn.preprocessing </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> scale</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 样本数据：每列代表一个特征</span></span>
<span class="line"><span style="color:#E1E4E8">raw_samples </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.array([</span></span>
<span class="line"><span style="color:#E1E4E8">    [</span><span style="color:#79B8FF">3.0</span><span style="color:#E1E4E8">,  </span><span style="color:#F97583">-</span><span style="color:#79B8FF">1.0</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">2.0</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#E1E4E8">    [</span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">,   </span><span style="color:#79B8FF">4.0</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">3.0</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#E1E4E8">    [</span><span style="color:#79B8FF">1.0</span><span style="color:#E1E4E8">,  </span><span style="color:#F97583">-</span><span style="color:#79B8FF">4.0</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">2.0</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"原始数据：</span><span style="color:#79B8FF">\n</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">, raw_samples)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"每列特征的均值："</span><span style="color:#E1E4E8">, raw_samples.mean(</span><span style="color:#FFAB70">axis</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">))</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"每列特征的标准差："</span><span style="color:#E1E4E8">, raw_samples.std(</span><span style="color:#FFAB70">axis</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 使用 sklearn 的 scale 函数进行标准化（均值归零，方差归一）</span></span>
<span class="line"><span style="color:#E1E4E8">std_samples </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> scale(raw_samples)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"标准化后的数据：</span><span style="color:#79B8FF">\n</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">, std_samples)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"标准化后每列特征的均值："</span><span style="color:#E1E4E8">, std_samples.mean(</span><span style="color:#FFAB70">axis</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">))</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"标准化后每列特征的标准差："</span><span style="color:#E1E4E8">, std_samples.std(</span><span style="color:#FFAB70">axis</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">))</span></span></code></pre>
<p>output</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>原始数据：</span></span>
<span class="line"><span> [[ 3. -1.  2.]</span></span>
<span class="line"><span> [ 0.  4.  3.]</span></span>
<span class="line"><span> [ 1. -4.  2.]]</span></span>
<span class="line"><span>每列特征的均值： [ 1.33333333 -0.33333333  2.33333333]</span></span>
<span class="line"><span>每列特征的标准差： [1.24721913 3.29983165 0.47140452]</span></span>
<span class="line"><span>标准化后的数据：</span></span>
<span class="line"><span> [[ 1.33630621 -0.20203051 -0.70710678]</span></span>
<span class="line"><span> [-1.06904497  1.31319831  1.41421356]</span></span>
<span class="line"><span> [-0.26726124 -1.1111678  -0.70710678]]</span></span>
<span class="line"><span>标准化后每列特征的均值： [ 5.55111512e-17  0.00000000e+00 -2.96059473e-16]</span></span>
<span class="line"><span>标准化后每列特征的标准差： [1. 1. 1.]</span></span></code></pre>
<p>运行上述代码，我们可以看到：
•	原始数据每列的均值可能不是0，标准差各不相同。
•	标准化转换后，输出的 std_samples 每列均值接近0，标准差接近1（由于浮点误差可能不是完全0和1，但非常接近）。</p>
<p>通过标准化，数据的尺度被拉到相同水平。这在训练诸如线性回归、逻辑回归和神经网络时尤为重要：特征标准化后，梯度下降求解更稳定，收敛更快；模型对不同特征的权重调整也更公平，不会因为未标准化数据某一维数值特别大而偏向它。</p>
<p>生活类比: 标准化有点像把不同单位的度量转换到统一标准下比较。想象比较两个人的财产，一个用人民币衡量，一个用日元衡量，直接比数字会产生误导（因为1日元远小于1人民币）。只有把两人的资产都换算成同一种货币，才能公正地比较谁更富有。对特征做标准化，就是为了公正比较不同量纲的特征对模型的贡献。</p>
<h3 id="min-max-归一化区间缩放"><a aria-hidden="true" tabindex="-1" href="#min-max-归一化区间缩放"><span class="icon icon-link"></span></a>Min-Max 归一化（区间缩放）</h3>
<p>归一化通常是指将数据按比例缩放到某个固定区间，典型情况下是缩放到[0, 1]范围（也称Min-Max缩放）。转换公式为对每个特征列执行：</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>X</mi><mtext>’</mtext><mo>=</mo><mfrac><mrow><mi>X</mi><mo>−</mo><msub><mi>X</mi><mi>min</mi><mo>⁡</mo></msub></mrow><mrow><msub><mi>X</mi><mi>max</mi><mo>⁡</mo></msub><mo>−</mo><msub><mi>X</mi><mi>min</mi><mo>⁡</mo></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">X’ = \frac{X - X_{\min}}{X_{\max} - X_{\min}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">’</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.1963em;vertical-align:-0.836em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">m</span><span class="mtight">a</span><span class="mtight">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">m</span><span class="mtight">i</span><span class="mtight">n</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">m</span><span class="mtight">i</span><span class="mtight">n</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.836em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>min</mi><mo>⁡</mo></msub></mrow><annotation encoding="application/x-tex">X_{\min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">m</span><span class="mtight">i</span><span class="mtight">n</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>max</mi><mo>⁡</mo></msub></mrow><annotation encoding="application/x-tex">X_{\max}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">m</span><span class="mtight">a</span><span class="mtight">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 分别是该特征列的最小值和最大值。经过这样的映射处理，每个特征的最小值变为0，最大值变为1，其他值按原相对位置映射到0~1之间。</p>
<p>归一化的作用也是为了让不同特征的取值范围具有可比性。特别是在计算欧氏距离等度量时，如果一个特征的范围远大于另一个，那么距离计算几乎完全被“大范围”特征主导；归一化可以避免这种问题。此外，将输入特征限定在0~1之间还可能加快某些模型的收敛（例如神经网络的梯度下降）。</p>
<p>我们用一个简单示例来演示Min-Max归一化：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> numpy </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> np</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sklearn.preprocessing </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> MinMaxScaler</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 样本数据：3个特征，每列数值差异较大</span></span>
<span class="line"><span style="color:#E1E4E8">raw_samples </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.array([</span></span>
<span class="line"><span style="color:#E1E4E8">    [ </span><span style="color:#79B8FF">1.0</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">2.0</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">300.0</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#E1E4E8">    [ </span><span style="color:#79B8FF">4.0</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">5.0</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">600.0</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#E1E4E8">    [ </span><span style="color:#79B8FF">7.0</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">8.0</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">900.0</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"原始数据：</span><span style="color:#79B8FF">\n</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">, raw_samples)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 初始化一个MinMax缩放器，将范围缩放到[0,1]</span></span>
<span class="line"><span style="color:#E1E4E8">mms </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> MinMaxScaler(</span><span style="color:#FFAB70">feature_range</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">))</span></span>
<span class="line"><span style="color:#E1E4E8">scaled_samples </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> mms.fit_transform(raw_samples)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"Min-Max归一化后的数据：</span><span style="color:#79B8FF">\n</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">, scaled_samples)</span></span></code></pre>
<p>output</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>原始数据：</span></span>
<span class="line"><span> [[  1.   2. 300.]</span></span>
<span class="line"><span> [  4.   5. 600.]</span></span>
<span class="line"><span> [  7.   8. 900.]]</span></span>
<span class="line"><span>Min-Max归一化后的数据：</span></span>
<span class="line"><span> [[0.  0.  0. ]</span></span>
<span class="line"><span> [0.5 0.5 0.5]</span></span>
<span class="line"><span> [1.  1.  1. ]]</span></span></code></pre>
<p>假设原始数据第三列数值远大于前两列（如上例第三列为百位量级而前两列为个位数），则输出结果中：
•	每列的最小值都被转化为0，最大值转化为1；
•	其他值按比例缩放，例如原始中第三列600介于最小300和最大900正中间，因此归一化后为0.5；同样地，原始第二列5在2到8区间中也大约居中，对应归一化结果约0.5。</p>
<p>Min-Max归一化的注意点在于：它会压缩原有的差值分布，对最大最小值（可能是异常值）非常敏感。如果数据中存在极端异常值，Min-Max归一化会把大部分正常数据挤在接近0的位置，失去分辨率。因此，在使用前通常先处理异常值或选用对异常值不太敏感的标准化方法。</p>
<p>工程提示: Scikit-Learn的 MinMaxScaler 可以方便地对数据进行归一化。如果想缩放到[0,1]以外的区间，也可以在创建 MinMaxScaler 时通过 feature_range=(min, max) 来指定新的缩放区间。</p>
<h3 id="按样本归一化normalization-by-norm"><a aria-hidden="true" tabindex="-1" href="#按样本归一化normalization-by-norm"><span class="icon icon-link"></span></a>按样本归一化（Normalization by norm）</h3>
<p>上面的标准化和Min-Max归一化都是针对特征列进行的变换。而有些情况下，我们会对每个样本的特征向量进行归一化处理，使得每个样本自身的所有特征值之和为1（或平方和为1）。这种操作通常用于关注各特征占比而非绝对大小的场景。</p>
<p>例如，一份统计中有两年的编程语言使用人数：2017年 Python 10万人，Java 20万人，PHP 5万人；2018年 Python 8万人，Java 10万人，PHP 1万人。两年总人数都不相同，如果直接比较人数增减，Python用了8万看似减少，但它在2018年总人数中的占比反而提升了。通过对每年数据做行归一化，我们将每年的总人数视为1，再看各语言所占比例，就能更直接地比较它们的相对变化。</p>
<p>在 sklearn 中，可以使用 preprocessing.normalize 来实现按样本的归一化。例如，设置 norm=‘l1’ 表示将每个样本向量按绝对值之和归一化；norm=‘l2’ 则表示按平方和的平方根归一化（即将每个样本看作一个向量，长度缩放为1）。一般情况下，l1 归一化会把每个样本的特征值绝对值之和缩放为1：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sklearn.preprocessing </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> normalize</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> numpy </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> np</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">raw_samples </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.array([</span></span>
<span class="line"><span style="color:#E1E4E8">    [</span><span style="color:#79B8FF">10.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">20.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">5.0</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#E1E4E8">    [ </span><span style="color:#79B8FF">8.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">10.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1.0</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#6A737D"># 使用 L1 范数归一化每个样本（行）</span></span>
<span class="line"><span style="color:#E1E4E8">norm_samples </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> normalize(raw_samples, </span><span style="color:#FFAB70">norm</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">'l1'</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"按样本归一化后的数据：</span><span style="color:#79B8FF">\n</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">, norm_samples)</span></span></code></pre>
<p>output</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>按样本归一化后的数据：</span></span>
<span class="line"><span> [[0.28571429 0.57142857 0.14285714]</span></span>
<span class="line"><span> [0.42105263 0.52631579 0.05263158]]</span></span></code></pre>
<p>输出的每行数据各特征之和都会等于1。例如，第一行 [10, 20, 5] 归一化后变为 [0.2857, 0.5714, 0.1429]（各元素即为原值占总和的比例，检查可得 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.2857</mn><mo>+</mo><mn>0.5714</mn><mo>+</mo><mn>0.1429</mn><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">0.2857+0.5714+0.1429=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">0.2857</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">0.5714</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.1429</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>）。这种预处理在需要比较组成成分而非绝对值大小的任务中（如文本单词频率向量归一化）非常有用。</p>
<h3 id="二值化简单粗暴的阈值过滤"><a aria-hidden="true" tabindex="-1" href="#二值化简单粗暴的阈值过滤"><span class="icon icon-link"></span></a>二值化：简单粗暴的阈值过滤</h3>
<p>有些情况下，我们关心的不是特征的具体值，而是它是否超过某个阈值。二值化（Binarization）就是把数值特征转换成只有0和1两种取值：低于阈值记为0，高于阈值记为1。这样做可以简化模型，只保留关键信息。例如，在图像处理中，我们可以将灰度图像二值化，以突出边缘轮廓而忽略细微的灰度变化。</p>
<p>对数据特征进行二值化在Scikit-Learn中也很容易实现：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> sklearn.preprocessing </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> sp</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> numpy </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> np</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">raw_samples </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.array([[</span><span style="color:#79B8FF">65.5</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">89.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">73.0</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#E1E4E8">                        [</span><span style="color:#79B8FF">55.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">99.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">98.5</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#E1E4E8">                        [</span><span style="color:#79B8FF">45.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">22.5</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">60.0</span><span style="color:#E1E4E8">]])</span></span>
<span class="line"><span style="color:#E1E4E8">binarizer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> sp.Binarizer(</span><span style="color:#FFAB70">threshold</span><span style="color:#F97583">=</span><span style="color:#79B8FF">60</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># 定义阈值为60</span></span>
<span class="line"><span style="color:#E1E4E8">bin_samples </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> binarizer.fit_transform(raw_samples)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"二值化处理后的数据：</span><span style="color:#79B8FF">\n</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">, bin_samples)</span></span></code></pre>
<p>output</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>二值化处理后的数据：</span></span>
<span class="line"><span> [[1. 1. 1.]</span></span>
<span class="line"><span> [0. 1. 1.]</span></span>
<span class="line"><span> [0. 0. 0.]]</span></span></code></pre>
<p>在上述代码中，我们将阈值设为60，那么输出中原始值不高于60的全部变为0，高于60的变为1。需要注意，二值化会丢失数值的细粒度信息（例如把65.5和89.0都变成1，区别消失了），而且这个转换不可逆（无法从结果0/1还原原始值）。因此，除非模型确实只需要关注超过阈值与否这种信息，否则应谨慎使用。如果想保留可逆的数值转换来表示类别信息，可以考虑下文的独热编码。</p>
<p>二值化适合某些特殊场景，例如将连续的声音信号处理成0/1以表示静音和有声，或者根据考试分数划定是否及格等。在这些场景下，阈值的选择非常重要，需要根据业务需求确定。</p>
<h2 id="分类变量编码独热编码与标签编码"><a aria-hidden="true" tabindex="-1" href="#分类变量编码独热编码与标签编码"><span class="icon icon-link"></span></a>分类变量编码：独热编码与标签编码</h2>
<p>原始数据中类别型（分类）特征无法直接输入大多数机器学习模型。比如性别、颜色、品牌这种非数值信息，我们需要先编码成数字形式。常见的类别编码方法有两种：独热编码（One-Hot Encoding） 和 标签编码（Label Encoding）。它们适用于不同的场景，下面分别介绍。</p>
<h3 id="独热编码one-hot-encoding"><a aria-hidden="true" tabindex="-1" href="#独热编码one-hot-encoding"><span class="icon icon-link"></span></a>独热编码（One-Hot Encoding）</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> numpy </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> np</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sklearn.preprocessing </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OneHotEncoder</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 原始数据：每行一个样本，包含三个类别特征</span></span>
<span class="line"><span style="color:#E1E4E8">raw_samples </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.array([</span></span>
<span class="line"><span style="color:#E1E4E8">    [</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">3</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#E1E4E8">    [</span><span style="color:#79B8FF">7</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">5</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#E1E4E8">    [</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">8</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">6</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#E1E4E8">    [</span><span style="color:#79B8FF">7</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">3</span><span style="color:#E1E4E8">,  </span><span style="color:#79B8FF">9</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#6A737D"># 定义 OneHotEncoder，sparse=False 表示输出稠密NumPy数组</span></span>
<span class="line"><span style="color:#E1E4E8">one_hot </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OneHotEncoder(</span><span style="color:#FFAB70">sparse_output</span><span style="color:#F97583">=</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">oh_samples </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> one_hot.fit_transform(raw_samples)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"独热编码后的结果：</span><span style="color:#79B8FF">\n</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">, oh_samples)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"编码后矩阵形状："</span><span style="color:#E1E4E8">, oh_samples.shape)</span></span>
<span class="line"><span style="color:#6A737D"># 可以通过 inverse_transform 将编码结果还原回原始类别</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"还原回原始数据：</span><span style="color:#79B8FF">\n</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">, one_hot.inverse_transform(oh_samples))</span></span></code></pre>
<p>output:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>独热编码后的结果：</span></span>
<span class="line"><span> [[1. 0. 1. 0. 0. 1. 0. 0. 0.]</span></span>
<span class="line"><span> [0. 1. 0. 1. 0. 0. 1. 0. 0.]</span></span>
<span class="line"><span> [1. 0. 0. 0. 1. 0. 0. 1. 0.]</span></span>
<span class="line"><span> [0. 1. 1. 0. 0. 0. 0. 0. 1.]]</span></span>
<span class="line"><span>编码后矩阵形状： (4, 9)</span></span>
<span class="line"><span>还原回原始数据：</span></span>
<span class="line"><span> [[1 3 2]</span></span>
<span class="line"><span> [7 5 4]</span></span>
<span class="line"><span> [1 8 6]</span></span>
<span class="line"><span> [7 3 9]]</span></span></code></pre>
<p>在这个例子中，我们有3列分类特征。OneHotEncoder 会自动识别每列中的不同取值种类，并为每一列分别创建对应的独热编码。最终输出 oh_samples 是一个4行×9列的矩阵：前三列对应原第一列特征可能的取值{1,7}，接下来的三列对应原第二列特征可能的取值{3,5,8}，最后三列对应原第三列特征可能的取值{2,4,6,9}（由于第三列有4种取值，其独热编码其实应占4列，但这里由于原始样本未出现某种取值，OneHotEncoder自动按实际出现的类别数编码）。打印结果可以看到每行有且仅有9个值中的几个为1，其余为0。例如第一行原始数据 [1, 3, 2] 编码后可能变成 [1,0,  1,0,0,  1,0,0,0]，对应含义是：第一列取值1（编码为1,0），第二列取值3（编码为1,0,0），第三列取值2（编码为1,0,0,0）。使用 inverse_transform 我们还可以验证编码正确性，它能将独热矩阵再转回原始的类别取值。</p>
<p>独热编码不会丢失信息，并且是可逆的（如上所示我们能还原回去）。但缺点是会使数据维度变高，特别是当类别种类很多时，会生成大量稀疏的0/1特征。这会增加模型的计算和存储负担，不过对于大多数线性模型和树模型来说这是常见处理方式。实际应用中，如果某个特征的类别种类非常多，我们可能考虑目标编码或降维等别的方法，但那超出了本文范围。</p>
<h3 id="标签编码label-encoding"><a aria-hidden="true" tabindex="-1" href="#标签编码label-encoding"><span class="icon icon-link"></span></a>标签编码（Label Encoding）</h3>
<p>标签编码是另一种简单的类别编码方式，即将每个类别直接用一个整数标签表示。比如“北京, 上海, 广州”可以映射为0, 1, 2。这种方法不会增加维度，直接将类别特征转换为了数值，但需要注意类别编码后的数值本身并没有大小顺序上的意义。如果对待这些数字不加注意，某些模型（尤其是线性回归、SVM这类对数值大小敏感的算法）可能错误地将类别的数字大小当成有序信号。如果类别本身没有大小关系（如颜色、城市），一般更倾向使用独热编码而非直接使用标签编码。</p>
<p>标签编码通常用于已有大小意义的序数特征（例如教育程度高中=0、大专=1、本科=2、硕士=3）或者用于对模型输出标签进行编码（如二分类标签正/负映射为1/0）。在预处理阶段，也经常先用标签编码把文字类别转成数字表示，然后再进一步喂给独热编码或其它算法。</p>
<p>下面演示使用 LabelEncoder 对简单的类别列表进行编码和解码：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> numpy </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> np</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sklearn.preprocessing </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> LabelEncoder</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">raw_labels </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.array([</span><span style="color:#9ECBFF">'lv'</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">'ee'</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">'lth'</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">'ee'</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">'tt'</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">'lv'</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#E1E4E8">label_encoder </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> LabelEncoder()</span></span>
<span class="line"><span style="color:#E1E4E8">encoded_labels </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> label_encoder.fit_transform(raw_labels)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"标签编码结果："</span><span style="color:#E1E4E8">, encoded_labels)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"还原回原始标签："</span><span style="color:#E1E4E8">, label_encoder.inverse_transform(encoded_labels))</span></span></code></pre>
<p>output</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>标签编码结果： [2 0 1 0 3 2]</span></span>
<span class="line"><span>还原回原始标签： ['lv' 'ee' 'lth' 'ee' 'tt' 'lv']</span></span></code></pre>
<p>输出的 encoded_labels 可能是 [0 2 0 1 2 1]，表示算法将’audi’映射为0，‘bmw’映射为1，‘ford’映射为2。（具体映射次序取决于LabelEncoder按类别字母排序或出现顺序。）通过 inverse_transform 可以确认编码无误地还原回了原始字符串数组。</p>
<p>标签编码简单直接，但对于无序类别特征应慎用，避免让模型误解数值之间不存在的大小关系；独热编码更安全通用，但会增加维度。视具体情况选择合适的编码方式是特征工程的一部分。</p>
<h2 id="总结"><a aria-hidden="true" tabindex="-1" href="#总结"><span class="icon icon-link"></span></a>总结</h2>
<p>数据预处理是机器学习过程中不可或缺的一步。通过清洗无效数据、合理填补缺失、规范特征尺度以及正确编码类别信息，我们为模型提供了一个健康干净的数据集。正如盖房子要打好地基一样，充分的数据预处理能让后续的模型训练事半功倍，模型的稳定性和精度都会有明显提升。在实际工程中，不同数据集和任务可能需要不同的预处理策略，但核心思想都是为了让数据更好地表达问题、符合模型假设。</p> </article>   </article> </div> </main> <footer data-astro-cid-sz7xmlte>
&copy; 2025 All rights reserved.
</footer>  </body></html>