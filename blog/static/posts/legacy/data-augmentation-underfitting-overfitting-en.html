<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Image Classification: Data Scarcity, Augmentation, and Underfitting/Overfitting | Yuxu Ge</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1/themes/prism-tomorrow.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <style>
        :root {
            --black: #111;
            --dark-grey: #444;
            --off-white: #f4f4f4;
            --vermilion: #C41E3A;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }

        html {
            font-size: 16px;
            scroll-behavior: smooth;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: var(--black);
            background-color: var(--off-white);
        }

        a { color: var(--vermilion); text-decoration: none; }
        a:hover { opacity: 0.75; }

        .layout { min-height: 100vh; }

        /* Sidebar */
        .sidebar {
            position: fixed; top: 0; left: 0;
            width: 320px; height: 100vh;
            background: var(--black); color: var(--off-white);
            padding: 3rem 2rem;
            display: flex; flex-direction: column; justify-content: space-between;
        }
        .sidebar-top { display: flex; flex-direction: column; align-items: center; text-align: center; }
        .avatar { display: block; width: 120px; height: 120px; border-radius: 50%; border: 3px solid var(--vermilion); margin-bottom: 1.5rem; overflow: hidden; transition: transform 0.2s ease; }
        .avatar:hover { opacity: 1; transform: scale(1.05); }
        .avatar img { width: 100%; height: 100%; object-fit: cover; }
        .name-block { margin-bottom: 0.5rem; }
        h1 { font-size: 1.8rem; font-weight: 700; letter-spacing: 1px; color: var(--off-white); }
        .title-role { font-size: 0.9rem; color: #888; margin-top: 0.25rem; font-weight: 400; }
        .contact-block { margin-top: 2.5rem; width: 100%; }
        .social-links { display: flex; flex-direction: column; align-items: center; gap: 0.75rem; }
        .social-links a { display: flex; align-items: center; gap: 0.75rem; width: 160px; color: var(--off-white); opacity: 0.7; padding: 0.5rem 0.75rem; border-radius: 4px; font-size: 0.9rem; transition: all 0.2s; }
        .social-links a:hover { color: var(--vermilion); opacity: 1; background: rgba(255,255,255,0.05); }
        .social-links svg { width: 20px; height: 20px; flex-shrink: 0; }
        .social-links .orcid-link { font-size: 0.68rem; gap: 0.5rem; white-space: nowrap; }
        .sidebar-footer { text-align: center; font-size: 0.75rem; color: #555; }

        /* Content */
        .content { margin-left: 320px; padding: 3rem 4rem; max-width: 900px; }
        .back-link { display: inline-block; font-size: 0.85rem; margin-bottom: 2rem; color: var(--dark-grey); }
        .back-link:hover { color: var(--vermilion); }

        /* Article */
        article h1 { font-size: 2rem; font-weight: 700; margin-bottom: 1.5rem; line-height: 1.3; color: var(--black); }
        article h2 { font-size: 1.4rem; font-weight: 600; margin: 2rem 0 1rem; color: var(--black); }
        article h3 { font-size: 1.1rem; font-weight: 600; margin: 1.5rem 0 0.75rem; color: var(--black); }
        article p { margin-bottom: 1rem; color: var(--dark-grey); }
        article ul, article ol { margin: 1rem 0 1rem 1.5rem; color: var(--dark-grey); }
        article li { margin-bottom: 0.5rem; }
        article strong { color: var(--black); }
        article code { font-family: "SF Mono", Monaco, monospace; font-size: 0.9em; background: #e8e8e8; padding: 0.15em 0.4em; border-radius: 3px; }
        article pre { margin: 1rem 0; border-radius: 6px; overflow-x: auto; background: #2d2d2d; padding: 1rem; }
        article pre code { background: none; padding: 0; font-size: 0.75em; white-space: pre !important; counter-reset: line; display: block; color: #ccc; }
        article pre code .line { counter-increment: line; }
        article pre code .line::before { content: counter(line); display: inline-block; width: 2.5em; margin-right: 1em; text-align: right; color: #666; user-select: none; }
        .code-toolbar { display: flex; justify-content: flex-start; padding: 0.35rem 0.5rem; background: #3a3a3a; border-radius: 6px 6px 0 0; }
        .code-toolbar + pre { margin-top: 0; border-radius: 0 0 6px 6px; }
        .copy-btn { padding: 0.2rem 0.5rem; font-size: 0.7rem; background: #555; color: #fff; border: none; border-radius: 3px; cursor: pointer; transition: all 0.2s; }
        .copy-btn:hover, .copy-btn:active { background: #777; }
        .copy-btn.copied { background: #2a2; }
        article blockquote { border-left: 3px solid var(--vermilion); padding-left: 1rem; margin: 1rem 0; color: #666; font-style: italic; }
        article hr { border: none; border-top: 1px solid #ddd; margin: 2rem 0; }
        article table { width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem; }
        article th, article td { border: 1px solid #ddd; padding: 0.5rem 0.75rem; text-align: left; }
        article th { background: #e8e8e8; font-weight: 600; }
        article img { max-width: 100%; height: auto; display: block; margin: 1.5rem 0; border-radius: 6px; }

        .loading { text-align: center; padding: 3rem; color: #888; }
        .error { color: var(--vermilion); }

        /* KaTeX math styles */
        .katex-display { overflow-x: auto; overflow-y: hidden; padding: 0.5rem 0; }
        .katex { font-size: 1.1em; }

        /* Responsive */
        @media (max-width: 900px) {
            .sidebar { position: relative; width: 100%; height: auto; padding: 2rem 1.5rem 1rem; }
            .social-links { flex-direction: row; flex-wrap: wrap; justify-content: center; }
            .social-links a, .social-links .orcid-link { width: auto; padding: 0.5rem; font-size: 0; gap: 0; }
            .social-links svg { width: 24px; height: 24px; }
            .social-links .hide-mobile { display: none; }
            .sidebar-footer { margin-top: 1rem; }
            .content { margin-left: 0; padding: 2rem 1rem; }
        }
        @media (max-width: 480px) {
            .sidebar { padding: 1.5rem 1rem 1rem; }
            .avatar { width: 80px; height: 80px; }
            h1 { font-size: 1.4rem; }
            article h1 { font-size: 1.5rem; }
        }
    </style>
</head>
<body>

<div class="layout">
    <aside class="sidebar" id="sidebar-content"></aside>
    <script src="/components/sidebar.js"></script>

    <main class="content">
        <a href="/blog/" class="back-link">← Back to Blog</a>
        <article>
<hr>
<h2>date: 2024-01-01
tags: [ai, image classification, data augmentation, overfitting, underfitting, pytorch]
legacy: true</h2>
<h1>Image Classification: Data Scarcity, Augmentation, and Underfitting/Overfitting</h1>
<table>
<thead>
<tr>
<th align="left">Dimension</th>
<th align="left">Under-fitting</th>
<th align="left">Over-fitting</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>Definition</strong></td>
<td align="left">The model fails to capture the underlying patterns in the data, exhibiting high bias.</td>
<td align="left">The model learns the noise in the training data as if it were a pattern, exhibiting high variance.</td>
</tr>
<tr>
<td align="left"><strong>Learning Curve</strong></td>
<td align="left">Both training and validation loss are high and converge early.</td>
<td align="left">Training loss decreases continuously, while validation loss starts to increase after a certain point.</td>
</tr>
<tr>
<td align="left"><strong>Analogy</strong></td>
<td align="left">Trying to fit a complex &quot;S&quot;-shaped curve with a straight line.</td>
<td align="left">Trying to draw a complex path that passes through every single training point.</td>
</tr>
<tr>
<td align="left"><strong>Trigger</strong></td>
<td align="left">The model&#39;s capacity is much lower than the complexity of the data.</td>
<td align="left">The model&#39;s capacity is much higher than the amount of information in the data.</td>
</tr>
<tr>
<td align="left"><strong>Diagnosis</strong></td>
<td align="left">Learning curves are parallel and high; bias cannot be reduced.</td>
<td align="left">The gap between training and validation accuracy is large (&gt;5-10%); predictions are often wrong but with high confidence.</td>
</tr>
<tr>
<td align="left"><strong>Solutions</strong></td>
<td align="left">① Increase model capacity or add features. <br> ② Train longer or tune hyperparameters. <br> ③ Reduce regularization.</td>
<td align="left">① <strong>Data augmentation</strong> or collect more data. <br> ② Use regularization (L1/L2, Dropout, Early Stopping). <br> ③ Simplify the model. <br> ④ Use model ensembling or distillation.</td>
</tr>
</tbody></table>
<h3>3. Data Augmentation under Data Scarcity</h3>
<p>Among all strategies for mitigating over-fitting, data augmentation is one of the most direct and effective methods. It expands the information content of the dataset without additional labeling costs by applying a series of random transformations to the existing training data, creating more diverse samples that the model has &quot;never seen&quot; before.</p>
<h4>3.1 Geometric Transformations</h4>
<p>These transformations simulate variations in an object&#39;s pose, scale, and position that might occur in the real world. Common operations include: random rotation (±15° to ±30°), random scaling (0.8× to 1.2×), and random translation (≤10% of image dimensions). More complex transformations like Elastic Distortion and Perspective Transformation can provide even stronger generalization capabilities.</p>
<h4>3.2 Pixel-level Perturbations</h4>
<p>These transformations aim to improve the model&#39;s robustness to changes in image quality. For instance, adding Gaussian Noise (e.g., σ=0.01-0.05 × 255) or Salt-and-Pepper Noise (e.g., ratio=0.3-0.5%) to the image, and applying GaussianBlur (kernel size=3 or 5) or MotionBlur.</p>
<h4>3.3 Color Space Transformations</h4>
<p>Transformations in the color space enhance the model&#39;s adaptability to variations in lighting, contrast, and color. <code>ColorJitter</code> is a common tool that randomly adjusts an image&#39;s brightness, contrast, and saturation (e.g., between 0.8 and 1.2 times the original). Converting an image to grayscale and then duplicating it across three channels is another effective technique to force the model to focus on shape rather than color.</p>
<h4>3.4 Synthetic/Mixing Methods</h4>
<p>In recent years, augmentation methods that mix information from multiple images have become very popular. They create samples that do not exist in the real world but are highly beneficial for model regularization.</p>
<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="left">Core Idea</th>
<th align="left">Recommended Probability (p)</th>
<th align="left">Notes</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>Cutout/GridMask</strong></td>
<td align="left">Randomly erases one or more rectangular patches from an image.</td>
<td align="left"><code>p=0.3</code></td>
<td align="left">Be careful not to occlude key objects in detection/segmentation tasks.</td>
</tr>
<tr>
<td align="left"><strong>Mixup</strong></td>
<td align="left">Linearly interpolates two images and their labels using a ratio λ sampled from a Beta distribution.</td>
<td align="left"><code>1.0</code> (as a separate epoch)</td>
<td align="left">α=0.2 is a common hyperparameter.</td>
</tr>
<tr>
<td align="left"><strong>CutMix</strong></td>
<td align="left">Cuts a patch from one image and pastes it onto another, with label weights adjusted by the patch area.</td>
<td align="left"><code>p=0.5</code></td>
<td align="left">α=1.0 is a common hyperparameter.</td>
</tr>
</tbody></table>
<h4>3.5 Auto-Augmentation</h4>
<p>Manually designing augmentation policy combinations is time-consuming. Thus, auto-augmentation methods were developed. <strong>RandAugment</strong> is a simple yet effective method that randomly selects N transformations from a predefined pool and applies them with a uniform magnitude M. It often yields significant gains on small datasets (e.g., with N=2, M=9). The more complex <strong>AutoAugment</strong> uses reinforcement learning to search for the optimal policy combination but is computationally expensive.</p>
<h3>4. Implementation in PyTorch</h3>
<p>Here, we demonstrate how to build a <code>transform</code> pipeline in PyTorch that includes various augmentation strategies and implement a plug-and-play Mixup function.</p>
<h4>4.1 Transform Pipeline</h4>
<pre><code class="language-python"><span class="line">import torchvision.transforms as T</span>
<span class="line">import torch</span>
<span class="line">import numpy as np</span>
<span class="line"></span>
<span class="line"># Custom Salt-and-Pepper noise class</span>
<span class="line">class SaltPepperNoise:</span>
<span class="line">    def __init__(self, ratio=0.003):</span>
<span class="line">        self.ratio = ratio</span>
<span class="line">    def __call__(self, img):</span>
<span class="line">        # ... (implementation omitted)</span>
<span class="line">        return img</span>
<span class="line"></span>
<span class="line">train_tf = T.Compose([</span>
<span class="line">    T.RandomResizedCrop(224, scale=(0.8, 1.0)),</span>
<span class="line">    T.RandomHorizontalFlip(p=0.5),</span>
<span class="line">    T.RandomRotation(20),</span>
<span class="line">    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),</span>
<span class="line">    T.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),</span>
<span class="line">    # SaltPepperNoise(0.003), # Custom transform</span>
<span class="line">    T.ToTensor(),</span>
<span class="line">    T.Normalize(mean=[0.485, 0.456, 0.406],</span>
<span class="line">                std=[0.229, 0.224, 0.225])</span>
<span class="line">])</span></code></pre>
<p>This pipeline integrates various geometric and color transformations. Note that the <code>Normalize</code> step is typically placed at the end, and its mean and standard deviation should be set based on the dataset used for the pre-trained model (e.g., ImageNet).</p>
<h4>4.2 Plug-and-Play Mixup/CutMix</h4>
<pre><code class="language-python"><span class="line">def mixup_data(x, y, alpha=0.2, use_cuda=True):</span>
<span class="line">    &#39;&#39;&#39;Returns mixed inputs, pairs of targets, and lambda&#39;&#39;&#39;</span>
<span class="line">    if alpha &gt; 0:</span>
<span class="line">        lam = np.random.beta(alpha, alpha)</span>
<span class="line">    else:</span>
<span class="line">        lam = 1</span>
<span class="line"></span>
<span class="line">    batch_size = x.size()[0]</span>
<span class="line">    if use_cuda:</span>
<span class="line">        index = torch.randperm(batch_size).cuda()</span>
<span class="line">    else:</span>
<span class="line">        index = torch.randperm(batch_size)</span>
<span class="line"></span>
<span class="line">    mixed_x = lam * x + (1 - lam) * x[index, :]</span>
<span class="line">    y_a, y_b = y, y[index]</span>
<span class="line">    return mixed_x, y_a, y_b, lam</span>
<span class="line"></span>
<span class="line">def mixup_criterion(criterion, pred, y_a, y_b, lam):</span>
<span class="line">    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)</span></code></pre>
<p>In your training loop, you can fetch <code>inputs</code> and <code>targets</code> from the DataLoader, call <code>mixup_data</code> to generate mixed data, and then compute the mixed loss using <code>mixup_criterion</code>.</p>
<h4>4.3 Learning Curve Monitoring</h4>
<p>Throughout the training process, it is crucial to continuously monitor the accuracy and loss curves for both the training and validation sets. A key practice is <strong>Early Stopping</strong>: when the validation loss stops decreasing and starts to rise for N consecutive epochs, training should be halted, and the model weights with the best previous validation performance should be saved. This effectively prevents the model from over-fitting in the later stages of training.</p>
<h3>5. Integrated Tuning Workflow</h3>
<p>A systematic tuning process can help you maximize the effectiveness of data augmentation:</p>
<ol>
<li><strong>Establish Baseline</strong>: First, train the model without any data augmentation and record its accuracy and loss as a baseline.</li>
<li><strong>Phase 1</strong>: Add basic geometric and color transformations; the learning rate can be kept the same or slightly increased.</li>
<li><strong>Phase 2</strong>: Introduce Mixup or CutMix, often in conjunction with Label Smoothing for better results.</li>
<li><strong>Phase 3</strong>: If computational resources permit, try using RandAugment or AutoAugment to automatically search for the optimal augmentation policy.</li>
<li><strong>Phase 4</strong>: Finally, fine-tune the model&#39;s capacity (e.g., network depth) and regularization parameters (e.g., Dropout rate, weight decay) based on the augmented data distribution.</li>
</ol>
<h3>6. Common Pitfalls &amp; Practical Tips</h3>
<ul>
<li><strong>Over-augmentation</strong>: Excessive augmentation can cause a significant distribution shift between the training set and the actual test set, thereby hurting performance. It&#39;s advisable to start with a small application probability (e.g., <code>p=0.3</code>).</li>
<li><strong>Label Synchronization</strong>: In object detection or semantic segmentation tasks, when applying geometric transformations to an image, the exact same transformations must be applied to the bounding boxes or masks.</li>
<li><strong>Validation Set Purity</strong>: The validation set should be kept as &quot;clean&quot; as possible to accurately reflect the model&#39;s performance on the original data distribution. Typically, only necessary resizing and center cropping are applied, without heavy augmentation.</li>
<li><strong>Mixup with Small Batches</strong>: Using Mixup with a very small batch size can excessively dilute the original signal. In such cases, consider increasing the batch size or adjusting the learning rate.</li>
</ul>
<h3>7. Conclusion &amp; Further Exploration</h3>
<p>When faced with the challenge of data scarcity, our core objective is to <strong>expand the diversity</strong> of the data through augmentation, not just to increase its quantity. The struggle against under-fitting and over-fitting is essentially about finding a delicate balance between the model&#39;s <strong>capacity</strong> and the data&#39;s <strong>information content</strong>.</p>
<p>The strategies introduced in this article are a classic starting point for solving this problem. Building on this foundation, you can further explore more advanced directions, such as: using unlabeled data for Self-supervised Pre-training, synthesizing new data with generative models (like Diffusion Models) or 3D rendering, and applying Semi-supervised Learning techniques.</p>

    </article>
    </main>
</div>

<script src="https://cdn.jsdelivr.net/npm/prismjs@1/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-python.min.js"></script>
<script>
Prism.highlightAll();
document.querySelectorAll('article pre').forEach(pre => {
    const code = pre.querySelector('code');
    if (!code) return;
    const toolbar = document.createElement('div');
    toolbar.className = 'code-toolbar';
    const btn = document.createElement('button');
    btn.className = 'copy-btn';
    btn.textContent = 'Copy';
    btn.onclick = () => {
        navigator.clipboard.writeText(code.textContent).then(() => {
            btn.textContent = 'Copied!';
            btn.classList.add('copied');
            setTimeout(() => { btn.textContent = 'Copy'; btn.classList.remove('copied'); }, 2000);
        });
    };
    toolbar.appendChild(btn);
    pre.parentNode.insertBefore(toolbar, pre);
});
</script>
</body>
</html>