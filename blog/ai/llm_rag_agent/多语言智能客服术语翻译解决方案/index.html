<!DOCTYPE html><html lang="zh" data-astro-cid-bvzihdzo> <head><!-- Global Metadata --><meta charset="utf-8"><!--<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">--><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"><link rel="shortcut icon" href="/favicon.png" type="image/png"><link rel="sitemap" href="/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" title="Ge Yuxu • AI &#38; Engineering" href="https://geyuxu.com/rss.xml"><meta name="generator" content="Astro v5.5.6"><!-- Font preloads --><link rel="preload" href="/fonts/atkinson-regular.woff" as="font" type="font/woff" crossorigin><link rel="preload" href="/fonts/atkinson-bold.woff" as="font" type="font/woff" crossorigin><!-- Canonical URL --><link rel="canonical" href="https://geyuxu.com/blog/ai/llm_rag_agent/%E5%A4%9A%E8%AF%AD%E8%A8%80%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%9C%AF%E8%AF%AD%E7%BF%BB%E8%AF%91%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"><!-- Primary Meta Tags --><title>多语言智能客服术语翻译解决方案</title><meta name="title" content="多语言智能客服术语翻译解决方案"><meta name="description"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://geyuxu.com/blog/ai/llm_rag_agent/%E5%A4%9A%E8%AF%AD%E8%A8%80%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%9C%AF%E8%AF%AD%E7%BF%BB%E8%AF%91%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"><meta property="og:title" content="多语言智能客服术语翻译解决方案"><meta property="og:description"><meta property="og:image" content="https://geyuxu.com/blog-placeholder-1.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://geyuxu.com/blog/ai/llm_rag_agent/%E5%A4%9A%E8%AF%AD%E8%A8%80%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%9C%AF%E8%AF%AD%E7%BF%BB%E8%AF%91%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"><meta property="twitter:title" content="多语言智能客服术语翻译解决方案"><meta property="twitter:description"><meta property="twitter:image" content="https://geyuxu.com/blog-placeholder-1.jpg"><script src="/js/jquery-3.7.1.min.js"></script><script is:global>
	window.addEventListener('DOMContentLoaded', () => {
		$('.toc ol').css({
		'list-style': 'none',   // 隐藏 1. 2. 3.
		'margin': 0,
		'padding-left': 0,       // 可按需调整
		});
		$('.toc ol > li').css({
		'list-style': 'none',   // 隐藏 1. 2. 3.
		'padding-left': 10  
		});
        $('.sidebar').append($('.toc'));
      });
	</script><style>:root{--accent: #2337ff;--accent-dark: #000d8a;--black: 15, 18, 25;--gray: 96, 115, 159;--gray-light: 229, 233, 240;--gray-dark: 34, 41, 57;--gray-gradient: rgba(var(--gray-light), 50%), #fff;--box-shadow: 0 2px 6px rgba(var(--gray), 25%), 0 8px 24px rgba(var(--gray), 33%), 0 16px 32px rgba(var(--gray), 33%)}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-regular.woff) format("woff");font-weight:400;font-style:normal;font-display:swap}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-bold.woff) format("woff");font-weight:700;font-style:normal;font-display:swap}body{font-family:Atkinson,sans-serif;margin:0;padding:0;text-align:left;background:linear-gradient(var(--gray-gradient)) no-repeat;background-size:100% 600px;word-wrap:break-word;overflow-wrap:break-word;color:rgb(var(--gray-dark));font-size:20px;line-height:1.7}main{width:960px;max-width:calc(100% - 2em);margin:auto;padding:3em 1em}h1,h2,h3,h4,h5,h6{margin:0 0 .5rem;color:rgb(var(--black));line-height:1.2}h1{font-size:3.052em}h2{font-size:2.441em}h3{font-size:1.953em}h4{font-size:1.563em}h5{font-size:1.25em}strong,b{font-weight:700}a,a:hover{color:var(--accent)}p{margin-bottom:1em}.prose p{margin-bottom:2em}textarea{width:100%;font-size:16px}input{font-size:16px}table{width:100%;border-collapse:collapse}th,td{border:1px solid rgba(var(--gray-light));padding:8px;text-align:left}th{background-color:rgba(var(--gray-light),.5)}img{max-width:100%;height:auto;border-radius:8px}code{padding:2px 5px;background-color:rgb(var(--gray-light));border-radius:2px}pre{padding:1.5em;border-radius:8px}pre>code{all:unset}blockquote{border-left:4px solid var(--accent);padding:0 0 0 20px;margin:0;font-size:1.333em}hr{border:none;border-top:1px solid rgb(var(--gray-light))}@media (max-width: 720px){body{font-size:18px}main{padding:1em}}.sr-only{border:0;padding:0;margin:0;position:absolute!important;height:1px;width:1px;overflow:hidden;clip:rect(1px 1px 1px 1px);clip:rect(1px,1px,1px,1px);clip-path:inset(50%);white-space:nowrap}.title_li{list-style:none}.title_li>a,.title_li_num>a,.toc-link,.series-list>li>a,.series_title>a{color:#7a888e}.content{max-width:960px;margin:0 auto;padding:2rem 1rem;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,sans-serif;font-size:1.05rem;line-height:1.75;color:#333}.content h1,.content h2,.content h3{font-weight:600;margin-top:2rem;margin-bottom:1rem;line-height:1.3}.content p{margin-bottom:1.25rem}.content a{color:var(--accent, #0070f3);text-decoration:underline}.content img{max-width:100%;border-radius:6px;margin:1.5rem 0}.content pre,.content code{font-family:Menlo,Monaco,Consolas,Courier New,monospace;font-size:.9em;border-radius:4px}.content pre{padding:1em;overflow-x:auto}article.prose{font-size:var(--fs-base);line-height:1.7}@media (max-width: 640px){nav>div,nav>h2{font-size:.6em}ul,ol{padding-left:20px}main,.content-wrapper{padding-left:0;width:350px;max-width:60350px0px;margin:0 auto}img,table,pre{max-width:100%}table,pre{overflow-x:auto}.content{margin:0;padding:0;border:0;max-width:3350px00px;font-size:11px}h1{font-size:1.3rem}h2{font-size:1.2rem}h3{font-size:1.1rem}h4{font-size:1rem}h5{font-size:.9rem}}
a[data-astro-cid-eimmu3lg]{display:inline-block;text-decoration:none}a[data-astro-cid-eimmu3lg].active{font-weight:bolder;text-decoration:underline}header[data-astro-cid-3ef6ksr2]{margin:0;padding:0 1em;background:#fff;box-shadow:0 2px 8px rgba(var(--black),5%)}h2[data-astro-cid-3ef6ksr2]{margin:0;font-size:1em}h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2],h2[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none}nav[data-astro-cid-3ef6ksr2]{display:flex;align-items:center;justify-content:space-between}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{padding:1em .5em;color:var(--black);border-bottom:4px solid transparent;text-decoration:none}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none;border-bottom-color:var(--accent)}.social-links[data-astro-cid-3ef6ksr2],.social-links[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{display:flex}@media (max-width: 720px){.social-links[data-astro-cid-3ef6ksr2]{display:none}}footer[data-astro-cid-sz7xmlte]{padding:2em 1em 6em;background:linear-gradient(var(--gray-gradient)) no-repeat;color:rgb(var(--gray));text-align:center}.social-links[data-astro-cid-sz7xmlte]{display:flex;justify-content:center;gap:1em;margin-top:1em}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]{text-decoration:none;color:rgb(var(--gray))}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]:hover{color:rgb(var(--gray-dark))}
.statement[data-astro-cid-uffxixac]{font-size:10px;color:gray}@media (max-width: 640px){.statement[data-astro-cid-uffxixac]{font-size:6px;color:gray}}
main[data-astro-cid-bvzihdzo].page{display:grid;grid-template-columns:260px minmax(0,1fr);width:100%;margin:0}aside[data-astro-cid-bvzihdzo].sidebar{box-sizing:border-box;width:260px;padding:2rem 1rem;font-size:.95rem;position:sticky;top:4rem;align-self:start}.sidebar[data-astro-cid-bvzihdzo] .meta[data-astro-cid-bvzihdzo] p[data-astro-cid-bvzihdzo]{margin:.25rem 0}nav[data-astro-cid-bvzihdzo].toc li[data-astro-cid-bvzihdzo]{margin:.35rem 0 .35rem 1rem}nav[data-astro-cid-bvzihdzo].toc a[data-astro-cid-bvzihdzo]{color:var(--gray-dark,#444);text-decoration:none}nav[data-astro-cid-bvzihdzo].toc a[data-astro-cid-bvzihdzo]:hover{text-decoration:underline}.content-wrapper[data-astro-cid-bvzihdzo]{display:flex;justify-content:center;padding:2rem 1rem}article[data-astro-cid-bvzihdzo].prose{max-width:740px;width:100%}@media (max-width: 768px){main[data-astro-cid-bvzihdzo].page{grid-template-columns:1fr}aside[data-astro-cid-bvzihdzo].sidebar{position:static;width:100%;padding:1rem}.content-wrapper[data-astro-cid-bvzihdzo]{justify-content:flex-start}article[data-astro-cid-bvzihdzo].prose{max-width:100%}}.series-list[data-astro-cid-bvzihdzo]{list-style:none;margin:0;padding-left:10px}.series-list[data-astro-cid-bvzihdzo]{list-style:none;margin:0;padding-left:10px;max-height:calc(16em + .5rem);overflow-y:auto}.series-list[data-astro-cid-bvzihdzo]::-webkit-scrollbar{width:6px}.series-list[data-astro-cid-bvzihdzo]::-webkit-scrollbar-thumb{background:#0003;border-radius:3px}
</style></head> <body data-astro-cid-bvzihdzo> <header data-astro-cid-3ef6ksr2> <nav data-astro-cid-3ef6ksr2> <!--<h2><a href="/">{SITE_TITLE}</a></h2>--> <h2 data-astro-cid-3ef6ksr2><a style="padding-left:0;color:blue;" href="/" data-astro-cid-3ef6ksr2>Ge Yuxu<br data-astro-cid-3ef6ksr2>AI & Engineering</a></h2> <div class="internal-links" data-astro-cid-3ef6ksr2> <a href="/" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Home </a>  <a href="/blog/1" class="active" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Blog </a>  <a href="/series" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Series </a>  <a href="/projects" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Projects </a>  </div> <div class="social-links" data-astro-cid-3ef6ksr2> <a href="https://github.com/geyuxu" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Go to Ge Yuxu's GitHub repo</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" data-astro-cid-3ef6ksr2><path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-astro-cid-3ef6ksr2></path></svg> </a> <a href="https://www.linkedin.com/in/geyuxu/" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Go to Ge Yuxu's LinkedIn profile</span> <svg viewBox="0 0 24 24" width="32" height="32" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" data-astro-cid-3ef6ksr2> <path fill="currentColor" d="M20.447 20.452H17.2v-5.569c0-1.328-.025-3.039-1.852-3.039-1.853 0-2.136 1.447-2.136 2.942v5.666h-3.248V9h3.122v1.561h.045c.435-.823 1.498-1.688 3.083-1.688 3.295 0 3.903 2.17 3.903 4.989v6.59zM5.337 7.433a1.882 1.882 0 110-3.764 1.882 1.882 0 010 3.764zm1.626 13.019H3.708V9h3.255v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.226.792 24 1.771 24h20.451C23.2 24 24 23.226 24 22.271V1.729C24 .774 23.2 0 22.222 0z" data-astro-cid-3ef6ksr2></path> </svg> </a> </div> </nav> <!-- Microsoft Clarity --> <script type="text/javascript">
		(function(c,l,a,r,i,t,y){
			c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
			t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
			y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
		})(window, document, "clarity", "script", "rc2w96osp6");
	</script> </header>  <main class="page" data-astro-cid-bvzihdzo> <!-- 左侧栏 --> <aside class="sidebar" data-astro-cid-bvzihdzo> <div class="meta" data-astro-cid-bvzihdzo> <b data-astro-cid-bvzihdzo>多语言智能客服术语翻译解决方案</b> <p data-astro-cid-bvzihdzo><time datetime="2024-10-10T16:00:00.000Z"> 2024/10/11 00:00:00 </time></p>   <p class="series_title" style="margin-top:1rem;font-weight:bold;" data-astro-cid-bvzihdzo>系列：<a href="/series/大模型-rag-agent专题/" data-astro-cid-bvzihdzo>大模型、RAG、Agent专题</a></p> <ul class="series-list" data-astro-cid-bvzihdzo> <li data-astro-cid-bvzihdzo>🍉&nbsp;<a href="/blog/ai/llm_rag_agent/打造具备行动力的智能客服用-agent-让大模型接入真实系统/" data-astro-cid-bvzihdzo>打造具备行动力的智能客服：用 Agent 让大模型接入真实系统</a></li><li data-astro-cid-bvzihdzo>🍉&nbsp;<a href="/blog/ai/llm_rag_agent/构建闭环智能会议助手从语音转写到语音摘要播报的技术实践/" data-astro-cid-bvzihdzo>构建闭环智能会议助手：从语音转写到语音摘要播报的技术实践</a></li><li data-astro-cid-bvzihdzo>🍉&nbsp;<a href="/blog/ai/llm_rag_agent/防范-ai-幻觉在医疗法律与教育场景中的应用安全实践/" data-astro-cid-bvzihdzo>防范 AI 幻觉：在医疗、法律与教育场景中的应用安全实践</a></li><li data-astro-cid-bvzihdzo>🍉&nbsp;<a href="/blog/ai/llm_rag_agent/构建多智能体营销系统从角色划分到调度协同的实践/" data-astro-cid-bvzihdzo>构建多智能体营销系统：从角色划分到调度协同的实践</a></li><li data-astro-cid-bvzihdzo>🍉&nbsp;<a href="/blog/ai/llm_rag_agent/法律咨询场景中的大模型微调前评估/" data-astro-cid-bvzihdzo>法律咨询场景中的大模型微调前评估</a></li><li data-astro-cid-bvzihdzo>🍉&nbsp;<a href="/blog/ai/llm_rag_agent/复杂结构文档解析keynote-转换与自定义-reader-双路径实践/" data-astro-cid-bvzihdzo>复杂结构文档解析：Keynote 转换与自定义 Reader 双路径实践</a></li><li data-astro-cid-bvzihdzo>🍉&nbsp;<a href="/blog/ai/llm_rag_agent/大语言模型是如何思考的从分词到推理的全过程揭秘/" data-astro-cid-bvzihdzo>大语言模型是如何“思考”的：从分词到推理的全过程揭秘</a></li><li data-astro-cid-bvzihdzo>🍉&nbsp;<a href="/blog/ai/llm_rag_agent/一次微调训练失败案例解析从-qwen25-的损失曲线看问题与优化/" data-astro-cid-bvzihdzo>一次微调训练失败案例解析：从 Qwen2.5 的损失曲线看问题与优化</a></li><li data-astro-cid-bvzihdzo>🍉&nbsp;<a href="/blog/ai/llm_rag_agent/多语言智能客服术语翻译解决方案/" data-astro-cid-bvzihdzo>多语言智能客服术语翻译解决方案</a></li><li data-astro-cid-bvzihdzo>🍉&nbsp;<a href="/blog/ai/llm_rag_agent/微调用领域数据提升大模型任务表现/" data-astro-cid-bvzihdzo>微调：用领域数据提升大模型任务表现</a></li><li data-astro-cid-bvzihdzo>🍉&nbsp;<a href="/blog/ai/llm_rag_agent/rag-系统中的敏感词防护策略从前置检测到即时拦截的全链路实践/" data-astro-cid-bvzihdzo>RAG 系统中的敏感词防护策略：从前置检测到即时拦截的全链路实践</a></li> </ul>  </div> <hr data-astro-cid-bvzihdzo> <br data-astro-cid-bvzihdzo> </aside> <!-- 右侧正文区域（flex 居中） --> <div class="content-wrapper" data-astro-cid-bvzihdzo> <article class="prose" data-astro-cid-bvzihdzo>  <article class="content"> <nav class="toc"><ol class="toc-level toc-level-1"><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#背景与挑战">背景与挑战</a></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#常见误区">常见误区</a></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#双管齐下的正确方案概述">双管齐下的正确方案概述</a><ol class="toc-level toc-level-2"><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#方法一提示词优化嵌入术语表">方法一：提示词优化（嵌入术语表）</a></li><li class="toc-item toc-item-h3"><a class="toc-link toc-link-h3" href="#方法二检索增强生成rag接入术语库">方法二：检索增强生成（RAG）接入术语库</a></li></ol></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#rag-原理速览">RAG 原理速览</a></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#实现术语检索流程">实现：术语检索流程</a></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#技术选型与实现细节">技术选型与实现细节</a></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#提示词rag融合优势">提示词+RAG：融合优势</a></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#产品架构与实践案例">产品架构与实践案例</a></li><li class="toc-item toc-item-h2"><a class="toc-link toc-link-h2" href="#结语">结语</a></li></ol></nav><h2 id="背景与挑战"><a aria-hidden="true" tabindex="-1" href="#背景与挑战"><span class="icon icon-link"></span></a>背景与挑战</h2>
<p>在全球化商业环境下，企业往往需要面向不同语言的用户提供智能客服服务。多语言智能客服系统必须确保回答内容在各种语言中保持一致和准确，尤其涉及企业内部非公开的专业术语时，翻译和解释的准确性尤为关键。传统的大语言模型（LLM）即使具备强大的通用翻译能力，但在面对特定领域的大量专业术语时，常常显得力不从心。这些术语可能是内部项目代号、产品功能名称或行业专有名词，在不同语言中有独特的表达方式，无法通过字面直接翻译。如果处理不当，智能客服的回答要么翻译错误，要么让用户无法理解，严重影响用户体验。举例来说，在游戏领域中，“绿毛虫”实际指代的是宝可梦系列中的角色 Caterpie，“密集大蛇”对应的是另一个特定角色 Hydrapple。直接使用机器翻译或未经增强的LLM进行翻译时，往往得不到预期结果。某次尝试中，Claude 3.5 模型在没有参考术语表的情况下将“密集大蛇”翻译为 “Dense Snake” 或 “Crowded Snake”，明显与期望的专有名称不符。可见，如何准确翻译并解释这些内部术语是多语言客服面临的一大难题。</p>
<h2 id="常见误区"><a aria-hidden="true" tabindex="-1" href="#常见误区"><span class="icon icon-link"></span></a>常见误区</h2>
<p>在尝试解决上述术语翻译难题时，一些错误做法可能看似简便，却隐藏风险，需要我们予以警惕并加以驳斥：</p>
<ul>
<li><strong>让模型跳过未知术语</strong>： 有人可能会提出，如果客服系统检测到一句话中含有模型未见过的术语，就让模型直接跳过不翻译或不回答。然而，这种做法会导致回答内容不完整，用户提出的问题中的关键部分被忽略。对于提问者而言，跳过术语等同于回避问题，无法真正解决用户的疑问。更糟的是，用户可能因收到不完整的答复而更加困惑或不满。在客服场景中，忽略任何用户提及的术语都会破坏沟通的连贯性与可信度。因此，让模型跳过未知术语并不是可取的方案。</li>
<li><strong>仅靠少量示例让模型类比推断</strong>： 另一种错误思路是给模型提供少数几个术语翻译示例，期望它能“举一反三”地类比翻译其他所有术语。虽然大语言模型具备一定的归纳推理能力，但这种过度推断极其不可靠。专业术语往往缺乏统一的语言模式，不同术语之间可能并没有可类比的关联。仅靠几条示例，模型很可能对未见过的术语产生错误的翻译，要么直接音译要么望文生义，甚至编造不存在的解释，造成错误信息的传播。此外，行业术语的翻译通常具有约定俗成的唯一正确对应，靠模型自行类比容易南辕北辙。因此，把希望寄托在少量示例的类推上，是解决术语翻译问题的歧途。</li>
</ul>
<p>上述两种做法一个逃避问题，一个赌运气，都无法有效解决多语言术语翻译和解释的需求。真正可靠的方案需要主动出击：既让模型预先了解术语含义，又能在实时回答时提供权威参考。下面我们将介绍的正是业界验证有效的双管齐下方案。</p>
<h2 id="双管齐下的正确方案概述"><a aria-hidden="true" tabindex="-1" href="#双管齐下的正确方案概述"><span class="icon icon-link"></span></a>双管齐下的正确方案概述</h2>
<p>要破解多语言客服术语翻译的难题，我们需要将提示词优化和 <strong>检索增强生成（RAG）</strong> 相结合，形成“双管齐下”的解决思路。其中：</p>
<ul>
<li>提示词优化（Prompt Engineering）：在模型提示词中嵌入整理好的术语翻译表，引导模型参照指定的翻译去回答。这相当于在对话一开始就给模型提供一份“术语指南”，确保模型对关键术语有正确的翻译和解释方向。通过精心设计提示词，我们可以在模型回答前对其进行“预训练场景”的引导，大幅提升其对内部术语的敏感度和准确性。</li>
<li>检索增强生成（RAG，Retrieval-Augmented Generation）：将客服系统接入一个术语数据库/知识库。在用户提问时，系统会实时检索用户问题或潜在回答中涉及的术语信息，将对应的翻译和解释作为参考知识提供给模型，使其生成的回答包含权威资料 。RAG框架可以被视为给模型配备了“外脑”，让模型不再孤军奋战地翻译术语，而是有据可依地给出解释。这种知识增强方法能够显著提高回答的准确性，并避免模型因知识盲区而产生的猜测或幻觉。</li>
</ul>
<p>通过结合上述两种方法，我们既能在提示阶段校准模型的翻译倾向，又能在生成阶段为模型提供最新最全的术语资料，最大程度上确保内部专业术语被准确翻译和解释。接下来，我们将分别深入探讨这两种方法的原理及实践，并阐述如何在产品架构和工程实现中将它们融为一体。</p>
<h3 id="方法一提示词优化嵌入术语表"><a aria-hidden="true" tabindex="-1" href="#方法一提示词优化嵌入术语表"><span class="icon icon-link"></span></a>方法一：提示词优化（嵌入术语表）</h3>
<p>提示词优化（Prompt Engineering）是指通过设计和优化输入给模型的提示内容，以引导模型产生更符合期望的输出。在术语翻译场景中，提示词优化的核心做法就是在提示中预先嵌入术语表，让模型在回答用户之前就“心中有数”。</p>
<p>具体而言，我们首先需要和业务团队一起梳理内部专业术语表：包括术语原文、各目标语言的标准翻译，以及术语的解释说明。整理出的术语表可以采用简单的格式，例如：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>术语：&#x3C;术语原文></span></span>
<span class="line"><span>翻译：&#x3C;目标语言翻译></span></span>
<span class="line"><span>解释：&#x3C;术语含义或背景解释></span></span></code></pre>
<p>对于支持多种语言的客服系统，可以为每个重要术语都准备多语言对照翻译。然后，在设计模型的系统提示（System Prompt）或 few-shot 示例时，将这份术语对照信息嵌入进去。例如，我们可以在系统提示中加入这样一段内容：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>注意：以下是内部术语对照表，请严格按照指定翻译和解释回答：</span></span>
<span class="line"><span>- 术语A： 翻译为 "Term A (目标语言)"，含义是...</span></span>
<span class="line"><span>- 术语B： 翻译为 "Term B (目标语言)"，含义是...</span></span>
<span class="line"><span>...</span></span></code></pre>
<p>通过这样的提示，模型在接收到用户问题后，会参考已经提供的术语对照来生成回答，从而确保这些词汇的翻译和解释与预期保持一致。</p>
<p>这种方法在术语数量较少的情况下非常有效。因为术语不多时，我们完全可以将关键术语都罗列在提示中，模型得到明确指引后往往能给出准确的翻译。提示词工程的优势在于实现简单、即时见效：无需修改模型内部，仅通过输入提示就起到了类似规则库的作用，让LLM变得“术语敏感”。</p>
<p>然而，提示词优化也存在局限性。当内部术语数量变多时，直接在提示中塞入几十上百条术语翻译将导致提示上下文过长。这不仅增加了每次对话的通信和计算成本，还可能因为提示内容过于庞杂而干扰模型对具体问题的关注。正如业内案例所示，如果面对大量专有术语，仅靠将映射“固化”在提示中，会使提示词臃肿冗长，反而影响模型性能和注意力。同时，模型的上下文窗口有限，大量术语堆积可能超过窗口长度，导致某些信息无效。此外，企业术语表是动态更新的，新术语不断出现，靠手工更新提示内容也不便于维护。</p>
<p>因此，在实践中，我们通常将提示词优化作为基础手段：保证少量核心术语始终在模型的认知范围内。而对于规模庞大的术语集或者频繁更新的专业名词库，则需要配合更智能的机制，即我们下面讨论的 RAG 检索增强方案。</p>
<h3 id="方法二检索增强生成rag接入术语库"><a aria-hidden="true" tabindex="-1" href="#方法二检索增强生成rag接入术语库"><span class="icon icon-link"></span></a>方法二：检索增强生成（RAG）接入术语库</h3>
<p>检索增强生成（RAG）是一种将信息检索与文本生成相结合的技术架构。在RAG框架下，模型不再孤立地基于提示生成回答，而是可以借助外部知识库来获取所需信息，再将这些信息融入回答。这种方式常用于问答类聊天机器人，以避免模型出现“幻觉”或回答不准确的问题。在我们的多语言客服术语翻译场景中，引入RAG意味着：将术语数据库接入到客服系统中，在用户提问时实时查询相关术语的翻译和解释，并提供给LLM参考。</p>
<h2 id="rag-原理速览"><a aria-hidden="true" tabindex="-1" href="#rag-原理速览"><span class="icon icon-link"></span></a>RAG 原理速览</h2>
<p>RAG的流程一般分为两个阶段：检索和生成。首先，系统会根据用户的输入内容进行检索，从知识库中找到最相关的文档片段（在本场景下就是与输入中的专业术语相关的翻译解释条目）。接着，将检索到的结果与原始用户问句一并送入LLM，作为扩充后的提示，让模型基于附加的权威资料来生成回答。这种“检索+生成”的策略确保了模型的输出有据可依——模型引用了外部知识库中的内容，从而提高答案的准确性并减少瞎猜的情况。</p>
<p>在术语翻译场景中，我们的知识库主要存储内部术语及其多语言释义。可以把它想象成一个双语或多语词典：给定某个术语，能够查到它在不同语言下的正规译名和详细解释说明。这种术语库可以由公司内部维护，确保涵盖所有相关的专有名词及最新定义。一旦将其与LLM集成，模型在回答涉及术语的问题时就可以实时查阅最权威、最新的翻译和解释，而不是依赖训练语料中过时或缺失的信息。</p>
<h2 id="实现术语检索流程"><a aria-hidden="true" tabindex="-1" href="#实现术语检索流程"><span class="icon icon-link"></span></a>实现：术语检索流程</h2>
<p>当用户发起一条问句后，引入RAG的智能客服系统会执行下列流程（产品架构角度）：</p>
<ol>
<li>识别查询中的术语：系统首先分析用户的提问内容，检测其中是否包含内部专业术语。可以通过简单的字符串匹配、正则表达，或更先进的分词+词典匹配方法来完成。例如，将用户提问进行分词，只保留那些命中术语库中关键词的部分 。这样可以提取出提问中所有潜在的内部术语。若用户使用的是本地语言描述某个内部概念，也可以反查得到对应的标准术语条目。</li>
<li>查询术语数据库：针对识别出的每个术语，系统在术语数据库（或知识库）中进行检索。这里的检索可以是精确匹配查询（例如以术语字符串为键在数据库中查找）或向量语义检索（将术语或问句编码为向量，在向量数据库中找相关条目）。在很多场景下，术语名称精确匹配即可获得对应翻译，因为术语往往固定写法。正如某些实践所示，通过基于专词的精准字符串匹配来召回映射，简单且高效。而如果用户输入并非完全匹配的术语（比如不同语言的同义描述），可以考虑在术语库中维护别名，或使用向量匹配来提高召回率。</li>
<li>获取术语翻译和解释：检索到的记录会包含该术语在目标语言下的标准翻译，以及对术语的解释说明。有的系统可能直接存储多语言对照表 ；也有的方案将术语解释文档向量化存储，检索返回整段解释文本供模型参考。这一步的结果是，我们获得了与用户提问相关的术语资料。如果一个问句涉及多个术语，则可能会取回多条记录。</li>
<li>构建增强提示：将上一步获取的术语资料与原始用户提问合并，构造成新的提示信息送入LLM。为了让模型清楚这些资料的用途，我们通常会在提示模板中加上说明，例如：参考以下术语定义回答用户问题：…，然后列出检索到的术语翻译和解释内容。LangChain 等框架提供了便捷的接口来将检索结果插入提示，例如使用RetrievalQA链会自动将文档内容拼接到prompt上下文中。在提示词中包含这些资料，等于为模型增加了“短期记忆”：即便这些术语不在模型训练集中，模型现在也能在回答时引用这些外部知识。</li>
<li>生成答案：增强提示准备好后，交由底层的大语言模型（例如 GPT-4、Claude 等）生成最终答复。因为提示中提供了术语翻译和解释，模型会据此进行语言组织，回答中包含正确的译名并适当解释术语涵义。值得注意的是，模型应使用用户所用的语言来回答。因此，如果术语资料库中的解释是另一种语言，模型还需将解释翻译成用户语言。这通常不成问题，因为模型擅长在多语言间转换文本。然而，为了准确起见，也可以在术语库中存储多语言版本的解释，直接根据会话语言选择相应版本，避免模型二次翻译可能引入的偏差。</li>
<li>返回结果并优化：最终的答案返回给用户，其中内部术语已被正确翻译。例如，当用户用西班牙语询问一个包含中文内部代号的问题时，系统可识别出该代号对应的官方英文名称及西班牙语解释，将其融入回答。用户将收到一段西班牙语答复，其中出现了正确的术语译名，并伴有对该术语的解释说明（可能以括号或附注形式）。这样，用户在自己的语言环境下理解了原本属于另一语言/文化的专有名词，沟通无障碍。如果发现某些术语没有匹配上或解释不够清晰，产品团队可以据此补充术语库，从而形成反馈循环，不断提升术语覆盖率和解释质量。</li>
</ol>
<p>通过上述流程，RAG 方法确保每次对话都能动态获取所需的术语知识，而不是事先把所有术语硬塞给模型。 的实践也表明，将海量专有术语存储在外部数据库中，查询时仅召回相关条目用于提示，是切实可行的方案。这种设计很好地平衡了信息丰富度和效率：一方面模型不再受限于自身训练语料，对于未见过的术语也能借助知识库做到心中有数；另一方面，又避免了把大量无关术语一次性注入上下文造成的干扰和成本 。</p>
<h2 id="技术选型与实现细节"><a aria-hidden="true" tabindex="-1" href="#技术选型与实现细节"><span class="icon icon-link"></span></a>技术选型与实现细节</h2>
<p>在工程实现层面，我们需要搭建起支撑上述流程的各个组件：</p>
<ul>
<li>术语数据库/知识库：可根据规模和性能要求选择适当的存储方案。如果术语量不大（几千条以内）且主要通过精确匹配检索，一个传统的关系型数据库或NoSQL键值存储足以胜任，例如 DynamoDB 这类键值数据库被证明非常适合存储和管理专业术语及其翻译。对于术语量巨大或需要模糊匹配的场景，则可考虑向量数据库（如 Milvus、Pinecone 等）将术语描述向量化，以支持语义查询。在多语言环境下，向量库可以使用多语言嵌入模型，将不同语言的术语描述映射到同一向量空间，从而实现跨语言的相似度匹配。无论哪种方案，都需保证查询延迟低、吞吐高，以支撑实时客服的要求。</li>
<li>检索服务：这是连接LLM和知识库的中间层。可以利用LangChain等开源框架来简化实现。LangChain 提供了统一的Retriever接口，支持多种后端（向量库、SQL、NoSQL）的检索操作，并能够方便地将结果接入LLM流程。开发者可以配置一个自定义检索链：当输入用户问题后，链条先调用 Retriever 从术语库检索相关文档，然后将文档结果与原问题一并传给LLM。借助 LangChain 的 PromptTemplate，我们还能自定义拼接提示的格式，确保术语资料以模型容易理解利用的方式呈现给它。</li>
<li>模型调用与编排：结合提示词优化和RAG后，整个系统的调用流程可以通过一个多步骤Chain或Agent来编排。比如，LangChain 支持定义多步骤的链：第一个步骤执行术语检索（如上所述），第二个步骤调用LLM产生回答。甚至可以构建一个 LangChain Agent，令其自主决定是否需要查询术语库这一工具。当用户问答涉及不熟悉的词汇时，Agent可以调用检索工具获取解释，然后再回答。这种 Agent 方案利用LLM的自主推理能力，动态地对接外部工具，更加灵活。不过相对也复杂一些，在术语场景下常规的固定检索->回答链已能够满足需求。</li>
<li>多语言支持：由于面向多语言用户，我们需要处理好源语言和目标语言。通常智能客服会检测用户提问的语言，以便选择适当的回复语言和内容。在术语检索上，也要考虑语言因素。如果术语库记录的是某一主语言的术语映射，那么当用户使用另一语言提问时，需要先将提问中的关键词规范化（可能涉及翻译或映射）到术语库的索引语言。例如，用户用法语提到一个内部项目代号，但术语库里这个代号记录的是英文->中文的翻译，则系统应能识别出这是同一术语，并返回其中文解释，然后让模型再将解释翻译回法语给用户。理想情况下，术语库可以直接存储多语对照，实现从任意语言到任意语言的映射查询 。实践中，可根据主要使用场景，将内部术语统一存为主语言（如英文）索引，在记录中附带其他语言翻译，查询时无论输入语种如何，都先定位主术语条目，再提取对应语言的翻译和解释供输出。</li>
</ul>
<p>通过上述组件的协同，我们构建出一个具备术语查询能力的多语言客服引擎。当用户提出的问题涉及内部知识时，系统会“想一想”数据库，查到权威资料后再回答，极大提升了回答的可靠性和专业性。这套机制的引入，从根本上避免了模型遗漏术语或张冠李戴的情况，使客服在复杂领域也能游刃有余。</p>
<h2 id="提示词rag融合优势"><a aria-hidden="true" tabindex="-1" href="#提示词rag融合优势"><span class="icon icon-link"></span></a>提示词+RAG：融合优势</h2>
<p>将提示词优化与RAG 检索增强结合起来，正是解决多语言术语翻译难题的最佳实践。两者相辅相成，各自发挥所长：</p>
<p>首先，提示词中的术语表为模型提供了先验指导。模型在对话一开始就了解有哪些词是特殊的、该如何翻译，从而在生成回复时倾向于使用正确的术语表述。这种预先校准有效避免了模型因自由生成而可能出现的胡乱翻译。此外，我们还能在提示中加入一些策略性的指示，例如“对于内部术语，请给出翻译并附加括号解释”，以规范模型回答格式。这确保了无论检索结果如何，模型都有遵循既定规则来处理术语的倾向性。</p>
<p>随后，RAG机制又为模型提供了实时的知识支援。即便提示中没覆盖到的术语，检索步骤也会把相关翻译和解释补充上来，让模型在回答时不至于“无词可用”。特别对于那些未曾在对话开头声明的新术语或最新定义，RAG可以动态填补信息空白。例如，当企业更新了某项产品功能名称，只需在术语数据库中更新对应条目，客服系统立刻就能通过检索获取新的翻译解释，无需等待模型重新训练或提示手工更新。这保证了知识库与模型输出的一致性和时效性。</p>
<p>两种手段的融合带来了明显的优势：</p>
<ul>
<li>翻译准确性最大化：提示词提供稳定的译法参考，RAG提供完整的背景说明，双重保障下，模型几乎不会翻译错关键术语。就算LLM对某个术语一知半解，检索到的资料也会纠正其倾向，使最终答案中的术语翻译万无一失。</li>
<li>解释完整性增强：由于客服回答需要不仅翻译术语还要解释其含义，模型在有了检索资料后，能够给出更翔实的解释说明。例如，提示词让模型知道术语X译成“产品Alpha”，而检索的结果提供了“产品Alpha是本公司新推出的一款高性能云服务器”，模型据此可以生成一句“产品Alpha（本公司新推出的高性能云服务器）”的回答，准确且富有信息量。这种解释的丰富程度远非模型单凭记忆可以做到。</li>
<li>减少幻觉与疏漏：LLM单纯依赖训练语料时，有可能编造不存在的术语解释或翻译（这在专业领域较为常见，即所谓幻觉现象）。通过提示+RAG，模型回答时几乎只会依据提供的术语表和检索资料来输出，与可信资料之外的杜撰大大减少。同时也避免了遗漏术语不答的情况发生——因为模型知道任何不懂的词系统都会提供资料给它。</li>
<li>易于维护和扩展：整个方案中，大部分专业知识维护在术语库中。当有新术语加入、旧术语更新或翻译调整时，只需修改术语库即可，提示模板可能只需很小变动甚至不变。模型本身无需重新训练，维护成本低。而提示词工程可以持续调优，例如发现模型倾向于漏掉解释，则可以在提示里强化“务必解释术语”的要求；发现某些口径不一致，可以在术语表中修正。RAG部分则可通过改进检索算法（例如优化向量索引，添加同义词）来提高命中率。架构解耦使系统具备良好的灵活性。</li>
</ul>
<p>需要注意的是，在融合使用时，要确保提示和检索结果信息之间不存在矛盾。一旦提供给模型的指导不一致，反而会令模型无所适从。因此团队应保持提示术语表与术语数据库内容的同步一致。如果提示中已经列出某术语的翻译，而检索又返回不同版本，就可能造成冲突。理想状态下，可以简化提示术语表，只保留少量示例或指导，而让绝大部分具体术语由RAG来提供，以避免重复与不一致。</p>
<h2 id="产品架构与实践案例"><a aria-hidden="true" tabindex="-1" href="#产品架构与实践案例"><span class="icon icon-link"></span></a>产品架构与实践案例</h2>
<p>从产品架构的角度来看，引入术语机制后的多语言智能客服系统通常呈现出模块化的分层设计：</p>
<ol>
<li>多语言支持层：负责语言检测与转换。例如识别用户消息的语言种类，将用户输入和输出在不同语言间转换。这一层确保了系统可以无缝面向全球用户，但在术语方案中需要与术语库配合处理多语言映射关系。</li>
<li>提示管理层：包含系统预置的提示词模板和术语表。当新会话开始或上下文重置时，这一层会生成适当的初始提示词，告诉LLM基本的回复风格和约定，包括嵌入的关键术语对照。它相当于客服AI的大脑中预先存储的“常识和规则”。</li>
<li>检索工具层：集成RAG所需的检索能力。通常由一个向量数据库或键值数据库及其查询接口组成，封装为服务供上层调用。在接受到用户查询后，这一层按照前述流程识别并查询术语，将结果返回给对话管理模块。比如，通过LangChain，可以将这一层实现为一个独立的Retriever对象，其背后连接着术语库的数据源。</li>
<li>LLM对话层：也就是大语言模型调用层。这里通过一个Chain或Agent来 orchestrate 对话：它接收用户输入，调用检索工具层获取扩展信息，然后将完整的提示送入LLM得到结果。LangChain 提供了现成的ConversationalRetrievalChain可用于类似场景，将聊天历史和检索知识结合考虑。在非开源框架下，也可以自行编排这些步骤，例如在应用代码中先查数据库再拼接字符串调用模型API。</li>
<li>响应生成与反馈层：模型输出结果后，这一层可以对结果再做处理，如格式化、高亮术语译名、或者与聊天上下文整合。最终答复发送给用户。如果有罕见术语未翻译或用户追问进一步细节，该层也负责将这些信息反馈回步骤2-4，进行下一轮交互。</li>
</ol>
<p>为了更直观地理解，可以参考某实际案例的架构流程 ：在离线阶段，企业先将自己的多语言术语映射表通过数据管道导入到一个高速数据库中（如 DynamoDB）供实时查询使用；在在线阶段，每当有用户提问进入系统，后端服务就会对提问进行切词，提取出其中的专有名词，然后调用数据库获取这些词的多语言映射关系，将检索结果构造成附加说明插入提示中，最后调用大语言模型生成翻译结果 。这个流程充分体现了我们讨论的提示+RAG融合策略：切词提取术语相当于在提示阶段锁定关注点，而数据库检索映射则是典型的RAG操作，两者共同服务于LLM回答。</p>
<p>在工程实践中，一些企业已经成功应用了上述方法。以亚马逊AWS团队分享的方案为例，他们使用 Amazon DynamoDB 存储了海量术语翻译映射，AWS Glue 定期离线处理更新术语数据，在线则通过 Lambda 函数实时检索术语并将结果融合到 Amazon Bedrock 提供的LLM模型提示中 。这一系统架构保证了模型面对游戏等垂直领域的大量专有名词时，仍能给出令用户满意的多语言答复。事实上，该方案在产品交互中取得了出色效果：用户问题无论包含多少内部代号，都能被准确理解和回应，而客服机器人的回答既有专业水准又通俗易懂。</p>
<h2 id="结语"><a aria-hidden="true" tabindex="-1" href="#结语"><span class="icon icon-link"></span></a>结语</h2>
<p>多语言智能客服要提供卓越的用户体验，离不开对术语翻译和解释机制的重视。内部专业术语承载着企业独有的知识和信息壁垒，如果翻译不当，将成为沟通中的绊脚石。本文提出的“提示词优化 + RAG方案双管齐下”正是一种切实可行的路径：通过提示词工程，模型一开始就被校准在正确的术语翻译轨道上；借助检索增强，模型又在每次回答时拥有权威的数据支撑。 这种结合让AI客服系统既有规则指导，又有即时查询能力，达到了静动结合、相辅相成的效果。</p>
<p>对于产品经理和工程师来说，这一方案在实现上具有很高的性价比。我们无须训练定制模型，仅通过构建术语知识库和巧妙的提示设计，就能大幅提升通用LLM在垂直领域、多语言场景下的表现。更重要的是，随着业务的发展，新术语层出不穷，此架构能够快速迭代——更新术语库即可同步到模型的回答中，敏捷而高效。展望未来，随着LangChain等LLM工具链的成熟，以及企业内部数据积累的丰富，术语翻译机制将变得更加智能。我们或许可以进一步引入知识图谱来捕捉术语之间的关系，或利用Few-Shot Prompt Tuning让模型更好地理解专业词汇。但无论技术如何演进，以用户理解为中心的理念不变：让每一位用户都能听懂我们的专业术语，并从中受益。</p>
<p>通过提示词优化和RAG架构的双管齐下，我们有信心打造出更专业、更可靠、更贴心的多语言智能客服系统，为全球用户提供一致且准确的服务体验。这不仅是技术方案的成功，也是产品价值的体现。</p> </article> <html lang="en" data-astro-cid-uffxixac> <head><!-- Global Metadata --><meta charset="utf-8"><!--<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">--><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"><link rel="shortcut icon" href="/favicon.png" type="image/png"><link rel="sitemap" href="/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" title="Ge Yuxu • AI &#38; Engineering" href="https://geyuxu.com/rss.xml"><meta name="generator" content="Astro v5.5.6"><!-- Font preloads --><link rel="preload" href="/fonts/atkinson-regular.woff" as="font" type="font/woff" crossorigin><link rel="preload" href="/fonts/atkinson-bold.woff" as="font" type="font/woff" crossorigin><!-- Canonical URL --><link rel="canonical" href="https://geyuxu.com/blog/ai/llm_rag_agent/%E5%A4%9A%E8%AF%AD%E8%A8%80%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%9C%AF%E8%AF%AD%E7%BF%BB%E8%AF%91%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"><!-- Primary Meta Tags --><title>Ge Yuxu • AI &amp; Engineering</title><meta name="title" content="Ge Yuxu • AI &#38; Engineering"><meta name="description" content="Welcome to my blog!"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://geyuxu.com/blog/ai/llm_rag_agent/%E5%A4%9A%E8%AF%AD%E8%A8%80%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%9C%AF%E8%AF%AD%E7%BF%BB%E8%AF%91%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"><meta property="og:title" content="Ge Yuxu • AI &#38; Engineering"><meta property="og:description" content="Welcome to my blog!"><meta property="og:image" content="https://geyuxu.com/blog-placeholder-1.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://geyuxu.com/blog/ai/llm_rag_agent/%E5%A4%9A%E8%AF%AD%E8%A8%80%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%9C%AF%E8%AF%AD%E7%BF%BB%E8%AF%91%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"><meta property="twitter:title" content="Ge Yuxu • AI &#38; Engineering"><meta property="twitter:description" content="Welcome to my blog!"><meta property="twitter:image" content="https://geyuxu.com/blog-placeholder-1.jpg"><script src="/js/jquery-3.7.1.min.js"></script></head> <body data-astro-cid-uffxixac>  <div class="statement" data-astro-cid-uffxixac> <blockquote data-astro-cid-uffxixac> <p data-astro-cid-uffxixac><strong data-astro-cid-uffxixac>脱敏说明</strong>：本文所有出现的表名、字段名、接口地址、变量名、IP地址及示例数据等均非真实，
    仅用于阐述技术思路与实现步骤，示例代码亦非公司真实代码。
    示例方案亦非公司真实完整方案，仅为本人记忆总结，用于技术学习探讨。<br data-astro-cid-uffxixac>
&nbsp;&nbsp;&nbsp;&nbsp;•&nbsp;文中所示任何标识符并不对应实际生产环境中的名称或编号。<br data-astro-cid-uffxixac>
&nbsp;&nbsp;&nbsp;&nbsp;•&nbsp;示例&nbsp;SQL、脚本、代码及数据等均为演示用途，不含真实业务数据，也不具备直接运行或复现的完整上下文。<br data-astro-cid-uffxixac>
&nbsp;&nbsp;&nbsp;&nbsp;•&nbsp;读者若需在实际项目中参考本文方案，请结合自身业务场景及数据安全规范，使用符合内部命名和权限控制的配置。</p> <p data-astro-cid-uffxixac><strong data-astro-cid-uffxixac>版权声明</strong>：本文版权归原作者所有，未经作者事先书面许可，任何单位或个人不得以任何方式复制、转载、摘编或用于商业用途。<br data-astro-cid-uffxixac>
&nbsp;&nbsp;&nbsp;&nbsp;•&nbsp;若需非商业性引用或转载本文内容，请务必注明出处并保持内容完整。<br data-astro-cid-uffxixac>
&nbsp;&nbsp;&nbsp;&nbsp;•&nbsp;对因商业使用、篡改或不当引用本文内容所产生的法律纠纷，作者保留追究法律责任的权利。<br data-astro-cid-uffxixac><br data-astro-cid-uffxixac> <em data-astro-cid-uffxixac>Copyright&nbsp;©&nbsp;1989–Present&nbsp;Ge&nbsp;Yuxu.&nbsp;All&nbsp;Rights&nbsp;Reserved.</em></p> </blockquote> </div></body></html>  </article> </div> </main> <footer data-astro-cid-sz7xmlte>
&copy; 2025 All rights reserved.
</footer>  </body></html>